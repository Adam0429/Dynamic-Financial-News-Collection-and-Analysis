{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "100%|██████████| 20961/20961 [00:27<00:00, 770.29it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:128: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:159: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.4774624373956594\n",
      "召回率： 0.4945109261897665\n",
      "精确率： 0.4806757045140566\n",
      "Epoch 1/10\n",
      "16768/16768 [==============================] - 2s 100us/step - loss: 0.6955 - acc: 0.5018A: 1s\n",
      "Epoch 2/10\n",
      "16768/16768 [==============================] - 2s 125us/step - loss: 0.6932 - acc: 0.5053\n",
      "Epoch 3/10\n",
      "16768/16768 [==============================] - 2s 108us/step - loss: 0.6931 - acc: 0.5016 0s - loss: 0.69\n",
      "Epoch 4/10\n",
      "16768/16768 [==============================] - 2s 100us/step - loss: 0.6931 - acc: 0.5077\n",
      "Epoch 5/10\n",
      "16768/16768 [==============================] - 2s 94us/step - loss: 0.6932 - acc: 0.5005: 0s - loss: 0.6933 - acc: 0.500\n",
      "Epoch 6/10\n",
      "16768/16768 [==============================] - 2s 103us/step - loss: 0.6932 - acc: 0.5039 1s - loss: \n",
      "Epoch 7/10\n",
      "16768/16768 [==============================] - 2s 90us/step - loss: 0.6933 - acc: 0.5051\n",
      "Epoch 8/10\n",
      "16768/16768 [==============================] - 2s 96us/step - loss: 0.6930 - acc: 0.5095\n",
      "Epoch 9/10\n",
      "16768/16768 [==============================] - 2s 90us/step - loss: 0.6932 - acc: 0.5023\n",
      "Epoch 10/10\n",
      "16768/16768 [==============================] - 2s 93us/step - loss: 0.6931 - acc: 0.5034\n",
      "4193/4193 [==============================] - 0s 10us/step\n",
      "[0.694064966127749, 0.47984736506406717]\n",
      "Python 3.6.3 (v3.6.3:2c5fed86e0, Oct  3 2017, 00:32:08) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n",
      "In [1]: exit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xlrd,xlwt\n",
    "import numpy as np\n",
    "# import spacy\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import json\n",
    "import IPython\n",
    "# nltk.download()\n",
    "\n",
    "def review_to_words(review_text):\n",
    "\t# 1. Remove HTML\n",
    "\t# review_text = BeautifulSoup(raw_review).get_text() \n",
    "\t#\n",
    "\t# 2. Remove non-letters\t\t\n",
    "\tletters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "\t#\n",
    "\t# 3. Convert to lower case, split into individual words\n",
    "\twords = letters_only.lower().split()\t\t\t\t\t\t\t \n",
    "\t#\n",
    "\t# 4. In Python, searching a set is much faster than searching\n",
    "\t#   a list, so convert the stop words to a set\n",
    "\t_stopwords = set(stopwords.words(\"english\"))\n",
    "\t_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\t_stopwords.append('would')\n",
    "\t_stopwords.append('kmh')\n",
    "\t_stopwords.append('mph')\n",
    "\t_stopwords.append('  ')\n",
    "\t_stopwords.append('Reuters')\t\t\t\t  \n",
    "\t# \n",
    "\t# 5. Remove stop words\n",
    "\tmeaningful_words = [w for w in words if not w in _stopwords]   \n",
    "\t#\n",
    "\t# 6. Join the words back into one string separated by space, \n",
    "\t# and return the result.\n",
    "\treturn(meaningful_words)\n",
    "\n",
    "def PS(w,contents,labels):\n",
    "\tN = len(contents)\n",
    "\tpos_count = 0\n",
    "\tneg_count = 0\n",
    "\tw_pos_count = 0\n",
    "\tw_neg_count = 0\n",
    "\tw_count = 0\n",
    "\tfor index in range(0,len(labels)):\n",
    "\t\tif w in contents[index]:\n",
    "\t\t\tw_count += 1\n",
    "\t\tlabel = labels[index].split(',')\n",
    "\t\tif '暂无数据' in label:\n",
    "\t\t\tcontinue\n",
    "\t\telif '1' in label and '0' not in label:\n",
    "\t\t\tpos_count += 1\n",
    "\t\t\tif w in contents[index]:\n",
    "\t\t\t\tw_pos_count += 1\n",
    "\t\t\tcontinue\n",
    "\t\telif '0' in label and '1' not in label:\n",
    "\t\t\tneg_count += 1\n",
    "\t\t\tif w in contents[index]:\n",
    "\t\t\t\tw_neg_count += 1\n",
    "\n",
    "\tfreq_pos = pos_count/N \n",
    "\tfreq_neg = neg_count/N\n",
    "\tfreq_w_pos = w_pos_count/N\n",
    "\tfreq_w_neg = w_neg_count/N\n",
    "\tfreq_w = w_count/N\n",
    "\n",
    "\tif freq_w_pos*N == 0:\n",
    "\t\tPMI_w_pos = 0\n",
    "\telse:\n",
    "\t\tPMI_w_pos = np.log(freq_w_pos*N/freq_w*freq_pos)\n",
    "\tif freq_w_neg*N == 0:\n",
    "\t\tPMI_w_neg = 0\n",
    "\telse:\n",
    "\t\tPMI_w_neg = np.log(freq_w_neg*N/freq_w*freq_neg)\n",
    "\treturn PMI_w_pos - PMI_w_neg\n",
    "\n",
    "\n",
    "\n",
    "workbook = xlrd.open_workbook(r'/Users/wangfeihong/Desktop/Dynamic-Financial-News-Collection-and-Analysis/data/labeled_data2018.xls')\n",
    "sheet = workbook.sheet_by_index(0)\n",
    "contents = sheet.col_values(1)\n",
    "prices = sheet.col_values(3)\n",
    "\n",
    "len_word = 0\n",
    "tokens = []\n",
    "sentences = []\n",
    "labels = []\n",
    "for price in prices:\n",
    "\t# if '暂无数据' in label:\n",
    "\t# \tlabels.append(0)\n",
    "\tprice_list = json.loads(price)\n",
    "\tup = price_list[2] - price_list[1]\n",
    "\tif up > 0:\n",
    "\t\tlabels.append(1)\t\t\n",
    "\telse:\n",
    "\t\tlabels.append(0)\n",
    "for content in tqdm(contents):\n",
    "\t# doc = nlp(content)\n",
    "\t# sents = list(doc.sents)   # 分解为句子\n",
    "\t# for sent in sents:\n",
    "\t# \tsentences.append(str(sents))\n",
    "\t# \ttoken = list(map(str,sent))\n",
    "\t# \ttokens.append(token)\n",
    "\tsentences.append(content)\n",
    "\ttoken = review_to_words(content)\n",
    "\ttokens.append(token)\n",
    "\n",
    "model = Word2Vec(sentences = tokens,min_count = 2)\n",
    "\n",
    "bag_of_keywords = set(['rise','drop','fall','gain','surge','shrink','jump','slump','surge'])\n",
    "stop = False\n",
    "bok_size = 100\n",
    "for i in range(10):\n",
    "\tnew_words = []\n",
    "\tif stop:break\n",
    "\tfor k in bag_of_keywords:\n",
    "\t\tif k in model.wv.vocab.keys():# wv = wordvector\n",
    "\t\t\tnew_words.extend(model.most_similar(k))\n",
    "for n in new_words:\n",
    "\tif n[0].islower() and len(n[0])>3 and n[0].isalpha():\n",
    "\t\tbag_of_keywords.add(n[0])\n",
    "\t\tif len(bag_of_keywords) == bok_size:\n",
    "\t\t\tstop = True\n",
    "\t\t\tbreak\n",
    "\n",
    "'''fit():计算数据的参数，\\mu（均值），\\sigma（标准差），并存储在对象中（例如实例化的CountVectorizer()等）。\n",
    "transform():将这些参数应用到数据集，进行标准化（尺度化）。'''\n",
    "\n",
    "## Bag of keywords 统计词语的两个api\n",
    "bag_of_keywords = np.array(list(bag_of_keywords))\n",
    "bok_tfidf = TfidfVectorizer(lowercase = False, min_df = 1, vocabulary=bag_of_keywords)\n",
    "X_bok_tfidf = bok_tfidf.fit_transform(sentences)\n",
    "X_bok_tfidf = X_bok_tfidf.toarray()\n",
    "bok_count = CountVectorizer(lowercase=False,min_df=1, vocabulary=bag_of_keywords)\n",
    "X_bok_count = bok_count.fit_transform(sentences)\n",
    "X_bok_count = X_bok_count.toarray()\n",
    "\n",
    "## Category tag\n",
    "category_tags = set(['published','presented','unveil','investment','bankrupt','acquisition','government'\n",
    "                     'sue','lawsuit','highlights'])\n",
    "stop = False\n",
    "cate_size = 100\n",
    "\n",
    "for _ in range(10):\n",
    "    new_words = []\n",
    "    if stop:break\n",
    "    for k in category_tags:\n",
    "        if k in model.wv.vocab.keys():\n",
    "            new_words.extend(model.most_similar(k))\n",
    "    for n in new_words:\n",
    "        if n[0].islower() and len(n[0])>3 and n[0].isalpha():\n",
    "            category_tags.add(n[0])\n",
    "            if len(category_tags) == cate_size:\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "\n",
    "category_tags = np.array(list(category_tags))\n",
    "\n",
    "ct_count = CountVectorizer(lowercase = False, min_df = 1, vocabulary = category_tags)\n",
    "X_ct_count = ct_count.fit_transform(sentences)\n",
    "X_ct_count = X_ct_count.toarray()\n",
    "\n",
    "ct_tfidf = TfidfVectorizer(lowercase = False, min_df = 1, vocabulary = category_tags)\n",
    "X_ct_idf = ct_tfidf.fit_transform(sentences)\n",
    "X_ct_idf = X_ct_idf.toarray()\n",
    "\n",
    "full_tfidf = TfidfVectorizer(lowercase=False, min_df = 1,vocabulary=bag_of_keywords,use_idf=False)\n",
    "X_full_tfidf = full_tfidf.fit_transform(sentences)\n",
    "X_full_tfidf = X_full_tfidf.toarray()\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "# x = np.random.random((664,200))\n",
    "# y = np.random.random((664, 10))\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = X_bok_count\n",
    "y = np.array(labels)\n",
    "y = to_categorical(y,num_classes=2)\n",
    "train_x,test_x,train_y,test_y=model_selection.train_test_split(x,y,test_size=0.2,shuffle=False)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))\n",
    "\n",
    "nmodel = Sequential()\n",
    "nmodel.add(Dense(units=num_classes, activation = 'relu', input_dim = x.shape[1]))\n",
    "# 输出就是在输出层有几个神经元,每个神经元代表着一个预测结果,label的序列长度为十，须要十个神经元与之对应。label用to_categorical转换\n",
    "nmodel.add(Dropout(0.5))\n",
    "#避免过拟合\n",
    "''' 多层的意义\n",
    "单层神经网络只能用于表示线性可分离的函数。也就是说非常简单的问题，例如，分类问题中可以被一行整齐地分隔开的两个类。如果你的问题相对简单，那么单层网络就足够了。\n",
    "\n",
    "然而，我们有兴趣解决的大多数问题都不是线性可分的。\n",
    "\n",
    "多层感知器可用于表示凸区域。这意味着，实际上，他们可以学习在一些高维空间中围绕实例绘制形状，以对它们进行分类，从而克服线性可分性的限制。'''\n",
    "nmodel.add(Dense(2, activation = 'relu'))\n",
    "nmodel.add(Dropout(0.5))\n",
    "# dropout:https://blog.csdn.net/program_developer/article/details/80737724\n",
    "nmodel.add(Dense(2, activation = 'softmax'))\n",
    "nmodel.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics = ['accuracy'])\n",
    "#verbose=1:更新日志 verbose=2:每个epoch一个进度行\n",
    "\n",
    "nmodel.fit(train_x,train_y,epochs=10, batch_size=5)\n",
    "score = nmodel.evaluate(test_x, test_y, batch_size=20)\n",
    "print(score)\n",
    "\n",
    "# predict = nmodel.predict_classes(x_test,batch_size=5)\n",
    "\n",
    "# for i in range(predict)\n",
    "\n",
    "import IPython\n",
    "IPython.embed()\n",
    "\n",
    "# for row in rows:\n",
    "# \trows[rows.index(row)] = review_to_words(row)\n",
    "# vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "#\t\t\t\t\t\t\t  tokenizer = None,\t\n",
    "#\t\t\t\t\t\t\t  preprocessor = None, \n",
    "#\t\t\t\t\t\t\t  stop_words = None,  \n",
    "#\t\t\t\t\t\t\t  max_features = 10) \n",
    "\n",
    "# train_data_features = vectorizer.fit_transform(rows)\n",
    "# train_data_features = train_data_features.toarray()\n",
    "# vocab = vectorizer.get_feature_names()\n",
    "# gnb = GaussianNB()\n",
    "# gnb.fit(train_data_features, [1]*100)\n",
    "# r = gnb.predict([np.ones(10)])\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
