{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import IPython\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from nltk import sent_tokenize\n",
    "from senticnet.senticnet import SenticNet\n",
    "from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = ['JJ','JJR','JJS']\n",
    "adv = ['RB','RBR','RBS']\n",
    "vb = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "nn = ['NN','NNS']\n",
    "\n",
    "sn = SenticNet()\n",
    "\n",
    "def review_to_words(review_text): \n",
    "    if '(Reuters) -' in review_text:\n",
    "        review_text = review_text.split('(Reuters) -')[1]\n",
    "    if '*' in review_text:\n",
    "        review_text = review_text.split('*')[1]\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.split()                             \n",
    "    _stopwords = set(stopwords.words(\"english\"))\n",
    "    _stopwords = nltk.corpus.stopwords.words('english')\n",
    "    _stopwords.append('would')\n",
    "    _stopwords.append('kmh')\n",
    "    _stopwords.append('mph')\n",
    "    _stopwords.append('  ')\n",
    "    _stopwords.append('Reuters')\n",
    "    _stopwords.append('reuters')\n",
    "    # _stopwords = []\n",
    "    meaningful_words = [w for w in words if w not in _stopwords]\n",
    "    return meaningful_words\n",
    "\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "# import spacy\n",
    "# nlp = spacy.load('en')\n",
    "# def stem_and_check(word):\n",
    "#     word = nlp(word)\n",
    "#     return word[0].lemma_\n",
    "    # word = inf.singularize(word)\n",
    "    # word = nltk.PorterStemmer().stem(word)\n",
    "    # if d.check(word):\n",
    "    #    return word\n",
    "    # suggest_words = d.suggest(word)\n",
    "    # if len(suggest_words) == 0:\n",
    "    #    return word\n",
    "    # return suggest_words[0]\n",
    "\n",
    "def my_read(path):\n",
    "    file = open(path)\n",
    "    words = []\n",
    "    for line in file.readlines():\n",
    "        words.append(line.strip())\n",
    "    return words\n",
    "\n",
    "    \n",
    "def output_cloud(count,name):\n",
    "    # 云图\n",
    "    text = '' \n",
    "    for key,value in count.items():\n",
    "        text += (key+' ') * (value)\n",
    "    wc = WordCloud(\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        max_font_size=100,      #字体大小\n",
    "        min_font_size=10,\n",
    "        collocations=False, \n",
    "        max_words=1000\n",
    "    )\n",
    "    wc.generate(text)\n",
    "    wc.to_file(name+'.png') #图片保存\n",
    "\n",
    "\n",
    "\n",
    "def vote(results,datas):\n",
    "    if len(datas) != len(results):\n",
    "        raise(\"vote error\")\n",
    "    vote_dict = {}\n",
    "    date_set = set([data['date'] for data in datas])\n",
    "    company_set = set([data['company'] for data in datas])\n",
    "    for company in company_set:\n",
    "        vote_dict[company] = {}\n",
    "        for date in date_set:\n",
    "            vote_dict[company][date] = set()\n",
    "    for idx in range(0,len(datas)):\n",
    "        date = datas[idx]['date']\n",
    "        company = datas[idx]['company']         \n",
    "        vote_dict[company][date].add(idx)\n",
    "        \n",
    "    new_results = []\n",
    "    print('voting')\n",
    "    for i in range(0,len(results)):\n",
    "        count = 0\n",
    "        company = datas[i]['company']\n",
    "        date = datas[i]['date']\n",
    "        for idx in vote_dict[company][date]:\n",
    "            count += results[idx]\n",
    "        if count < 0:  \n",
    "            new_results.append(np.float64(-1))\n",
    "        else:\n",
    "            new_results.append(np.float64(1))\n",
    "    return np.array(new_results)\n",
    "\n",
    "def accuracy(y,y2):\n",
    "    if len(y) != len(y2):\n",
    "        raise Exception(\"error\")\n",
    "    count = 0\n",
    "    for i in range(0,len(y)):\n",
    "        if y[i] == y2[i]:\n",
    "            count += 1\n",
    "    return count/len(y2)\n",
    "\n",
    "# concept_info = sn.concept('love')\n",
    "# polarity_value = sn.polarity_value('love')\n",
    "# polarity_intense = sn.polarity_intense('love')\n",
    "# moodtags = sn.moodtags('love')\n",
    "# semantics = sn.semantics('love')\n",
    "# sentics = sn.sentics('love') \n",
    " \n",
    "def sentiment_score(text):\n",
    "    t = TextBlob(text)\n",
    "    score = t.sentiment.polarity\n",
    "    return score\n",
    "    # tokens = nltk.word_tokenize(text)\n",
    "    # pos_tags = nltk.pos_tag(tokens)\n",
    "    # score = 0\n",
    "    # count = 0\n",
    "    # for word,tag in pos_tags:\n",
    "    #   if word in sn.data.keys():\n",
    "    #    score += float(sn.polarity_intense(word))\n",
    "    #    count += 1\n",
    "    #    # print(word,sn.polarity_intense(word))\n",
    "    # if count == 0: #mid\n",
    "    #   return -1 \n",
    "    # return score/count\n",
    "    \n",
    "def load_data(path):\n",
    "    workbook = xlrd.open_workbook(path)\n",
    "    worksheet = workbook.sheet_by_index(0)\n",
    "    contents = worksheet.col_values(1)\n",
    "    companies = worksheet.col_values(2)\n",
    "    prices = worksheet.col_values(3)\n",
    "    dates = worksheet.col_values(4)\n",
    "    datas = []\n",
    "    for i in tqdm(range(0,len(contents))):\n",
    "    # if '*' not in contents[i]:\n",
    "        # if companies[i] != 'Apple Inc.':\n",
    "        #     continue\n",
    "        price_list = json.loads(prices[i])   \n",
    "        if price_list[2] == price_list[1]:\n",
    "            continue\n",
    "        data = {}\n",
    "        data['content'] = contents[i]\n",
    "        sents = sent_tokenize(data['content'])\n",
    "        data['tokens'] = []\n",
    "        data['tags'] = []\n",
    "        for sent in sents:\n",
    "            token = review_to_words(sent) # 去停用词会影响词性标注吗？？\n",
    "            data['tokens'].append(token)\n",
    "            data['tags'].append(nltk.pos_tag(token))\n",
    "        data['company'] = companies[i]\n",
    "        data['rate'] = (price_list[2]-price_list[1])/price_list[1]\n",
    "        data['date'] = dates[i]\n",
    "        datas.append(data)\n",
    "\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sent_dict(datas):\n",
    "    count = {}\n",
    "\n",
    "    POS = 0\n",
    "    NEG = 0\n",
    "\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "\n",
    "    N = 0 #len of tokens\n",
    "        \n",
    "    for data in datas:\n",
    "        for tokens in data['tokens']: \n",
    "            N += len(tokens)\n",
    "            rate = data['rate'] # 选当天的股票变化判断涨跌，因为相关度当天的最高\n",
    "            if rate>0:\n",
    "                pos_count += 1\n",
    "                POS += len(tokens)\n",
    "                for token in tokens:\n",
    "                    if len(token) < 3:\n",
    "                        continue\n",
    "                    if 'not' in tokens:\n",
    "                        token = 'not_'+token\n",
    "                    if token in count.keys():\n",
    "                        count[token]['pos'] += 1\n",
    "                        count[token]['pos_rate'] += rate\n",
    "                    else:\n",
    "                        count[token] = {'pos':1,'neg':0,'pos_rate':rate,'neg_rate':0} \n",
    "\n",
    "            if rate<0:\n",
    "                neg_count += 1\n",
    "                NEG += len(tokens)\n",
    "                for token in tokens:\n",
    "                    if len(token) < 3:\n",
    "                        continue\n",
    "                    if 'not' in tokens:\n",
    "                        token = 'not_'+token\n",
    "                    if token in count.keys():\n",
    "                        count[token]['neg'] += 1\n",
    "                        count[token]['neg_rate'] -= rate\n",
    "                    else:\n",
    "                        count[token] = {'pos':0,'neg':1,'pos_rate':0,'neg_rate':-rate}\n",
    "                        ## freq\n",
    "    copy = count.copy()\n",
    "    sent_words = [] # PD>0.3情感值\n",
    "\n",
    "    freq_pos = pos_count/len(datas) \n",
    "    freq_neg = neg_count/len(datas)\n",
    "\n",
    "    # DS sent and PMI sent\n",
    "    for word,value in copy.items():\n",
    "        if value['pos']+value['neg']<len(datas)/100:\n",
    "            del count[word]\n",
    "            continue\n",
    "        freq_w_pos = value['pos']/len(datas)\n",
    "        freq_w_neg = value['neg']/len(datas)\n",
    "        freq_w = (value['pos']+value['neg'])/len(datas)\n",
    "        if freq_w_pos*N == 0:\n",
    "            PMI_w_pos = 0\n",
    "        else:\n",
    "            PMI_w_pos = np.log2(freq_w_pos*N/freq_w*freq_pos)\n",
    "        if freq_w_neg*N == 0:\n",
    "            PMI_w_neg = 0\n",
    "        else:\n",
    "            PMI_w_neg = np.log2(freq_w_neg*N/freq_w*freq_neg)\n",
    "        count[word]['PMI_sent'] = PMI_w_pos - PMI_w_neg\n",
    "\n",
    "        pos = value['pos']/len(datas)\n",
    "        neg = value['neg']/len(datas)\n",
    "        value['PD'] = (pos-neg)/(pos+neg) # polarity difference\n",
    "        if abs(value['PD']) > 0.3 and nltk.pos_tag([word])[0][1] in adj+adv:  \n",
    "            sent_words.append(word)\n",
    "        count[word]['sent'] = value['PD']*value['PD'] * np.sign(value['PD'])\n",
    "\n",
    "        pos_rate = value['pos_rate']/len(datas)\n",
    "        neg_rate = value['neg_rate']/len(datas)\n",
    "        value['PD_rate'] = (pos_rate-neg_rate)/(pos_rate+neg_rate) # polarity difference\n",
    "        count[word]['sent_rate'] = value['PD_rate']*value['PD_rate'] * np.sign(value['PD_rate'])\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news2vector(datas,count,bl_sent):\n",
    "    for idx in range(0,len(datas)):    \n",
    "        datas[idx]['DsVector'] = [0,0,0,0]\n",
    "        datas[idx]['DsVector_rate'] = [0,0,0,0]\n",
    "        datas[idx]['SnVector'] = [0,0,0,0]\n",
    "        datas[idx]['BlVector'] = [0,0,0,0]\n",
    "        datas[idx]['PmiVector'] = [0,0,0,0]\n",
    "        datas[idx]['ContextVector'] = [0,0,0,0]\n",
    "        \n",
    "        \n",
    "        for tags in datas[idx]['tags']:\n",
    "            for word,tag in tags:\n",
    "                if tag in adj:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector'][0] += count[word]['sent']\n",
    "                        datas[idx]['DsVector_rate'][0] += count[word]['sent_rate']\n",
    "                        datas[idx]['PmiVector'][0] += count[word]['PMI_sent']\n",
    "                    if word in sn.data.keys():\n",
    "                        datas[idx]['SnVector'][0] += float(sn.polarity_intense(word))\n",
    "                    if word in bl_sent.keys():\n",
    "                        datas[idx]['BlVector'][0] += bl_sent[word]\n",
    "                elif tag in adv:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector'][1] += count[word]['sent']\n",
    "                        datas[idx]['DsVector_rate'][1] += count[word]['sent_rate']\n",
    "                        datas[idx]['PmiVector'][1] += count[word]['PMI_sent']\n",
    "                    if word in sn.data.keys():\n",
    "                        datas[idx]['SnVector'][1] += float(sn.polarity_intense(word))\n",
    "                    if word in bl_sent.keys():\n",
    "                        datas[idx]['BlVector'][1] += bl_sent[word]  \n",
    "                elif tag in nn:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector'][2] = count[word]['sent']\n",
    "                        datas[idx]['DsVector_rate'][2] += count[word]['sent_rate']\n",
    "                        datas[idx]['PmiVector'][2] += count[word]['PMI_sent']\n",
    "                    if word in sn.data.keys():\n",
    "                        datas[idx]['SnVector'][2] += float(sn.polarity_intense(word))\n",
    "                    if word in bl_sent.keys():\n",
    "                        datas[idx]['BlVector'][2] += bl_sent[word]\n",
    "                elif tag in vb:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector'][3] = count[word]['sent']\n",
    "                        datas[idx]['DsVector_rate'][3] += count[word]['sent_rate']\n",
    "                        datas[idx]['PmiVector'][3] += count[word]['PMI_sent']\n",
    "                    if word in sn.data.keys():\n",
    "                        datas[idx]['SnVector'][3] += float(sn.polarity_intense(word))\n",
    "                    if word in bl_sent.keys():\n",
    "                        datas[idx]['BlVector'][3] += bl_sent[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news2vector2(datas,count,bl_sent):\n",
    "    for idx in range(0,len(datas)):\n",
    "        datas[idx]['DsVector_rate'] = [0,0,0,0]\n",
    "        for tags in datas[idx]['tags']:\n",
    "            for word,tag in tags:\n",
    "                if tag in adj:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector_rate'][0] += count[word]['sent_rate']\n",
    "\n",
    "                elif tag in adv:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector_rate'][1] += count[word]['sent_rate']\n",
    "\n",
    "                elif tag in nn:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector_rate'][2] += count[word]['sent_rate']\n",
    "\n",
    "                elif tag in vb:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector_rate'][3] += count[word]['sent_rate']\n",
    "        # datas[idx]['DsVector'] = [adv_score,adv_score,noun_score,verb_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load finish\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "datas = pickle.load(open('/home/stocksentiment/datas.pkl','rb'))\n",
    "count = pickle.load(open('/home/stocksentiment/sent_dict.pkl', 'rb'))\n",
    "print('load finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'~/Dynamic-Financial-News-Collection-and-Analysis/labeled_data.xls'\n",
    "\n",
    "# datas = load_data(path)\n",
    "# count = train_sent_dict(datas)\n",
    "\n",
    "# import pickle\n",
    "# output = open('sent_dict.pkl', 'wb')\n",
    "# input = open('sent_dict.pkl', 'rb')\n",
    "# s = pickle.dump(count, output)\n",
    "# output.close()\n",
    "# clf2 = pickle.load(input)\n",
    "# input.close()\n",
    "# print clf2.predict(X[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 新闻情感值和股价的相关度\n",
    "workbook = xlrd.open_workbook(path)\n",
    "worksheet = workbook.sheet_by_index(0)\n",
    "contents = worksheet.col_values(1)\n",
    "companies = worksheet.col_values(2)\n",
    "prices = worksheet.col_values(3)\n",
    "dates = worksheet.col_values(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rates = []\n",
    "score_list = []\n",
    "\n",
    "for i in range(0,len(contents)):\n",
    "    rate = []\n",
    "    price_list = json.loads(prices[i])\n",
    "    for idx in range(0,6):\n",
    "        rate.append((price_list[idx+1]-price_list[idx])/price_list[idx])\n",
    "    rates.append(rate)\n",
    "    score_list.append(sentiment_score(contents[i]))\n",
    "    \n",
    "## 合并一天新闻\n",
    "# for i in tqdm(range(0,len(contents))):\n",
    "#     rate = []\n",
    "#     price_list = json.loads(prices[i])\n",
    "#     for idx in range(0,6):\n",
    "#       rate.append((price_list[idx+1]-price_list[idx])/price_list[idx])\n",
    "#     if rate in rates:\n",
    "#         idx = rates.index(rate)\n",
    "#         score_list[idx] += sentiment_score(contents[i])\n",
    "#         continue\n",
    "#     rates.append(rate)\n",
    "#     score_list.append(sentiment_score(contents[i]))\n",
    "\n",
    "# 情感极性与六天内（包括）新闻涨跌比率的相关度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           rates    scores\n",
      "rates   1.000000  0.033801\n",
      "scores  0.033801  1.000000\n",
      "           rates    scores\n",
      "rates   1.000000  0.002254\n",
      "scores  0.002254  1.000000\n",
      "           rates    scores\n",
      "rates   1.000000  0.001984\n",
      "scores  0.001984  1.000000\n",
      "           rates    scores\n",
      "rates   1.000000 -0.000839\n",
      "scores -0.000839  1.000000\n",
      "           rates    scores\n",
      "rates   1.000000 -0.000409\n",
      "scores -0.000409  1.000000\n",
      "           rates    scores\n",
      "rates   1.000000 -0.002007\n",
      "scores -0.002007  1.000000\n"
     ]
    }
   ],
   "source": [
    "# 股票与新闻情感相关性\n",
    "fiveday_rate_list = []\n",
    "for i in range(0,6):\n",
    "    rate = [x[i] for x in rates]\n",
    "    data = {\n",
    "        'scores':score_list,\n",
    "        'rates':rate\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "   # print(df)\n",
    "    print(df.corr(\"kendall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-c09847d564d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \"\"\"\n\u001b[1;32m    935\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 936\u001b[0;31m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m   1589\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1573\u001b[0m                 )\n\u001b[1;32m   1574\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1576\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# wordvec\n",
    "tokens = []\n",
    "for data in datas:\n",
    "    for ts in data['tokens']:\n",
    "        tokens.append(ts)\n",
    "model = Word2Vec(sentences = tokens,min_count = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:56: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xlrd,xlwt\n",
    "import numpy as np\n",
    "# import spacy\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "sentences = [data['content'] for data in datas]\n",
    "\n",
    "bag_of_keywords = set(['fail', 'success', 'win', 'drop', 'rise', 'shrink', 'jump', 'gain', 'down', 'up'])\n",
    "stop = False\n",
    "bok_size = 100\n",
    "for i in range(10):\n",
    "\tnew_words = []\n",
    "\tif stop:break\n",
    "\tfor k in bag_of_keywords:\n",
    "\t\tif k in model.wv.vocab.keys():# wv = wordvector\n",
    "\t\t\tnew_words.extend(model.most_similar(k))\n",
    "for n in new_words:\n",
    "\tif n[0].islower() and len(n[0])>3 and n[0].isalpha():\n",
    "\t\tbag_of_keywords.add(n[0])\n",
    "\t\tif len(bag_of_keywords) == bok_size:\n",
    "\t\t\tstop = True\n",
    "\t\t\tbreak\n",
    "\n",
    "'''fit():计算数据的参数，\\mu（均值），\\sigma（标准差），并存储在对象中（例如实例化的CountVectorizer()等）。\n",
    "transform():将这些参数应用到数据集，进行标准化（尺度化）。'''\n",
    "\n",
    "## Bag of keywords 统计词语的两个api\n",
    "bag_of_keywords = np.array(list(bag_of_keywords))\n",
    "bok_tfidf = TfidfVectorizer(lowercase = False, min_df = 1, vocabulary=bag_of_keywords)\n",
    "X_bok_tfidf = bok_tfidf.fit_transform(sentences)\n",
    "X_bok_tfidf = X_bok_tfidf.toarray()\n",
    "\n",
    "## Category tag\n",
    "category_tags = set(['published','presented','unveil','investment','bankrupt','acquisition','government'\n",
    "                     'sue','lawsuit','highlights'])\n",
    "stop = False\n",
    "cate_size = 100\n",
    "\n",
    "for _ in range(10):\n",
    "    new_words = []\n",
    "    if stop:break\n",
    "    for k in category_tags:\n",
    "        if k in model.wv.vocab.keys():\n",
    "            new_words.extend(model.most_similar(k))\n",
    "    for n in new_words:\n",
    "        if n[0].islower() and len(n[0])>3 and n[0].isalpha():\n",
    "            category_tags.add(n[0])\n",
    "            if len(category_tags) == cate_size:\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "\n",
    "category_tags = np.array(list(category_tags))\n",
    "\n",
    "ct_tfidf = TfidfVectorizer(lowercase = False, min_df = 1, vocabulary = category_tags)\n",
    "X_ct_idf = ct_tfidf.fit_transform(sentences)\n",
    "X_ct_idf = X_ct_idf.toarray()\n",
    "\n",
    "full_tfidf = TfidfVectorizer(lowercase=False, min_df = 1,vocabulary=bag_of_keywords,use_idf=False)\n",
    "X_full_tfidf = full_tfidf.fit_transform(sentences)\n",
    "X_full_tfidf = X_full_tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.34411929, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.60995393, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.4684358 ,\n",
       "       0.        , 0.        , 0.34388431, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.26456707, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.31914554, 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ct_idf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_full_tfidf==X_bok_tfidf).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_bok_tfidf\n",
      "accuracy： 0.5206199757735932\n",
      "recall： 0.4999558721922025\n",
      "precision： 0.497998753016745\n",
      "X_ct_idf\n",
      "accuracy： 0.5209503358660941\n",
      "recall： 0.5000862740324018\n",
      "accuracy： 0.52898770329963\n",
      "X_full_tfidf\n",
      "accuracy： 0.5209641008699483\n",
      "recall： 0.5000856567806256\n",
      "accuracy： 0.5535724715342379\n"
     ]
    }
   ],
   "source": [
    "# x_bok RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('X_bok_tfidf')\n",
    "X = X_bok_tfidf\n",
    "train_x,test_x,train_y,test_y=model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('accuracy：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('recall：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('precision：',precision_score(test_y, clf.predict(test_x), average='macro'))\n",
    "    \n",
    "print('X_ct_idf')\n",
    "X = X_ct_idf\n",
    "train_x,test_x,train_y,test_y=model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('accuracy：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('recall：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('accuracy：',precision_score(test_y, clf.predict(test_x), average='macro'))\n",
    "\n",
    "print('X_full_tfidf')\n",
    "X = X_full_tfidf\n",
    "train_x,test_x,train_y,test_y=model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('accuracy：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('recall：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('accuracy：',precision_score(test_y, clf.predict(test_x), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_bok_tfidf\n",
      "accuracy： 0.5072266270234556\n",
      "recall： 0.49985112388246794\n",
      "precision： 0.49983023404856497\n",
      "voting\n",
      "X_ct_idf\n",
      "accuracy： 0.5209503358660941\n",
      "recall： 0.5000862740324018\n",
      "accuracy： 0.52898770329963\n",
      "\n",
      "X_full_tfidf\n",
      "accuracy： 0.5209641008699483\n",
      "recall： 0.5000856567806256\n",
      "accuracy： 0.5535724715342379\n"
     ]
    }
   ],
   "source": [
    "# x_bok RandomForestClassifier\n",
    "print('X_bok_tfidf')\n",
    "X = X_bok_tfidf\n",
    "train_x,test_x,train_y,test_y=model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "clf = GaussianNB()\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('accuracy：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('recall：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('precision：',precision_score(test_y, clf.predict(test_x), average='macro'))\n",
    "\n",
    "vote_predict_y = vote(predict_y,datas[-len(test_x):])\n",
    "vote_recall = recall_score(test_y,vote_predict_y,average = 'macro')\n",
    "vote_precision = precision_score(test_y, vote_predict_y, average='macro')\n",
    "\n",
    "    \n",
    "print('X_ct_idf')\n",
    "X = X_ct_idf\n",
    "train_x,test_x,train_y,test_y=model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('accuracy：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('recall：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('accuracy：',precision_score(test_y, clf.predict(test_x), average='macro'))\n",
    "\n",
    "print('')\n",
    "\n",
    "print('X_full_tfidf')\n",
    "X = X_full_tfidf\n",
    "train_x,test_x,train_y,test_y=model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('accuracy：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('recall：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('accuracy：',precision_score(test_y, clf.predict(test_x), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-1f9437ea8cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpos_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mneg_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mpos_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_words' is not defined"
     ]
    }
   ],
   "source": [
    "# neg pos 词\n",
    "pos_words = {}\n",
    "neg_words = {}\n",
    "for word in sent_words:\n",
    "   if count[word]['sent'] > 0:\n",
    "      pos_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "   else:\n",
    "      neg_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "\n",
    "output_cloud(pos_words,'pos')\n",
    "output_cloud(neg_words,'neg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PD': 0.022119462732212183,\n",
       " 'PD_rate': 0.04246705264244967,\n",
       " 'PMI_sent': 0.12259288489754994,\n",
       " 'neg': 371555,\n",
       " 'neg_rate': 5250.115713877801,\n",
       " 'pos': 388364,\n",
       " 'pos_rate': 5715.806092502332,\n",
       " 'sent': 0.0004892706315617237,\n",
       " 'sent_rate': 0.0018034505601365913}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count['company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg pos 词\n",
    "pos_words = {}\n",
    "neg_words = {}\n",
    "for word in count.keys():\n",
    "   if count[word]['sent'] > 0:\n",
    "      pos_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "   else:\n",
    "      neg_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "\n",
    "output_cloud(pos_words,'pos')\n",
    "output_cloud(neg_words,'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cloud(pos_words,'pos')\n",
    "output_cloud(neg_words,'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 求于bl词典的覆盖率\n",
    "bl_sent = {}\n",
    "bl_pos = my_read('/home/stocksentiment/Dynamic-Financial-News-Collection-and-Analysis/sentiment_analysis/bl/positive.txt')  # 4783\n",
    "bl_neg = my_read('/home/stocksentiment/Dynamic-Financial-News-Collection-and-Analysis/sentiment_analysis/bl/negative.txt')  # 2006\n",
    "\n",
    "\n",
    "for word in bl_pos:\n",
    "    bl_sent[word] = 1\n",
    "for word in bl_pos:\n",
    "    bl_sent[word] = -1\n",
    "# pc = 0\n",
    "# for word in pos_words:\n",
    "#    if word in bl_pos:\n",
    "#       pc += 1\n",
    "# pos_accuracy = pc/len(pos_words)  # 0.2857142857142857\n",
    "\n",
    "# nc = 0\n",
    "# for word in neg_words:\n",
    "#    if word in bl_neg:\n",
    "#       nc += 1\n",
    "# neg_accuracy = nc/len(neg_words)  # 0.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## context sentiment dict\n",
    "# sent_words = [word.lower() for word in sent_words]\n",
    "# feature_words = {}\n",
    "# sentiment_feature = {}\n",
    "\n",
    "# for data in tqdm(datas):\n",
    "#     for tags in data['tags']:\n",
    "#         for word,tag in tags:\n",
    "#             if tag not in nn or len(word)<3: # vb+nn\n",
    "#                 continue\n",
    "#             # word = stem_and_check(word)\n",
    "#             if word not in feature_words.keys():\n",
    "#                 feature_words[word] = 1\n",
    "#             else:\n",
    "#                 feature_words[word] += 1\n",
    "\n",
    "# # avg_f = sum([item[1] for item in feature_words.items()])/len(feature_words.keys())\n",
    "# res = sorted(feature_words.items(),key=lambda feature_words:feature_words[1],reverse=True)\n",
    "# words = res[:400]\n",
    "# # for word,value in tqdm(copy.items()):\n",
    "# #     if value<avg_f+200:\n",
    "# #         del feature_words[word]\n",
    "\n",
    "# feature_words = [inf.singularize(word).lower() for word,freq in words]\n",
    "\n",
    "# sf_len = 0\n",
    "# for data in tqdm(datas):\n",
    "#     rate = data['rate']\n",
    "#     for tokens in data['tokens']:\n",
    "#         token_dict = {}\n",
    "#         for token in tokens:\n",
    "#             token_dict[inf.singularize(token).lower()] = token\n",
    "#         _tokens = [inf.singularize(token).lower() for token in tokens]\n",
    "\n",
    "#         for w in list(set(sent_words).intersection(set(_tokens))):\n",
    "#             for f in list(set(feature_words).intersection(set(_tokens))):\n",
    "#                 if f != w:\n",
    "#                     if abs(_tokens.index(w)-_tokens.index(f))<3 and ',' not in data['content'][min(data['content'].index(token_dict[f]),data['content'].index(token_dict[w])):max(data['content'].index(token_dict[f]),data['content'].index(token_dict[w]))]:\n",
    "#                         sf_len += 1\n",
    "#                         if f not in sentiment_feature.keys():\n",
    "#                             sentiment_feature[f] = {}\n",
    "#                             if rate > 0:\n",
    "#                                 sentiment_feature[f][w] = {'pos':1,'neg':0}\n",
    "#                             if rate < 0:\n",
    "#                                 sentiment_feature[f][w] = {'pos':0,'neg':1}\n",
    "#                         else:\n",
    "#                             if w not in sentiment_feature[f].keys():\n",
    "#                                 sentiment_feature[f][w] = {'pos':0,'neg':0}\n",
    "#                             if rate > 0:\n",
    "#                                 sentiment_feature[f][w]['pos'] += 1\n",
    "#                             if rate < 0:\n",
    "#                                 sentiment_feature[f][w]['neg'] += 1\n",
    "\n",
    "# avg_sf = sf_len/len(sentiment_feature.keys())\n",
    "# copy = sentiment_feature.copy()\n",
    "\n",
    "# for f,v in tqdm(sentiment_feature.items()):\n",
    "#     for w,value in v.items():\n",
    "#         if value['pos']+value['neg']<avg_sf: #avg_sf\n",
    "#             # print(f,w,value)\n",
    "#             # del sentiment_feature[f][w]\n",
    "#             sentiment_feature[f][w]['sent'] = 0\n",
    "#             continue\n",
    "#         pos = value['pos']/POS\n",
    "#         neg = value['neg']/NEG\n",
    "        \n",
    "#         value['PD'] = (pos-neg)/(pos+neg) # polarity difference\n",
    "#         sentiment_feature[f][w]['sent'] = value['PD'] * value['PD'] * np.sign(value['PD'])\n",
    "\n",
    "# res = sorted(sentiment_feature.items(),key=lambda sentiment_feature:sentiment_feature[1]['sent'],reverse=False)\n",
    "\n",
    "## company word\n",
    "# company_pos = {}\n",
    "# company_neg = {}\n",
    "# for key,value in sentiment_feature.items():\n",
    "#    if 'company' in key:\n",
    "#       if sentiment_feature[key]['sent'] > 0:\n",
    "#          company_pos[key.split('_')[0]] = sentiment_feature[key]['pos']\n",
    "#       else:\n",
    "#          company_neg[key.split('_')[0]] = sentiment_feature[key]['neg']\n",
    "\n",
    "# output_cloud(company_pos,'company_pos')\n",
    "# output_cloud(company_neg,'company_neg')\n",
    "\n",
    "\n",
    "# for r in res[:30]:\n",
    "#    print(r[0],r[1]['sent'],r[1]['pos']+r[1]['neg'])\n",
    "#    print(' ')\n",
    "\n",
    "# print(' ')\n",
    "# print('========================')\n",
    "\n",
    "# for r in res[-40:]:\n",
    "#    print(r[0],r[1]['sent'],r[1]['pos'],r[1]['neg'])\n",
    "#    print(' ')\n",
    "\n",
    "# 展示\n",
    "# pos_res = {}\n",
    "# for r in res:\n",
    "#     if r[1]['sent'] == 1.0:\n",
    "#         pos_res[r[0]] = r[1]['pos']+r[1]['neg']\n",
    "# pos_res = sorted(pos_res.items(),key=lambda pos_res:pos_res[1],reverse=True)\n",
    "# for r in pos_res[:20]:\n",
    "#     print(r[0],'1.0',r[1])\n",
    "#     print(' ')\n",
    "\n",
    "# print(' ')\n",
    "# print('========================')\n",
    "# print(' ')\n",
    "\n",
    "# neg_res = {}\n",
    "# for r in res:\n",
    "#     if r[1]['sent'] == -1.0:\n",
    "#         neg_res[r[0]] = r[1]['pos']+r[1]['neg']\n",
    "# neg_res = sorted(neg_res.items(),key=lambda neg_res:neg_res[1],reverse=True)\n",
    "# for r in neg_res[:20]:\n",
    "#     print(r[0],'-1.0',r[1])\n",
    "#     print(' ')\n",
    "# for sf,value in sentiment_feature.items():\n",
    "#    if value['pos']+value['neg'] > avg_sf:\n",
    "#       print(sf,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 363237/363237 [1:15:57<00:00, 27.32it/s] \n"
     ]
    }
   ],
   "source": [
    "news2vector(datas,count,bl_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump finish\n"
     ]
    }
   ],
   "source": [
    "output = open('datas.pkl', 'wb')\n",
    "pickle.dump(datas, output)\n",
    "print('dump finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BlVector': [0, 0, -4, 0],\n",
       " 'ContextVector': [0, 0, 0, 0],\n",
       " 'DsVector': [0.009382414055471773,\n",
       "  0.003466796193975887,\n",
       "  0.00011491759888645986,\n",
       "  0.0008279551668078367],\n",
       " 'DsVector_rate': [0.04473495161500582,\n",
       "  0.01280244453155389,\n",
       "  0.2797177700726252,\n",
       "  0.10969579071672703],\n",
       " 'PmiVector': [2.3897680592562054,\n",
       "  0.9182778690101259,\n",
       "  12.363459195184863,\n",
       "  4.756092484475477],\n",
       " 'SnVector': [-0.6450000000000007,\n",
       "  -2.5869999999999997,\n",
       "  12.297000000000002,\n",
       "  1.055],\n",
       " 'company': 'Ameriprise Financial',\n",
       " 'content': 'NEW YORK  (Reuters) - H&R Block Inc will pay as much as $20.2 million to settle a New York lawsuit accusing it of fraudulently marketing retirement accounts that caused hundreds of thousands of mostly lower-income clients to lose money.    New York Attorney General Andrew Cuomo said the accord calls for the largest U.S. tax preparer to refund $11.4 million to $19.4 million of fees to customers nationwide who opened one of its Express IRAs, a type of individual retirement account. H&R Block will also pay $750,000 in fines and other costs to the state, and convert Express IRAs into new retirement accounts that do not charge fees, Cuomo said. The size of the refund depends on the number of claims made, he said. The attorney general said H&R Block also settled private class-action lawsuits based on the same allegations, and which were pending in the federal court in Kansas City, Missouri, where the company is based. Norman Siegel, a lawyer representing plaintiffs in the private litigation, in an e-mail declined to discuss terms of that settlement, but said he expects to file papers with the court in the next few days. New York had accused H&R Block of steering more than 600,000 customers to Express IRAs, without disclosing hidden fees that wiped out the interest that 85 percent of them could earn. Eliot Spitzer, Cuomo\\'s predecessor, had first sued H&R Block over the marketing of Express IRAs in March 2006. \"H&R Block\\'s aggressive peddling of fee-laden retirement accounts that were virtually guaranteed to lose money needlessly cost families across the country millions of their hard-earned dollars,\" Cuomo said in a statement on Monday. Gene King, an H&R Block spokesman, called the New York settlement \"satisfactory for all parties.\" He had no immediate comment on the class-action settlement. Spitzer originally sought $250 million of civil penalties and other remedies. His lawsuit had said the median Express IRA account had a $323 balance, too low for investors to offset such charges as $10 annual maintenance fees, $15 set-up fees, $15 \"re-contribution\" fees and $25 termination fees. Among the defendants in the New York case was H&R Block Financial Advisors Inc, which the company sold in 2008 to Ameriprise Financial Inc. Ameriprise did not return a call seeking comment. In late afternoon trading, H&R Block shares were up 7 cents at $22.69 on the New York Stock Exchange. The case is New York v. H&R Block Inc, New York State Supreme Court, No. 401110/2006. (Reporting by Jonathan Stempel; Editing by  Andre Grenon  and  Richard Chang )',\n",
       " 'date': '2010-01-04',\n",
       " 'rate': 0.023319966907406453,\n",
       " 'tags': [[('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('pay', 'NN'),\n",
       "   ('much', 'JJ'),\n",
       "   ('million', 'CD'),\n",
       "   ('settle', 'JJ'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('lawsuit', 'NN'),\n",
       "   ('accusing', 'VBG'),\n",
       "   ('fraudulently', 'RB'),\n",
       "   ('marketing', 'VBG'),\n",
       "   ('retirement', 'NN'),\n",
       "   ('accounts', 'NNS'),\n",
       "   ('caused', 'VBD'),\n",
       "   ('hundreds', 'NNS'),\n",
       "   ('thousands', 'NNS'),\n",
       "   ('mostly', 'RB'),\n",
       "   ('lower', 'JJR'),\n",
       "   ('income', 'NN'),\n",
       "   ('clients', 'NNS'),\n",
       "   ('lose', 'VB'),\n",
       "   ('money', 'NN')],\n",
       "  [('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('Attorney', 'NNP'),\n",
       "   ('General', 'NNP'),\n",
       "   ('Andrew', 'NNP'),\n",
       "   ('Cuomo', 'NNP'),\n",
       "   ('said', 'VBD'),\n",
       "   ('accord', 'NN'),\n",
       "   ('calls', 'VBZ'),\n",
       "   ('largest', 'JJS'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('tax', 'NN'),\n",
       "   ('preparer', 'NN'),\n",
       "   ('refund', 'NN'),\n",
       "   ('million', 'CD'),\n",
       "   ('million', 'CD'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('customers', 'NNS'),\n",
       "   ('nationwide', 'RB'),\n",
       "   ('opened', 'VBD'),\n",
       "   ('one', 'CD'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRAs', 'NNPS'),\n",
       "   ('type', 'NN'),\n",
       "   ('individual', 'JJ'),\n",
       "   ('retirement', 'NN'),\n",
       "   ('account', 'NN')],\n",
       "  [('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('also', 'RB'),\n",
       "   ('pay', 'VBP'),\n",
       "   ('fines', 'NNS'),\n",
       "   ('costs', 'NNS'),\n",
       "   ('state', 'NN'),\n",
       "   ('convert', 'NN'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRAs', 'NNP'),\n",
       "   ('new', 'JJ'),\n",
       "   ('retirement', 'NN'),\n",
       "   ('accounts', 'NNS'),\n",
       "   ('charge', 'NN'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('Cuomo', 'NNP'),\n",
       "   ('said', 'VBD')],\n",
       "  [('The', 'DT'),\n",
       "   ('size', 'NN'),\n",
       "   ('refund', 'NN'),\n",
       "   ('depends', 'VBZ'),\n",
       "   ('number', 'NN'),\n",
       "   ('claims', 'NNS'),\n",
       "   ('made', 'VBN'),\n",
       "   ('said', 'VBD')],\n",
       "  [('The', 'DT'),\n",
       "   ('attorney', 'NN'),\n",
       "   ('general', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('also', 'RB'),\n",
       "   ('settled', 'VBD'),\n",
       "   ('private', 'JJ'),\n",
       "   ('class', 'NN'),\n",
       "   ('action', 'NN'),\n",
       "   ('lawsuits', 'NNS'),\n",
       "   ('based', 'VBN'),\n",
       "   ('allegations', 'NNS'),\n",
       "   ('pending', 'VBG'),\n",
       "   ('federal', 'JJ'),\n",
       "   ('court', 'NN'),\n",
       "   ('Kansas', 'NNP'),\n",
       "   ('City', 'NNP'),\n",
       "   ('Missouri', 'NNP'),\n",
       "   ('company', 'NN'),\n",
       "   ('based', 'VBN')],\n",
       "  [('Norman', 'NNP'),\n",
       "   ('Siegel', 'NNP'),\n",
       "   ('lawyer', 'NN'),\n",
       "   ('representing', 'VBG'),\n",
       "   ('plaintiffs', 'NNS'),\n",
       "   ('private', 'JJ'),\n",
       "   ('litigation', 'NN'),\n",
       "   ('e', 'FW'),\n",
       "   ('mail', 'NN'),\n",
       "   ('declined', 'VBD'),\n",
       "   ('discuss', 'JJ'),\n",
       "   ('terms', 'NNS'),\n",
       "   ('settlement', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('expects', 'VBZ'),\n",
       "   ('file', 'JJ'),\n",
       "   ('papers', 'NNS'),\n",
       "   ('court', 'NN'),\n",
       "   ('next', 'JJ'),\n",
       "   ('days', 'NNS')],\n",
       "  [('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('accused', 'VBD'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('steering', 'VBG'),\n",
       "   ('customers', 'NNS'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRAs', 'NNP'),\n",
       "   ('without', 'IN'),\n",
       "   ('disclosing', 'VBG'),\n",
       "   ('hidden', 'NN'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('wiped', 'VBD'),\n",
       "   ('interest', 'NN'),\n",
       "   ('percent', 'NN'),\n",
       "   ('could', 'MD'),\n",
       "   ('earn', 'VB')],\n",
       "  [('Eliot', 'NNP'),\n",
       "   ('Spitzer', 'NNP'),\n",
       "   ('Cuomo', 'NNP'),\n",
       "   ('predecessor', 'NN'),\n",
       "   ('first', 'RB'),\n",
       "   ('sued', 'VBN'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('marketing', 'NN'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRAs', 'NNP'),\n",
       "   ('March', 'NNP')],\n",
       "  [('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('aggressive', 'JJ'),\n",
       "   ('peddling', 'NN'),\n",
       "   ('fee', 'NN'),\n",
       "   ('laden', 'JJ'),\n",
       "   ('retirement', 'NN'),\n",
       "   ('accounts', 'NNS'),\n",
       "   ('virtually', 'RB'),\n",
       "   ('guaranteed', 'VBP'),\n",
       "   ('lose', 'JJ'),\n",
       "   ('money', 'NN'),\n",
       "   ('needlessly', 'RB'),\n",
       "   ('cost', 'VBN'),\n",
       "   ('families', 'NNS'),\n",
       "   ('across', 'IN'),\n",
       "   ('country', 'NN'),\n",
       "   ('millions', 'NNS'),\n",
       "   ('hard', 'RB'),\n",
       "   ('earned', 'VBD'),\n",
       "   ('dollars', 'NNS'),\n",
       "   ('Cuomo', 'NNP'),\n",
       "   ('said', 'VBD'),\n",
       "   ('statement', 'NN'),\n",
       "   ('Monday', 'NNP')],\n",
       "  [('Gene', 'NNP'),\n",
       "   ('King', 'NNP'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('spokesman', 'NN'),\n",
       "   ('called', 'VBD'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('settlement', 'NN'),\n",
       "   ('satisfactory', 'NN'),\n",
       "   ('parties', 'NNS')],\n",
       "  [('He', 'PRP'),\n",
       "   ('immediate', 'JJ'),\n",
       "   ('comment', 'NN'),\n",
       "   ('class', 'NN'),\n",
       "   ('action', 'NN'),\n",
       "   ('settlement', 'NN')],\n",
       "  [('Spitzer', 'NNP'),\n",
       "   ('originally', 'RB'),\n",
       "   ('sought', 'VBD'),\n",
       "   ('million', 'CD'),\n",
       "   ('civil', 'JJ'),\n",
       "   ('penalties', 'NNS'),\n",
       "   ('remedies', 'NNS')],\n",
       "  [('His', 'PRP$'),\n",
       "   ('lawsuit', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('median', 'JJ'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRA', 'NNP'),\n",
       "   ('account', 'NN'),\n",
       "   ('balance', 'NN'),\n",
       "   ('low', 'JJ'),\n",
       "   ('investors', 'NNS'),\n",
       "   ('offset', 'VBD'),\n",
       "   ('charges', 'NNS'),\n",
       "   ('annual', 'JJ'),\n",
       "   ('maintenance', 'NN'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('set', 'VBP'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('contribution', 'NN'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('termination', 'NN'),\n",
       "   ('fees', 'NNS')],\n",
       "  [('Among', 'IN'),\n",
       "   ('defendants', 'NNS'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('case', 'NN'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('Financial', 'NNP'),\n",
       "   ('Advisors', 'NNPS'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('company', 'NN'),\n",
       "   ('sold', 'VBD'),\n",
       "   ('Ameriprise', 'NNP'),\n",
       "   ('Financial', 'NNP'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('Ameriprise', 'NNP'),\n",
       "   ('return', 'NN'),\n",
       "   ('call', 'NN'),\n",
       "   ('seeking', 'VBG'),\n",
       "   ('comment', 'NN')],\n",
       "  [('In', 'IN'),\n",
       "   ('late', 'JJ'),\n",
       "   ('afternoon', 'NN'),\n",
       "   ('trading', 'NN'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('shares', 'NNS'),\n",
       "   ('cents', 'NNS'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('Stock', 'NNP'),\n",
       "   ('Exchange', 'NNP')],\n",
       "  [('The', 'DT'),\n",
       "   ('case', 'NN'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('v', 'NN'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('State', 'NNP'),\n",
       "   ('Supreme', 'NNP'),\n",
       "   ('Court', 'NNP'),\n",
       "   ('No', 'NNP')],\n",
       "  [],\n",
       "  [('Reporting', 'VBG'),\n",
       "   ('Jonathan', 'NNP'),\n",
       "   ('Stempel', 'NNP'),\n",
       "   ('Editing', 'NNP'),\n",
       "   ('Andre', 'NNP'),\n",
       "   ('Grenon', 'NNP'),\n",
       "   ('Richard', 'NNP'),\n",
       "   ('Chang', 'NNP')]],\n",
       " 'tokens': [['H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'Inc',\n",
       "   'pay',\n",
       "   'much',\n",
       "   'million',\n",
       "   'settle',\n",
       "   'New',\n",
       "   'York',\n",
       "   'lawsuit',\n",
       "   'accusing',\n",
       "   'fraudulently',\n",
       "   'marketing',\n",
       "   'retirement',\n",
       "   'accounts',\n",
       "   'caused',\n",
       "   'hundreds',\n",
       "   'thousands',\n",
       "   'mostly',\n",
       "   'lower',\n",
       "   'income',\n",
       "   'clients',\n",
       "   'lose',\n",
       "   'money'],\n",
       "  ['New',\n",
       "   'York',\n",
       "   'Attorney',\n",
       "   'General',\n",
       "   'Andrew',\n",
       "   'Cuomo',\n",
       "   'said',\n",
       "   'accord',\n",
       "   'calls',\n",
       "   'largest',\n",
       "   'U',\n",
       "   'S',\n",
       "   'tax',\n",
       "   'preparer',\n",
       "   'refund',\n",
       "   'million',\n",
       "   'million',\n",
       "   'fees',\n",
       "   'customers',\n",
       "   'nationwide',\n",
       "   'opened',\n",
       "   'one',\n",
       "   'Express',\n",
       "   'IRAs',\n",
       "   'type',\n",
       "   'individual',\n",
       "   'retirement',\n",
       "   'account'],\n",
       "  ['H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'also',\n",
       "   'pay',\n",
       "   'fines',\n",
       "   'costs',\n",
       "   'state',\n",
       "   'convert',\n",
       "   'Express',\n",
       "   'IRAs',\n",
       "   'new',\n",
       "   'retirement',\n",
       "   'accounts',\n",
       "   'charge',\n",
       "   'fees',\n",
       "   'Cuomo',\n",
       "   'said'],\n",
       "  ['The', 'size', 'refund', 'depends', 'number', 'claims', 'made', 'said'],\n",
       "  ['The',\n",
       "   'attorney',\n",
       "   'general',\n",
       "   'said',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'also',\n",
       "   'settled',\n",
       "   'private',\n",
       "   'class',\n",
       "   'action',\n",
       "   'lawsuits',\n",
       "   'based',\n",
       "   'allegations',\n",
       "   'pending',\n",
       "   'federal',\n",
       "   'court',\n",
       "   'Kansas',\n",
       "   'City',\n",
       "   'Missouri',\n",
       "   'company',\n",
       "   'based'],\n",
       "  ['Norman',\n",
       "   'Siegel',\n",
       "   'lawyer',\n",
       "   'representing',\n",
       "   'plaintiffs',\n",
       "   'private',\n",
       "   'litigation',\n",
       "   'e',\n",
       "   'mail',\n",
       "   'declined',\n",
       "   'discuss',\n",
       "   'terms',\n",
       "   'settlement',\n",
       "   'said',\n",
       "   'expects',\n",
       "   'file',\n",
       "   'papers',\n",
       "   'court',\n",
       "   'next',\n",
       "   'days'],\n",
       "  ['New',\n",
       "   'York',\n",
       "   'accused',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'steering',\n",
       "   'customers',\n",
       "   'Express',\n",
       "   'IRAs',\n",
       "   'without',\n",
       "   'disclosing',\n",
       "   'hidden',\n",
       "   'fees',\n",
       "   'wiped',\n",
       "   'interest',\n",
       "   'percent',\n",
       "   'could',\n",
       "   'earn'],\n",
       "  ['Eliot',\n",
       "   'Spitzer',\n",
       "   'Cuomo',\n",
       "   'predecessor',\n",
       "   'first',\n",
       "   'sued',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'marketing',\n",
       "   'Express',\n",
       "   'IRAs',\n",
       "   'March'],\n",
       "  ['H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'aggressive',\n",
       "   'peddling',\n",
       "   'fee',\n",
       "   'laden',\n",
       "   'retirement',\n",
       "   'accounts',\n",
       "   'virtually',\n",
       "   'guaranteed',\n",
       "   'lose',\n",
       "   'money',\n",
       "   'needlessly',\n",
       "   'cost',\n",
       "   'families',\n",
       "   'across',\n",
       "   'country',\n",
       "   'millions',\n",
       "   'hard',\n",
       "   'earned',\n",
       "   'dollars',\n",
       "   'Cuomo',\n",
       "   'said',\n",
       "   'statement',\n",
       "   'Monday'],\n",
       "  ['Gene',\n",
       "   'King',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'spokesman',\n",
       "   'called',\n",
       "   'New',\n",
       "   'York',\n",
       "   'settlement',\n",
       "   'satisfactory',\n",
       "   'parties'],\n",
       "  ['He', 'immediate', 'comment', 'class', 'action', 'settlement'],\n",
       "  ['Spitzer',\n",
       "   'originally',\n",
       "   'sought',\n",
       "   'million',\n",
       "   'civil',\n",
       "   'penalties',\n",
       "   'remedies'],\n",
       "  ['His',\n",
       "   'lawsuit',\n",
       "   'said',\n",
       "   'median',\n",
       "   'Express',\n",
       "   'IRA',\n",
       "   'account',\n",
       "   'balance',\n",
       "   'low',\n",
       "   'investors',\n",
       "   'offset',\n",
       "   'charges',\n",
       "   'annual',\n",
       "   'maintenance',\n",
       "   'fees',\n",
       "   'set',\n",
       "   'fees',\n",
       "   'contribution',\n",
       "   'fees',\n",
       "   'termination',\n",
       "   'fees'],\n",
       "  ['Among',\n",
       "   'defendants',\n",
       "   'New',\n",
       "   'York',\n",
       "   'case',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'Financial',\n",
       "   'Advisors',\n",
       "   'Inc',\n",
       "   'company',\n",
       "   'sold',\n",
       "   'Ameriprise',\n",
       "   'Financial',\n",
       "   'Inc',\n",
       "   'Ameriprise',\n",
       "   'return',\n",
       "   'call',\n",
       "   'seeking',\n",
       "   'comment'],\n",
       "  ['In',\n",
       "   'late',\n",
       "   'afternoon',\n",
       "   'trading',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'shares',\n",
       "   'cents',\n",
       "   'New',\n",
       "   'York',\n",
       "   'Stock',\n",
       "   'Exchange'],\n",
       "  ['The',\n",
       "   'case',\n",
       "   'New',\n",
       "   'York',\n",
       "   'v',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'Inc',\n",
       "   'New',\n",
       "   'York',\n",
       "   'State',\n",
       "   'Supreme',\n",
       "   'Court',\n",
       "   'No'],\n",
       "  [],\n",
       "  ['Reporting',\n",
       "   'Jonathan',\n",
       "   'Stempel',\n",
       "   'Editing',\n",
       "   'Andre',\n",
       "   'Grenon',\n",
       "   'Richard',\n",
       "   'Chang']]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_dict = {}\n",
    "date_set = set([data['date'] for data in datas])\n",
    "company_set = set([data['company'] for data in datas])\n",
    "for company in company_set:\n",
    "    vote_dict[company] = {}\n",
    "    for date in date_set:\n",
    "        vote_dict[company][date] = set()\n",
    "for idx in range(0,len(datas)):\n",
    "    date = datas[idx]['date']\n",
    "    company = datas[idx]['company']         \n",
    "    vote_dict[company][date].add(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2017-10-13': set(),\n",
       " '2016-07-08': {302925, 302947, 302948, 303008},\n",
       " '2017-07-20': {338115, 338196, 338207},\n",
       " '2014-05-02': set(),\n",
       " '2013-09-12': set(),\n",
       " '2014-10-24': set(),\n",
       " '2016-05-31': set(),\n",
       " '2015-03-16': set(),\n",
       " '2012-05-08': set(),\n",
       " '2012-10-22': set(),\n",
       " '2017-11-24': set(),\n",
       " '2014-08-15': set(),\n",
       " '2017-01-05': set(),\n",
       " '2015-06-16': set(),\n",
       " '2014-04-10': set(),\n",
       " '2012-10-25': {146601},\n",
       " '2017-11-09': set(),\n",
       " '2016-11-15': set(),\n",
       " '2015-07-17': set(),\n",
       " '2011-09-13': set(),\n",
       " '2015-01-06': set(),\n",
       " '2015-04-15': set(),\n",
       " '2017-01-23': set(),\n",
       " '2010-01-07': set(),\n",
       " '2010-12-28': set(),\n",
       " '2017-06-28': set(),\n",
       " '2017-11-01': set(),\n",
       " '2012-02-09': set(),\n",
       " '2010-11-01': set(),\n",
       " '2014-02-18': set(),\n",
       " '2011-09-19': set(),\n",
       " '2015-10-08': set(),\n",
       " '2012-06-06': set(),\n",
       " '2013-10-02': set(),\n",
       " '2011-03-15': set(),\n",
       " '2012-06-20': set(),\n",
       " '2013-11-15': set(),\n",
       " '2012-05-23': set(),\n",
       " '2012-09-12': set(),\n",
       " '2013-03-06': set(),\n",
       " '2013-08-15': set(),\n",
       " '2015-09-10': set(),\n",
       " '2018-03-12': set(),\n",
       " '2016-02-18': set(),\n",
       " '2012-01-26': {94679, 94947},\n",
       " '2017-02-06': set(),\n",
       " '2016-08-16': set(),\n",
       " '2012-06-08': set(),\n",
       " '2011-10-21': set(),\n",
       " '2012-01-24': {94096},\n",
       " '2018-02-28': set(),\n",
       " '2014-01-29': set(),\n",
       " '2017-01-20': set(),\n",
       " '2017-10-09': set(),\n",
       " '2016-08-08': set(),\n",
       " '2017-07-07': set(),\n",
       " '2011-12-30': set(),\n",
       " '2016-03-04': set(),\n",
       " '2015-10-16': set(),\n",
       " '2017-01-10': set(),\n",
       " '2012-01-19': set(),\n",
       " '2010-04-21': set(),\n",
       " '2013-06-05': set(),\n",
       " '2015-02-19': set(),\n",
       " '2015-11-06': set(),\n",
       " '2013-06-07': set(),\n",
       " '2012-09-04': set(),\n",
       " '2012-09-18': {139526},\n",
       " '2017-02-02': {322265},\n",
       " '2011-12-29': set(),\n",
       " '2014-03-18': set(),\n",
       " '2014-09-08': set(),\n",
       " '2013-09-10': set(),\n",
       " '2014-08-08': set(),\n",
       " '2017-07-14': set(),\n",
       " '2017-05-09': {331460},\n",
       " '2011-01-07': set(),\n",
       " '2013-02-08': set(),\n",
       " '2015-01-02': set(),\n",
       " '2013-10-22': {210054},\n",
       " '2011-12-28': set(),\n",
       " '2011-05-10': set(),\n",
       " '2014-04-22': {231569},\n",
       " '2011-08-19': set(),\n",
       " '2012-05-11': set(),\n",
       " '2016-03-03': set(),\n",
       " '2012-09-11': set(),\n",
       " '2011-10-04': set(),\n",
       " '2016-02-19': set(),\n",
       " '2015-03-25': set(),\n",
       " '2012-02-17': set(),\n",
       " '2013-07-03': set(),\n",
       " '2016-07-15': set(),\n",
       " '2014-10-28': set(),\n",
       " '2015-10-21': set(),\n",
       " '2017-05-24': set(),\n",
       " '2010-07-07': set(),\n",
       " '2014-12-01': set(),\n",
       " '2014-11-19': set(),\n",
       " '2012-04-09': set(),\n",
       " '2014-07-15': set(),\n",
       " '2016-01-08': {287243},\n",
       " '2014-05-12': {233861},\n",
       " '2010-02-03': set(),\n",
       " '2012-10-11': set(),\n",
       " '2016-11-14': set(),\n",
       " '2015-10-15': set(),\n",
       " '2017-06-14': set(),\n",
       " '2017-03-10': set(),\n",
       " '2015-01-29': {255186, 255304, 255368},\n",
       " '2017-08-29': set(),\n",
       " '2016-03-17': set(),\n",
       " '2017-01-17': set(),\n",
       " '2015-03-24': set(),\n",
       " '2017-09-12': {343504},\n",
       " '2015-04-20': set(),\n",
       " '2013-03-05': set(),\n",
       " '2015-08-04': set(),\n",
       " '2010-02-16': set(),\n",
       " '2017-07-12': set(),\n",
       " '2014-10-20': set(),\n",
       " '2014-07-28': set(),\n",
       " '2015-08-28': set(),\n",
       " '2015-10-22': set(),\n",
       " '2013-11-01': set(),\n",
       " '2012-05-15': set(),\n",
       " '2010-03-05': set(),\n",
       " '2011-12-22': set(),\n",
       " '2016-07-28': {305013, 305078, 305112, 305141},\n",
       " '2016-07-27': set(),\n",
       " '2012-03-29': set(),\n",
       " '2017-01-04': set(),\n",
       " '2010-05-25': set(),\n",
       " '2016-12-23': set(),\n",
       " '2011-09-14': set(),\n",
       " '2012-09-10': set(),\n",
       " '2017-09-01': set(),\n",
       " '2015-11-17': set(),\n",
       " '2010-09-01': set(),\n",
       " '2010-06-30': set(),\n",
       " '2016-08-19': set(),\n",
       " '2016-09-15': set(),\n",
       " '2015-12-11': set(),\n",
       " '2014-09-22': set(),\n",
       " '2016-06-14': set(),\n",
       " '2014-05-07': set(),\n",
       " '2012-05-18': set(),\n",
       " '2015-04-10': set(),\n",
       " '2010-08-19': set(),\n",
       " '2011-04-01': set(),\n",
       " '2011-03-23': set(),\n",
       " '2015-08-18': set(),\n",
       " '2012-01-23': set(),\n",
       " '2014-07-29': set(),\n",
       " '2013-10-15': set(),\n",
       " '2013-10-28': set(),\n",
       " '2013-09-24': set(),\n",
       " '2011-12-15': set(),\n",
       " '2015-07-09': set(),\n",
       " '2014-10-14': set(),\n",
       " '2010-06-28': set(),\n",
       " '2010-11-23': set(),\n",
       " '2017-04-13': set(),\n",
       " '2010-10-29': set(),\n",
       " '2012-10-16': set(),\n",
       " '2018-01-12': {355209},\n",
       " '2012-05-03': set(),\n",
       " '2011-09-08': {74733},\n",
       " '2012-08-17': set(),\n",
       " '2017-11-13': set(),\n",
       " '2014-12-11': set(),\n",
       " '2011-06-06': {57318},\n",
       " '2015-05-15': set(),\n",
       " '2013-09-05': set(),\n",
       " '2014-01-24': set(),\n",
       " '2012-08-06': set(),\n",
       " '2010-02-19': set(),\n",
       " '2013-09-26': set(),\n",
       " '2014-08-19': set(),\n",
       " '2016-07-22': set(),\n",
       " '2014-12-24': set(),\n",
       " '2011-03-22': set(),\n",
       " '2016-07-06': set(),\n",
       " '2011-06-16': set(),\n",
       " '2013-01-29': {162399},\n",
       " '2017-08-21': set(),\n",
       " '2010-10-12': set(),\n",
       " '2013-09-17': set(),\n",
       " '2010-11-16': set(),\n",
       " '2013-04-15': set(),\n",
       " '2011-09-12': set(),\n",
       " '2010-06-16': set(),\n",
       " '2011-03-28': set(),\n",
       " '2015-12-16': set(),\n",
       " '2014-01-07': {220525},\n",
       " '2013-09-04': set(),\n",
       " '2012-12-21': set(),\n",
       " '2017-02-01': set(),\n",
       " '2017-07-28': set(),\n",
       " '2017-11-07': {349864},\n",
       " '2011-02-02': set(),\n",
       " '2010-12-07': set(),\n",
       " '2016-12-13': set(),\n",
       " '2015-08-11': set(),\n",
       " '2016-03-30': set(),\n",
       " '2015-08-13': set(),\n",
       " '2017-02-17': set(),\n",
       " '2014-11-17': set(),\n",
       " '2016-05-20': set(),\n",
       " '2011-09-20': {76868},\n",
       " '2014-05-08': set(),\n",
       " '2012-12-14': set(),\n",
       " '2011-02-15': set(),\n",
       " '2013-08-23': set(),\n",
       " '2016-09-21': set(),\n",
       " '2013-11-27': set(),\n",
       " '2011-06-22': set(),\n",
       " '2013-04-03': set(),\n",
       " '2016-10-26': set(),\n",
       " '2013-10-04': set(),\n",
       " '2016-05-09': set(),\n",
       " '2010-03-04': set(),\n",
       " '2011-05-05': set(),\n",
       " '2010-01-06': set(),\n",
       " '2010-11-11': set(),\n",
       " '2014-11-28': set(),\n",
       " '2013-05-06': set(),\n",
       " '2015-11-24': set(),\n",
       " '2016-10-13': {311777},\n",
       " '2014-08-04': set(),\n",
       " '2011-01-14': set(),\n",
       " '2013-12-10': set(),\n",
       " '2015-12-23': set(),\n",
       " '2015-10-27': set(),\n",
       " '2010-10-27': set(),\n",
       " '2018-04-05': set(),\n",
       " '2012-06-22': set(),\n",
       " '2012-05-17': set(),\n",
       " '2014-02-21': set(),\n",
       " '2012-05-24': set(),\n",
       " '2013-06-28': set(),\n",
       " '2014-11-20': set(),\n",
       " '2011-03-04': {40513},\n",
       " '2010-08-27': set(),\n",
       " '2016-05-26': set(),\n",
       " '2018-02-07': set(),\n",
       " '2012-11-02': set(),\n",
       " '2010-07-06': set(),\n",
       " '2010-12-20': set(),\n",
       " '2015-09-24': set(),\n",
       " '2013-08-09': set(),\n",
       " '2018-01-26': set(),\n",
       " '2011-12-12': set(),\n",
       " '2013-01-07': set(),\n",
       " '2015-03-10': set(),\n",
       " '2010-09-09': set(),\n",
       " '2011-07-15': set(),\n",
       " '2017-05-10': set(),\n",
       " '2011-07-25': set(),\n",
       " '2015-03-11': set(),\n",
       " '2013-10-21': set(),\n",
       " '2015-10-01': set(),\n",
       " '2011-02-17': set(),\n",
       " '2017-11-28': {351809},\n",
       " '2013-05-21': set(),\n",
       " '2010-09-23': set(),\n",
       " '2013-01-11': set(),\n",
       " '2013-04-24': set(),\n",
       " '2011-05-03': set(),\n",
       " '2017-03-31': set(),\n",
       " '2012-08-14': set(),\n",
       " '2015-09-23': set(),\n",
       " '2015-10-28': set(),\n",
       " '2012-05-07': set(),\n",
       " '2016-08-03': set(),\n",
       " '2014-05-27': set(),\n",
       " '2012-02-13': {98473},\n",
       " '2011-08-09': set(),\n",
       " '2013-03-15': set(),\n",
       " '2015-10-23': set(),\n",
       " '2013-08-08': set(),\n",
       " '2010-02-17': set(),\n",
       " '2017-12-07': set(),\n",
       " '2015-02-17': set(),\n",
       " '2015-03-09': set(),\n",
       " '2016-12-12': set(),\n",
       " '2011-03-01': set(),\n",
       " '2011-07-28': set(),\n",
       " '2013-11-07': set(),\n",
       " '2012-03-22': set(),\n",
       " '2012-11-01': set(),\n",
       " '2010-10-22': set(),\n",
       " '2014-03-28': set(),\n",
       " '2013-09-16': set(),\n",
       " '2015-04-23': {262472, 262524},\n",
       " '2014-07-22': {239634, 239698, 239739},\n",
       " '2010-01-05': {16},\n",
       " '2011-04-19': {48808, 48921, 49099},\n",
       " '2015-06-03': set(),\n",
       " '2016-07-20': set(),\n",
       " '2015-07-07': set(),\n",
       " '2010-12-10': {31970},\n",
       " '2016-05-19': set(),\n",
       " '2018-03-13': set(),\n",
       " '2016-09-29': set(),\n",
       " '2011-02-28': set(),\n",
       " '2010-06-29': set(),\n",
       " '2011-11-11': set(),\n",
       " '2018-03-22': set(),\n",
       " '2017-11-03': {349521},\n",
       " '2013-05-29': set(),\n",
       " '2018-01-10': set(),\n",
       " '2013-10-01': set(),\n",
       " '2015-07-27': set(),\n",
       " '2013-08-19': set(),\n",
       " '2010-06-03': set(),\n",
       " '2017-02-08': set(),\n",
       " '2017-05-16': set(),\n",
       " '2013-07-11': set(),\n",
       " '2017-02-14': set(),\n",
       " '2011-09-07': set(),\n",
       " '2013-02-14': set(),\n",
       " '2010-12-21': set(),\n",
       " '2018-04-03': set(),\n",
       " '2012-03-05': {102463},\n",
       " '2018-03-28': set(),\n",
       " '2010-09-16': set(),\n",
       " '2016-02-02': set(),\n",
       " '2010-12-15': set(),\n",
       " '2012-09-17': set(),\n",
       " '2012-07-27': set(),\n",
       " '2014-08-06': set(),\n",
       " '2012-03-28': set(),\n",
       " '2015-12-08': set(),\n",
       " '2013-01-04': set(),\n",
       " '2018-01-22': set(),\n",
       " '2016-09-14': set(),\n",
       " '2011-06-08': set(),\n",
       " '2014-03-06': set(),\n",
       " '2011-12-07': set(),\n",
       " '2018-01-30': {356871,\n",
       "  356875,\n",
       "  356880,\n",
       "  356896,\n",
       "  356899,\n",
       "  356904,\n",
       "  356919,\n",
       "  356928,\n",
       "  357009,\n",
       "  357016,\n",
       "  357026,\n",
       "  357027},\n",
       " '2015-12-18': set(),\n",
       " '2014-01-16': {222827},\n",
       " '2017-12-01': set(),\n",
       " '2015-05-11': set(),\n",
       " '2013-01-10': set(),\n",
       " '2014-08-13': set(),\n",
       " '2013-06-11': set(),\n",
       " '2010-12-06': set(),\n",
       " '2016-01-04': set(),\n",
       " '2011-08-22': set(),\n",
       " '2015-05-04': set(),\n",
       " '2017-10-16': set(),\n",
       " '2015-09-04': set(),\n",
       " '2010-06-24': set(),\n",
       " '2017-03-21': set(),\n",
       " '2010-04-22': set(),\n",
       " '2012-12-31': set(),\n",
       " '2011-02-11': set(),\n",
       " '2010-06-25': set(),\n",
       " '2015-08-31': set(),\n",
       " '2010-11-22': set(),\n",
       " '2012-07-31': set(),\n",
       " '2017-01-27': set(),\n",
       " '2015-05-14': set(),\n",
       " '2012-04-02': set(),\n",
       " '2018-03-20': set(),\n",
       " '2016-06-06': set(),\n",
       " '2010-09-17': set(),\n",
       " '2010-11-18': set(),\n",
       " '2015-06-26': set(),\n",
       " '2011-10-11': set(),\n",
       " '2012-12-17': set(),\n",
       " '2014-11-12': set(),\n",
       " '2010-10-06': set(),\n",
       " '2012-02-22': set(),\n",
       " '2017-07-24': set(),\n",
       " '2010-04-01': {1918},\n",
       " '2013-03-21': set(),\n",
       " '2013-10-03': set(),\n",
       " '2012-02-10': set(),\n",
       " '2012-09-19': set(),\n",
       " '2016-07-05': set(),\n",
       " '2018-01-23': set(),\n",
       " '2017-04-07': set(),\n",
       " '2010-08-12': set(),\n",
       " '2015-11-02': set(),\n",
       " '2010-10-05': {18493},\n",
       " '2012-06-01': set(),\n",
       " '2016-03-01': set(),\n",
       " '2012-06-14': set(),\n",
       " '2015-07-14': set(),\n",
       " '2014-06-26': set(),\n",
       " '2017-08-10': set(),\n",
       " '2014-08-05': set(),\n",
       " '2015-05-05': set(),\n",
       " '2013-07-02': set(),\n",
       " '2014-11-13': set(),\n",
       " '2010-01-25': set(),\n",
       " '2011-08-05': set(),\n",
       " '2013-05-22': set(),\n",
       " '2017-08-03': set(),\n",
       " '2017-09-29': set(),\n",
       " '2011-10-26': set(),\n",
       " '2017-04-17': {328767},\n",
       " '2010-04-13': set(),\n",
       " '2013-06-19': set(),\n",
       " '2011-11-01': set(),\n",
       " '2011-10-17': {82600},\n",
       " '2015-12-04': set(),\n",
       " '2013-08-20': {199449},\n",
       " '2012-12-11': set(),\n",
       " '2016-07-13': set(),\n",
       " '2016-05-10': set(),\n",
       " '2018-01-16': set(),\n",
       " '2010-05-27': set(),\n",
       " '2014-10-15': set(),\n",
       " '2016-08-11': set(),\n",
       " '2016-03-08': set(),\n",
       " '2013-12-03': set(),\n",
       " '2013-01-02': set(),\n",
       " '2012-06-13': set(),\n",
       " '2011-01-26': set(),\n",
       " '2012-10-10': set(),\n",
       " '2015-06-15': set(),\n",
       " '2011-09-26': set(),\n",
       " '2016-02-24': set(),\n",
       " '2013-04-12': set(),\n",
       " '2016-09-09': set(),\n",
       " '2015-03-31': set(),\n",
       " '2012-02-15': set(),\n",
       " '2016-10-20': set(),\n",
       " '2010-12-09': set(),\n",
       " '2013-04-26': set(),\n",
       " '2012-09-06': set(),\n",
       " '2014-03-04': set(),\n",
       " '2016-09-23': set(),\n",
       " '2010-04-20': set(),\n",
       " '2013-11-22': set(),\n",
       " '2010-03-08': set(),\n",
       " '2018-02-21': set(),\n",
       " '2011-05-17': set(),\n",
       " '2010-04-29': set(),\n",
       " '2014-05-23': set(),\n",
       " '2014-01-09': {221044},\n",
       " '2017-04-04': set(),\n",
       " '2015-11-11': set(),\n",
       " '2010-03-19': set(),\n",
       " '2015-07-30': {274331, 274438},\n",
       " '2011-06-17': set(),\n",
       " '2013-09-13': set(),\n",
       " '2010-02-08': set(),\n",
       " '2016-09-12': set(),\n",
       " '2012-11-28': set(),\n",
       " '2011-05-09': set(),\n",
       " '2013-04-25': {178884},\n",
       " '2011-07-06': set(),\n",
       " '2013-12-12': set(),\n",
       " '2017-09-14': set(),\n",
       " '2010-01-14': set(),\n",
       " '2015-12-02': set(),\n",
       " '2010-08-10': set(),\n",
       " '2016-06-10': set(),\n",
       " '2017-11-22': set(),\n",
       " '2013-03-13': set(),\n",
       " '2015-02-05': set(),\n",
       " '2010-08-13': set(),\n",
       " '2014-11-25': set(),\n",
       " '2012-08-30': set(),\n",
       " '2015-11-16': set(),\n",
       " '2010-07-02': set(),\n",
       " '2011-09-29': set(),\n",
       " '2017-02-09': set(),\n",
       " '2015-08-27': set(),\n",
       " '2016-11-11': set(),\n",
       " '2014-04-08': set(),\n",
       " '2015-11-19': set(),\n",
       " '2011-04-29': set(),\n",
       " '2016-10-19': set(),\n",
       " '2015-04-09': set(),\n",
       " '2017-12-04': set(),\n",
       " '2013-01-30': set(),\n",
       " '2012-02-07': set(),\n",
       " '2012-02-21': set(),\n",
       " '2012-11-05': set(),\n",
       " '2015-07-02': set(),\n",
       " '2013-11-08': set(),\n",
       " '2016-12-30': set(),\n",
       " '2016-12-21': set(),\n",
       " '2016-03-07': set(),\n",
       " '2012-10-23': {145901, 146006},\n",
       " '2016-11-30': set(),\n",
       " '2010-08-25': set(),\n",
       " '2014-11-06': set(),\n",
       " '2011-11-22': set(),\n",
       " '2018-02-13': {358388},\n",
       " '2011-08-30': {72989},\n",
       " '2012-06-21': set(),\n",
       " '2016-11-21': set(),\n",
       " '2010-08-18': set(),\n",
       " '2012-02-02': set(),\n",
       " '2017-01-19': set(),\n",
       " '2015-05-29': set(),\n",
       " '2011-10-31': set(),\n",
       " '2011-03-24': set(),\n",
       " '2013-12-31': set(),\n",
       " '2013-12-02': set(),\n",
       " '2011-01-03': set(),\n",
       " '2015-12-29': set(),\n",
       " '2017-04-27': set(),\n",
       " '2010-01-08': set(),\n",
       " '2016-04-27': set(),\n",
       " '2015-01-21': set(),\n",
       " '2017-01-24': set(),\n",
       " '2013-10-08': {207646},\n",
       " '2015-05-01': set(),\n",
       " '2014-08-25': set(),\n",
       " '2011-12-19': {88798},\n",
       " '2016-01-25': set(),\n",
       " '2010-06-22': set(),\n",
       " '2014-08-22': set(),\n",
       " '2012-12-12': set(),\n",
       " '2015-05-18': set(),\n",
       " '2017-11-06': set(),\n",
       " '2015-05-27': set(),\n",
       " '2012-06-26': set(),\n",
       " '2015-01-26': {254665},\n",
       " '2010-05-26': set(),\n",
       " '2013-11-12': set(),\n",
       " '2013-07-17': set(),\n",
       " '2016-02-11': set(),\n",
       " '2011-01-31': set(),\n",
       " '2013-02-11': set(),\n",
       " '2016-08-09': set(),\n",
       " '2013-03-14': set(),\n",
       " '2015-04-01': set(),\n",
       " '2012-03-21': set(),\n",
       " '2014-06-16': set(),\n",
       " '2016-01-22': set(),\n",
       " '2012-02-23': set(),\n",
       " '2017-04-18': {328875, 328894, 329020, 329021},\n",
       " '2013-08-28': set(),\n",
       " '2013-06-04': set(),\n",
       " '2016-12-01': set(),\n",
       " '2012-07-17': set(),\n",
       " '2016-08-30': set(),\n",
       " '2015-10-06': set(),\n",
       " '2010-03-17': set(),\n",
       " '2010-08-02': set(),\n",
       " '2012-08-13': set(),\n",
       " '2016-08-15': set(),\n",
       " '2017-11-20': set(),\n",
       " '2017-09-05': set(),\n",
       " '2013-05-13': set(),\n",
       " '2016-03-31': set(),\n",
       " '2016-04-11': set(),\n",
       " '2010-07-09': set(),\n",
       " '2011-01-20': set(),\n",
       " '2016-06-23': set(),\n",
       " '2016-06-16': set(),\n",
       " '2018-01-02': set(),\n",
       " '2011-06-02': set(),\n",
       " '2013-01-03': set(),\n",
       " '2013-06-17': set(),\n",
       " '2015-10-14': set(),\n",
       " '2010-05-05': set(),\n",
       " '2012-12-18': set(),\n",
       " '2016-12-08': set(),\n",
       " '2013-11-29': set(),\n",
       " '2013-05-28': set(),\n",
       " '2012-06-04': set(),\n",
       " '2016-06-24': set(),\n",
       " '2016-08-29': set(),\n",
       " '2012-08-23': set(),\n",
       " '2015-02-12': set(),\n",
       " '2011-03-17': set(),\n",
       " '2016-11-29': set(),\n",
       " '2010-02-24': set(),\n",
       " '2016-11-22': set(),\n",
       " '2011-11-25': set(),\n",
       " '2010-05-19': set(),\n",
       " '2013-08-16': set(),\n",
       " '2015-06-05': set(),\n",
       " '2014-12-23': set(),\n",
       " '2018-03-09': set(),\n",
       " '2011-10-05': set(),\n",
       " '2010-06-02': set(),\n",
       " '2017-08-23': set(),\n",
       " '2016-05-13': set(),\n",
       " '2010-07-14': {9816},\n",
       " '2016-05-04': set(),\n",
       " '2014-03-27': set(),\n",
       " '2010-10-19': {21445},\n",
       " '2016-08-04': set(),\n",
       " '2017-07-11': set(),\n",
       " '2011-09-09': {74990},\n",
       " '2011-04-14': set(),\n",
       " '2015-09-28': set(),\n",
       " '2011-05-16': {53963},\n",
       " '2012-08-21': set(),\n",
       " '2015-08-05': set(),\n",
       " '2015-01-13': set(),\n",
       " '2013-07-18': set(),\n",
       " '2017-08-08': set(),\n",
       " '2010-05-21': set(),\n",
       " '2012-08-27': set(),\n",
       " '2012-03-08': {103381},\n",
       " '2014-11-26': set(),\n",
       " '2016-04-29': set(),\n",
       " '2010-03-02': set(),\n",
       " '2013-12-04': set(),\n",
       " '2016-07-12': set(),\n",
       " '2016-04-06': set(),\n",
       " '2017-04-21': set(),\n",
       " '2015-11-27': set(),\n",
       " '2012-04-19': set(),\n",
       " '2015-07-16': set(),\n",
       " '2015-10-20': {280843, 280942},\n",
       " '2014-07-25': set(),\n",
       " '2015-05-13': set(),\n",
       " '2015-08-25': set(),\n",
       " '2015-08-03': set(),\n",
       " '2017-01-25': set(),\n",
       " '2012-08-31': set(),\n",
       " '2015-09-11': set(),\n",
       " '2012-12-07': set(),\n",
       " '2017-01-03': set(),\n",
       " '2010-07-15': set(),\n",
       " '2017-01-12': set(),\n",
       " '2012-01-10': set(),\n",
       " '2010-12-01': {30191, 30331},\n",
       " '2016-04-08': set(),\n",
       " '2017-05-15': set(),\n",
       " '2013-08-14': set(),\n",
       " '2011-08-29': set(),\n",
       " '2012-11-16': set(),\n",
       " '2015-04-28': set(),\n",
       " '2010-07-01': set(),\n",
       " '2014-03-12': set(),\n",
       " '2016-03-21': set(),\n",
       " '2012-11-08': set(),\n",
       " '2013-05-14': set(),\n",
       " '2013-04-16': set(),\n",
       " '2014-03-26': set(),\n",
       " '2017-09-08': {343335},\n",
       " '2017-12-26': set(),\n",
       " '2014-10-10': set(),\n",
       " '2010-10-26': set(),\n",
       " '2011-05-25': set(),\n",
       " '2011-10-28': set(),\n",
       " '2012-12-03': set(),\n",
       " '2012-04-03': set(),\n",
       " '2015-03-27': set(),\n",
       " '2015-12-24': set(),\n",
       " '2014-05-30': set(),\n",
       " '2014-01-31': {224591},\n",
       " '2015-04-16': set(),\n",
       " '2013-11-06': set(),\n",
       " '2012-03-07': set(),\n",
       " '2010-04-27': set(),\n",
       " '2011-06-03': set(),\n",
       " '2013-08-26': set(),\n",
       " '2014-04-16': set(),\n",
       " '2016-08-22': set(),\n",
       " '2010-01-12': set(),\n",
       " '2010-04-09': set(),\n",
       " '2014-11-04': set(),\n",
       " '2013-12-18': set(),\n",
       " '2011-05-20': set(),\n",
       " '2013-05-03': set(),\n",
       " '2017-11-08': set(),\n",
       " '2012-12-06': set(),\n",
       " '2016-09-06': set(),\n",
       " '2013-10-23': set(),\n",
       " '2013-09-06': set(),\n",
       " '2011-08-16': set(),\n",
       " '2012-12-10': set(),\n",
       " '2016-10-10': set(),\n",
       " '2013-06-12': set(),\n",
       " '2016-07-19': set(),\n",
       " '2012-08-24': set(),\n",
       " '2017-02-23': {324504},\n",
       " '2017-03-16': set(),\n",
       " '2016-06-21': set(),\n",
       " '2015-12-07': set(),\n",
       " '2012-10-02': set(),\n",
       " '2013-02-01': set(),\n",
       " '2016-10-31': set(),\n",
       " '2013-02-06': set(),\n",
       " '2016-10-05': set(),\n",
       " '2016-04-07': {294678},\n",
       " '2010-05-06': set(),\n",
       " '2012-04-17': set(),\n",
       " '2017-10-05': set(),\n",
       " '2012-02-28': set(),\n",
       " '2014-09-10': set(),\n",
       " '2015-07-21': {271532,\n",
       "  271578,\n",
       "  271651,\n",
       "  271656,\n",
       "  271723,\n",
       "  271805,\n",
       "  271819,\n",
       "  271832,\n",
       "  271862,\n",
       "  271880,\n",
       "  271904},\n",
       " '2014-10-30': {248551},\n",
       " '2010-05-14': set(),\n",
       " '2018-01-19': set(),\n",
       " '2016-03-09': set(),\n",
       " '2010-12-31': set(),\n",
       " '2017-12-05': set(),\n",
       " '2014-06-24': {237289},\n",
       " '2011-11-17': set(),\n",
       " '2011-10-03': set(),\n",
       " '2013-03-12': set(),\n",
       " '2015-06-29': set(),\n",
       " '2010-03-30': set(),\n",
       " '2015-08-19': set(),\n",
       " '2012-01-20': set(),\n",
       " '2011-07-01': set(),\n",
       " '2012-05-04': {114713},\n",
       " '2016-12-05': set(),\n",
       " '2015-06-12': set(),\n",
       " '2010-02-18': set(),\n",
       " '2015-02-09': set(),\n",
       " '2010-03-26': set(),\n",
       " '2017-01-30': set(),\n",
       " '2017-06-07': {334046, 334127},\n",
       " '2011-11-07': set(),\n",
       " '2015-04-06': set(),\n",
       " '2012-05-31': set(),\n",
       " '2017-03-07': set(),\n",
       " '2011-07-12': set(),\n",
       " '2016-07-26': set(),\n",
       " '2012-04-30': set(),\n",
       " '2018-02-23': set(),\n",
       " '2014-03-13': set(),\n",
       " '2016-05-05': set(),\n",
       " '2015-08-07': set(),\n",
       " '2012-12-20': set(),\n",
       " '2013-10-24': set(),\n",
       " '2015-02-26': set(),\n",
       " '2013-04-08': set(),\n",
       " '2012-02-03': set(),\n",
       " '2014-06-09': set(),\n",
       " '2010-08-06': set(),\n",
       " '2014-10-09': set(),\n",
       " '2010-05-12': set(),\n",
       " '2013-04-29': set(),\n",
       " '2015-09-29': set(),\n",
       " '2015-04-30': set(),\n",
       " '2011-11-08': set(),\n",
       " '2010-12-29': set(),\n",
       " '2013-10-25': set(),\n",
       " '2013-01-08': set(),\n",
       " '2011-08-31': set(),\n",
       " '2012-11-14': set(),\n",
       " '2016-01-15': set(),\n",
       " '2013-05-02': set(),\n",
       " '2010-09-15': set(),\n",
       " '2011-06-24': set(),\n",
       " '2016-08-25': set(),\n",
       " '2010-07-21': set(),\n",
       " '2013-07-31': set(),\n",
       " '2015-11-30': set(),\n",
       " '2012-05-30': set(),\n",
       " '2018-02-20': {358936},\n",
       " '2015-08-21': set(),\n",
       " '2014-07-02': set(),\n",
       " '2011-03-16': set(),\n",
       " '2015-09-25': set(),\n",
       " '2014-04-21': set(),\n",
       " '2016-01-28': {288968, 289081},\n",
       " '2014-12-03': set(),\n",
       " '2014-12-09': set(),\n",
       " '2017-07-13': set(),\n",
       " '2013-02-13': set(),\n",
       " '2015-01-30': set(),\n",
       " '2010-02-04': {805},\n",
       " '2010-03-10': set(),\n",
       " '2011-01-04': set(),\n",
       " '2017-12-21': set(),\n",
       " '2010-04-16': set(),\n",
       " '2013-12-16': set(),\n",
       " '2014-07-03': set(),\n",
       " '2017-11-30': set(),\n",
       " '2014-01-08': {220756},\n",
       " '2012-09-25': {140705},\n",
       " '2015-08-20': set(),\n",
       " '2010-08-30': set(),\n",
       " '2011-04-20': set(),\n",
       " '2010-06-17': set(),\n",
       " '2017-12-15': set(),\n",
       " '2015-03-06': set(),\n",
       " '2011-03-11': set(),\n",
       " '2014-02-06': {225086},\n",
       " '2015-07-28': set(),\n",
       " '2016-05-18': set(),\n",
       " '2017-04-10': set(),\n",
       " '2011-02-10': set(),\n",
       " '2017-08-15': set(),\n",
       " '2010-07-20': {10878, 10907, 10927},\n",
       " '2011-12-23': set(),\n",
       " '2010-12-16': set(),\n",
       " '2015-12-03': set(),\n",
       " '2013-11-18': set(),\n",
       " '2016-04-12': set(),\n",
       " '2017-07-06': set(),\n",
       " '2017-11-14': set(),\n",
       " '2014-11-11': set(),\n",
       " '2015-11-03': set(),\n",
       " '2014-10-23': set(),\n",
       " '2011-04-26': set(),\n",
       " '2017-07-21': {338333},\n",
       " '2014-01-10': {221350},\n",
       " '2012-07-16': {128207},\n",
       " '2014-12-10': set(),\n",
       " '2018-02-14': set(),\n",
       " '2013-07-15': set(),\n",
       " '2012-01-05': set(),\n",
       " '2012-05-02': set(),\n",
       " '2014-04-24': set(),\n",
       " '2010-09-28': set(),\n",
       " '2015-03-05': set(),\n",
       " '2017-03-02': set(),\n",
       " '2012-04-16': set(),\n",
       " '2015-08-06': set(),\n",
       " '2012-03-26': set(),\n",
       " '2011-07-19': {64828, 64870, 64904},\n",
       " '2015-01-28': {255049},\n",
       " '2011-12-20': set(),\n",
       " '2016-03-24': set(),\n",
       " '2011-07-08': set(),\n",
       " '2011-05-13': set(),\n",
       " '2015-04-14': set(),\n",
       " '2016-11-18': set(),\n",
       " '2012-06-25': set(),\n",
       " '2011-11-10': set(),\n",
       " '2015-03-12': set(),\n",
       " '2016-09-19': set(),\n",
       " '2013-07-24': set(),\n",
       " '2012-11-20': set(),\n",
       " '2014-02-24': set(),\n",
       " '2016-12-20': set(),\n",
       " '2012-03-13': set(),\n",
       " '2011-11-23': set(),\n",
       " '2010-09-10': set(),\n",
       " '2012-04-12': set(),\n",
       " '2016-05-16': set(),\n",
       " '2015-10-02': set(),\n",
       " '2013-10-31': set(),\n",
       " '2013-12-27': set(),\n",
       " '2011-11-30': set(),\n",
       " '2014-06-17': set(),\n",
       " '2017-11-21': set(),\n",
       " '2010-08-23': set(),\n",
       " '2015-07-01': set(),\n",
       " '2013-06-18': set(),\n",
       " '2017-07-10': set(),\n",
       " '2011-04-21': {49537},\n",
       " '2016-08-12': set(),\n",
       " '2013-01-15': set(),\n",
       " '2016-05-12': set(),\n",
       " '2013-03-20': set(),\n",
       " '2018-01-31': set(),\n",
       " '2010-02-02': set(),\n",
       " '2014-09-25': set(),\n",
       " '2014-09-26': set(),\n",
       " '2012-07-25': set(),\n",
       " '2017-02-03': set(),\n",
       " '2011-05-27': set(),\n",
       " '2017-06-27': set(),\n",
       " '2011-01-06': set(),\n",
       " '2011-12-16': set(),\n",
       " '2014-01-27': set(),\n",
       " '2018-01-25': set(),\n",
       " '2013-02-25': set(),\n",
       " '2017-01-31': {321837, 321850, 321904},\n",
       " '2014-07-10': {238518},\n",
       " '2014-03-31': set(),\n",
       " '2012-11-13': {149536, 149705},\n",
       " '2014-04-29': set(),\n",
       " '2016-04-18': set(),\n",
       " '2017-04-24': set(),\n",
       " '2016-07-14': set(),\n",
       " '2016-12-19': set(),\n",
       " '2016-10-07': set(),\n",
       " '2015-12-22': set(),\n",
       " '2010-09-20': set(),\n",
       " '2013-10-29': set(),\n",
       " '2012-12-13': set(),\n",
       " '2016-02-04': set(),\n",
       " '2011-05-24': set(),\n",
       " '2017-06-12': set(),\n",
       " '2012-06-11': set(),\n",
       " '2014-05-28': {235235},\n",
       " '2017-09-28': set(),\n",
       " '2016-07-07': set(),\n",
       " '2013-11-05': set(),\n",
       " '2013-08-21': {199581},\n",
       " '2011-06-01': set(),\n",
       " '2011-07-13': set(),\n",
       " '2014-10-27': set(),\n",
       " '2014-08-18': set(),\n",
       " '2011-02-07': set(),\n",
       " '2014-04-15': set(),\n",
       " '2010-08-03': set(),\n",
       " '2010-09-29': set(),\n",
       " '2014-05-15': set(),\n",
       " '2016-03-23': set(),\n",
       " '2014-09-02': set(),\n",
       " '2011-02-25': set(),\n",
       " '2016-01-21': set(),\n",
       " '2010-09-02': set(),\n",
       " '2015-01-09': set(),\n",
       " '2011-08-08': set(),\n",
       " '2012-10-01': set(),\n",
       " '2012-01-11': set(),\n",
       " '2014-06-18': set(),\n",
       " '2015-05-06': set(),\n",
       " '2011-01-18': set(),\n",
       " '2012-07-10': set(),\n",
       " '2015-01-07': set(),\n",
       " '2011-07-29': set(),\n",
       " '2014-11-18': set(),\n",
       " '2013-09-27': set(),\n",
       " '2017-09-21': {344600},\n",
       " '2015-01-27': set(),\n",
       " '2016-02-03': set(),\n",
       " '2018-03-27': set(),\n",
       " '2012-09-24': set(),\n",
       " '2013-08-22': set(),\n",
       " '2012-07-02': set(),\n",
       " '2017-08-30': {342384},\n",
       " '2014-01-03': {220035},\n",
       " '2015-11-12': set(),\n",
       " '2014-09-23': set(),\n",
       " '2010-05-07': set(),\n",
       " '2014-09-04': set(),\n",
       " '2017-03-29': set(),\n",
       " '2015-11-05': set(),\n",
       " '2017-03-17': set(),\n",
       " '2016-09-08': set(),\n",
       " '2016-12-09': set(),\n",
       " '2011-09-23': set(),\n",
       " '2016-01-19': set(),\n",
       " '2017-06-22': {335598, 335644},\n",
       " '2011-07-18': {64674},\n",
       " '2013-08-12': set(),\n",
       " '2016-04-22': set(),\n",
       " '2014-08-26': set(),\n",
       " '2011-07-07': set(),\n",
       " '2014-10-16': set(),\n",
       " '2011-06-29': set(),\n",
       " '2017-05-30': set(),\n",
       " '2014-04-30': set(),\n",
       " '2015-10-09': set(),\n",
       " '2017-07-03': set(),\n",
       " '2015-11-23': {284230},\n",
       " '2017-04-19': set(),\n",
       " '2013-04-30': set(),\n",
       " '2011-04-12': set(),\n",
       " '2014-11-24': set(),\n",
       " '2018-03-08': {360515, 360550, 360581},\n",
       " '2013-01-22': set(),\n",
       " '2016-07-29': {305283},\n",
       " '2013-12-11': set(),\n",
       " '2018-03-19': set(),\n",
       " '2010-06-04': set(),\n",
       " '2014-07-21': set(),\n",
       " '2014-07-11': set(),\n",
       " '2015-09-09': set(),\n",
       " '2012-09-20': set(),\n",
       " '2011-11-09': set(),\n",
       " '2017-12-29': set(),\n",
       " '2016-11-08': set(),\n",
       " '2012-09-28': set(),\n",
       " '2014-11-07': set(),\n",
       " '2011-10-10': set(),\n",
       " '2015-12-31': set(),\n",
       " '2011-01-21': set(),\n",
       " '2014-04-04': set(),\n",
       " '2010-07-26': set(),\n",
       " '2015-07-20': set(),\n",
       " '2015-09-14': set(),\n",
       " '2013-01-24': set(),\n",
       " '2016-09-27': set(),\n",
       " '2011-08-03': {67918},\n",
       " '2014-03-14': set(),\n",
       " '2016-11-03': set(),\n",
       " '2017-06-21': {335494, 335503, 335506},\n",
       " '2013-03-01': set(),\n",
       " '2014-03-24': set(),\n",
       " '2015-08-12': set(),\n",
       " '2012-11-12': set(),\n",
       " '2016-06-30': set(),\n",
       " '2012-06-05': set(),\n",
       " '2017-06-13': set(),\n",
       " '2014-07-07': set(),\n",
       " '2012-08-07': set(),\n",
       " '2018-03-05': {360134, 360162, 360166},\n",
       " '2016-10-14': set(),\n",
       " '2011-04-07': set(),\n",
       " '2011-09-16': set(),\n",
       " '2011-11-15': set(),\n",
       " '2010-10-15': set(),\n",
       " '2017-09-06': set(),\n",
       " '2015-05-21': set(),\n",
       " ...}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote_dict['Harley-Davidson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlVector==============\n",
      "accuracy： 0.520881510846823\n",
      "recall： 0.49998793967470384\n",
      "precision： 0.4271112946520752\n",
      "f1_score 0.4606853038336461\n",
      "voting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stocksentiment/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting accuracy： 0.5208952758506772\n",
      "voting recall： 0.5\n",
      "voting precision： 0.2604476379253386\n",
      "voting f1_score 0.34249253326092854\n",
      "\n",
      "DsVector_rate==============\n",
      "accuracy： 0.5214871710164078\n",
      "recall： 0.5009265814680646\n",
      "precision： 0.529247230471489\n",
      "f1_score 0.5146976225543572\n",
      "voting\n",
      "voting accuracy： 0.5209365708622399\n",
      "voting recall： 0.5001306865504281\n",
      "voting precision： 0.5153190319154286\n",
      "voting f1_score 0.507611271218312\n",
      "\n",
      "SnVector==============\n",
      "accuracy： 0.5206337407774474\n",
      "recall： 0.49985037680282735\n",
      "precision： 0.4860512225962932\n",
      "f1_score 0.4928542298913098\n",
      "voting\n",
      "voting accuracy： 0.5206199757735932\n",
      "voting recall： 0.4997530309590441\n",
      "voting precision： 0.41038596104575886\n",
      "voting f1_score 0.4506819940631131\n",
      "\n",
      "DsVector==============\n",
      "accuracy： 0.5242264067833939\n",
      "recall： 0.5044502805334159\n",
      "precision： 0.5429033077458991\n",
      "f1_score 0.522970902968638\n",
      "voting\n",
      "voting accuracy： 0.5286036780090299\n",
      "voting recall： 0.5084375949024605\n",
      "voting precision： 0.6253113955024281\n",
      "voting f1_score 0.5608504610545494\n",
      "\n",
      "PmiVector==============\n",
      "accuracy： 0.5257818522189186\n",
      "recall： 0.5061807470269806\n",
      "precision： 0.5518193638132911\n",
      "f1_score 0.5280157061176989\n",
      "voting\n",
      "voting accuracy： 0.5341922695738355\n",
      "voting recall： 0.5144704588811876\n",
      "voting precision： 0.6351529419026016\n",
      "voting f1_score 0.568477338331117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "# RandomForest\n",
    "X1 = [data['DsVector'] for data in datas]\n",
    "X2 = [data['SnVector'] for data in datas]\n",
    "X3 = [data['BlVector'] for data in datas]\n",
    "X4 = [data['PmiVector'] for data in datas]\n",
    "X5 = [data['DsVector_rate'] for data in datas]\n",
    "\n",
    "Xs = {'DsVector':X1,'SnVector':X2,'BlVector':X3,'PmiVector':X4,'DsVector_rate':X5}\n",
    "\n",
    "Y = [np.sign(data['rate']) for data in datas]\n",
    "for vectorname in Xs.keys():\n",
    "    print(vectorname+'==============')\n",
    "    X = Xs[vectorname]\n",
    "    train_x,test_x,train_y,test_y = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "\n",
    "    clf.fit(np.array(train_x), np.array(train_y))\n",
    "\n",
    "    predict_y = clf.predict(test_x)\n",
    "    recall = recall_score(test_y,clf.predict(test_x),average = 'macro')\n",
    "    precision = precision_score(test_y, clf.predict(test_x), average='macro')\n",
    "\n",
    "    print('accuracy：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "    print('recall：',recall)\n",
    "    print('precision：',precision)\n",
    "    print('f1_score',2*recall*precision/(recall+precision))\n",
    "\n",
    "    vote_predict_y = vote(predict_y,datas[-len(test_x):])\n",
    "    vote_recall = recall_score(test_y,vote_predict_y,average = 'macro')\n",
    "    vote_precision = precision_score(test_y, vote_predict_y, average='macro')\n",
    "\n",
    "    print('voting accuracy：',accuracy(vote_predict_y,test_y))\n",
    "    print('voting recall：',vote_recall)\n",
    "    print('voting precision：',vote_precision)\n",
    "    print('voting f1_score',2*vote_recall*vote_precision/(vote_recall+vote_precision))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlVector==============\n",
      "accuracy： 0.5207851558198436\n",
      "recall： 0.5010099241239496\n",
      "precision： 0.5096778012454636\n",
      "f1_score 0.5053066938877044\n",
      "voting\n",
      "voting accuracy： 0.5208677458429688\n",
      "voting recall： 0.5003469866055271\n",
      "voting precision： 0.5097657840922931\n",
      "voting f1_score 0.5050124725558685\n",
      "\n",
      "DsVector_rate==============\n",
      "accuracy： 0.48305528025547845\n",
      "recall： 0.502237350919005\n",
      "precision： 0.5141714991563476\n",
      "f1_score 0.5081343627324632\n",
      "voting\n",
      "voting accuracy： 0.4815273648276622\n",
      "voting recall： 0.5014115204290613\n",
      "voting precision： 0.5148850052055574\n",
      "voting f1_score 0.5080589508953243\n",
      "\n",
      "SnVector==============\n",
      "accuracy： 0.5121269683955512\n",
      "recall： 0.5002180083164723\n",
      "precision： 0.5003223382921838\n",
      "f1_score 0.5002701678648954\n",
      "voting\n",
      "voting accuracy： 0.5176880299526484\n",
      "voting recall： 0.5011960585608762\n",
      "voting precision： 0.5031678829563104\n",
      "voting f1_score 0.5021800351597105\n",
      "\n",
      "DsVector==============\n",
      "accuracy： 0.48379859046360535\n",
      "recall： 0.5024287581893708\n",
      "precision： 0.5118029282265224\n",
      "f1_score 0.507072522206906\n",
      "voting\n",
      "voting accuracy： 0.48277998017839446\n",
      "voting recall： 0.5022589160105827\n",
      "voting precision： 0.517171175039173\n",
      "voting f1_score 0.5096059770015489\n",
      "\n",
      "PmiVector==============\n",
      "accuracy： 0.48494108578350403\n",
      "recall： 0.5025596223989819\n",
      "precision： 0.50882902860426\n",
      "f1_score 0.5056748940723046\n",
      "voting\n",
      "voting accuracy： 0.48448684065631537\n",
      "voting recall： 0.5031159074656588\n",
      "voting precision： 0.5151277799940626\n",
      "voting f1_score 0.5090509937538601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB\n",
    "for vectorname in Xs.keys():\n",
    "    print(vectorname+'==============')\n",
    "    X = Xs[vectorname]\n",
    "    train_x,test_x,train_y,test_y = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "\n",
    "    clf = GaussianNB()\n",
    "\n",
    "    clf.fit(np.array(train_x), np.array(train_y))\n",
    "\n",
    "    predict_y = clf.predict(test_x)\n",
    "    recall = recall_score(test_y,clf.predict(test_x),average = 'macro')\n",
    "    precision = precision_score(test_y, clf.predict(test_x), average='macro')\n",
    "\n",
    "    print('accuracy：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "    print('recall：',recall)\n",
    "    print('precision：',precision)\n",
    "    print('f1_score',2*recall*precision/(recall+precision))\n",
    "\n",
    "    vote_predict_y = vote(predict_y,datas[-len(test_x):])\n",
    "    vote_recall = recall_score(test_y,vote_predict_y,average = 'macro')\n",
    "    vote_precision = precision_score(test_y, vote_predict_y, average='macro')\n",
    "\n",
    "    print('voting accuracy：',accuracy(vote_predict_y,test_y))\n",
    "    print('voting recall：',vote_recall)\n",
    "    print('voting precision：',vote_precision)\n",
    "    print('voting f1_score',2*vote_recall*vote_precision/(vote_recall+vote_precision))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlVector==============\n",
      "Epoch 1/10\n",
      "290589/290589 [==============================] - 93s 319us/step - loss: 0.7055 - acc: 0.5070\n",
      "Epoch 2/10\n",
      "290589/290589 [==============================] - 93s 322us/step - loss: 0.6931 - acc: 0.5073\n",
      "Epoch 3/10\n",
      "290589/290589 [==============================] - 93s 321us/step - loss: 0.6931 - acc: 0.5072\n",
      "Epoch 4/10\n",
      "290589/290589 [==============================] - 92s 315us/step - loss: 0.6931 - acc: 0.5071\n",
      "Epoch 5/10\n",
      "290589/290589 [==============================] - 92s 316us/step - loss: 0.6931 - acc: 0.5080\n",
      "Epoch 6/10\n",
      "290589/290589 [==============================] - 92s 318us/step - loss: 0.6931 - acc: 0.5080\n",
      "Epoch 7/10\n",
      "290589/290589 [==============================] - 93s 319us/step - loss: 0.6931 - acc: 0.5074\n",
      "Epoch 8/10\n",
      "290589/290589 [==============================] - 93s 319us/step - loss: 0.6931 - acc: 0.5077\n",
      "Epoch 9/10\n",
      "290589/290589 [==============================] - 92s 318us/step - loss: 0.6931 - acc: 0.5073\n",
      "Epoch 10/10\n",
      "290589/290589 [==============================] - 91s 315us/step - loss: 0.6931 - acc: 0.5082\n",
      "72648/72648 [==============================] - 8s 116us/step\n",
      "[0.6923282538591837, 0.520895285103189]\n",
      "DsVector_rate==============\n",
      "Epoch 1/10\n",
      "290589/290589 [==============================] - 93s 322us/step - loss: 0.6931 - acc: 0.5076\n",
      "Epoch 2/10\n",
      "290589/290589 [==============================] - 93s 319us/step - loss: 0.6931 - acc: 0.5075\n",
      "Epoch 3/10\n",
      "290589/290589 [==============================] - 93s 320us/step - loss: 0.6931 - acc: 0.5078\n",
      "Epoch 4/10\n",
      "290589/290589 [==============================] - 94s 322us/step - loss: 0.6931 - acc: 0.5077\n",
      "Epoch 5/10\n",
      "290589/290589 [==============================] - 92s 315us/step - loss: 0.6931 - acc: 0.5078\n",
      "Epoch 6/10\n",
      "290589/290589 [==============================] - 92s 317us/step - loss: 0.6931 - acc: 0.5067\n",
      "Epoch 7/10\n",
      "290589/290589 [==============================] - 93s 321us/step - loss: 0.6931 - acc: 0.5069\n",
      "Epoch 8/10\n",
      "290589/290589 [==============================] - 94s 323us/step - loss: 0.6931 - acc: 0.5078\n",
      "Epoch 9/10\n",
      "290589/290589 [==============================] - 93s 318us/step - loss: 0.6931 - acc: 0.5079\n",
      "Epoch 10/10\n",
      "290589/290589 [==============================] - 94s 323us/step - loss: 0.6931 - acc: 0.5070\n",
      "72648/72648 [==============================] - 9s 121us/step\n",
      "[0.6927924251952842, 0.520895285103189]\n",
      "SnVector==============\n",
      "Epoch 1/10\n",
      "290589/290589 [==============================] - 96s 330us/step - loss: 0.7002 - acc: 0.5066\n",
      "Epoch 2/10\n",
      "290589/290589 [==============================] - 94s 324us/step - loss: 0.6931 - acc: 0.5073\n",
      "Epoch 3/10\n",
      "290589/290589 [==============================] - 95s 328us/step - loss: 0.6931 - acc: 0.5071\n",
      "Epoch 4/10\n",
      "290589/290589 [==============================] - 94s 325us/step - loss: 0.6931 - acc: 0.5080\n",
      "Epoch 5/10\n",
      "290589/290589 [==============================] - 94s 324us/step - loss: 0.6931 - acc: 0.5076\n",
      "Epoch 6/10\n",
      "290589/290589 [==============================] - 94s 323us/step - loss: 0.6931 - acc: 0.5077\n",
      "Epoch 7/10\n",
      "290589/290589 [==============================] - 95s 326us/step - loss: 0.6931 - acc: 0.5084\n",
      "Epoch 8/10\n",
      "290589/290589 [==============================] - 95s 326us/step - loss: 0.6931 - acc: 0.5072\n",
      "Epoch 9/10\n",
      "290589/290589 [==============================] - 94s 324us/step - loss: 0.6931 - acc: 0.5079\n",
      "Epoch 10/10\n",
      "290589/290589 [==============================] - 94s 325us/step - loss: 0.6931 - acc: 0.5063\n",
      "72648/72648 [==============================] - 9s 126us/step\n",
      "[0.6923900348717992, 0.520895285103189]\n",
      "DsVector==============\n",
      "Epoch 1/10\n",
      "290589/290589 [==============================] - 96s 332us/step - loss: 0.6931 - acc: 0.5070\n",
      "Epoch 2/10\n",
      "290589/290589 [==============================] - 95s 326us/step - loss: 0.6931 - acc: 0.5070\n",
      "Epoch 3/10\n",
      "290589/290589 [==============================] - 96s 329us/step - loss: 0.6931 - acc: 0.5076\n",
      "Epoch 4/10\n",
      "290589/290589 [==============================] - 96s 331us/step - loss: 0.6931 - acc: 0.5069\n",
      "Epoch 5/10\n",
      "290589/290589 [==============================] - 96s 332us/step - loss: 0.6931 - acc: 0.5076\n",
      "Epoch 6/10\n",
      "290589/290589 [==============================] - 95s 327us/step - loss: 0.6931 - acc: 0.5074\n",
      "Epoch 7/10\n",
      "290589/290589 [==============================] - 96s 331us/step - loss: 0.6931 - acc: 0.5071\n",
      "Epoch 8/10\n",
      "290589/290589 [==============================] - 96s 332us/step - loss: 0.6931 - acc: 0.5069\n",
      "Epoch 9/10\n",
      "290589/290589 [==============================] - 98s 337us/step - loss: 0.6931 - acc: 0.5074\n",
      "Epoch 10/10\n",
      "290589/290589 [==============================] - 97s 333us/step - loss: 0.6930 - acc: 0.5079\n",
      "72648/72648 [==============================] - 11s 147us/step\n",
      "[0.6924278195186434, 0.520895285103189]\n",
      "PmiVector==============\n",
      "Epoch 1/10\n",
      "290589/290589 [==============================] - 99s 340us/step - loss: 0.6970 - acc: 0.5075\n",
      "Epoch 2/10\n",
      "290589/290589 [==============================] - 97s 334us/step - loss: 0.6931 - acc: 0.5080\n",
      "Epoch 3/10\n",
      "290589/290589 [==============================] - 98s 338us/step - loss: 0.6931 - acc: 0.5065\n",
      "Epoch 4/10\n",
      "290589/290589 [==============================] - 98s 336us/step - loss: 0.6931 - acc: 0.5087\n",
      "Epoch 5/10\n",
      "290589/290589 [==============================] - 97s 335us/step - loss: 0.6931 - acc: 0.5077\n",
      "Epoch 6/10\n",
      "290589/290589 [==============================] - 97s 335us/step - loss: 0.6931 - acc: 0.5069\n",
      "Epoch 7/10\n",
      "290589/290589 [==============================] - 96s 331us/step - loss: 0.6931 - acc: 0.5074\n",
      "Epoch 8/10\n",
      "290589/290589 [==============================] - 97s 335us/step - loss: 0.6931 - acc: 0.5073\n",
      "Epoch 9/10\n",
      "290589/290589 [==============================] - 98s 336us/step - loss: 0.6931 - acc: 0.5078\n",
      "Epoch 10/10\n",
      "290589/290589 [==============================] - 98s 338us/step - loss: 0.6931 - acc: 0.5069\n",
      "72648/72648 [==============================] - 10s 137us/step\n",
      "[0.6923520357391406, 0.520895285103189]\n"
     ]
    }
   ],
   "source": [
    "# dnn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Y = [np.sign(data['rate']) for data in datas]\n",
    "for vectorname in Xs.keys():\n",
    "    print(vectorname+'==============')\n",
    "    X = Xs[vectorname]\n",
    "    train_x,test_x,Y1,Y2 = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "    # train_x = [data['DsVector_rate'] for data in datas]\n",
    "    # Y = [np.sign(data['rate']) for data in datas]\n",
    "    # test_x = [data['DsVector_rate'] for data in datas2[2000:]]\n",
    "    # Y2 = [np.sign(data['rate']) for data in datas2[2000:]]\n",
    "\n",
    "    train_y = []\n",
    "    for y in Y1:\n",
    "        if y == 1:\n",
    "            train_y.append(np.array([0,1]))\n",
    "        else:    \n",
    "            train_y.append(np.array([1,0]))\n",
    "\n",
    "    test_y = []\n",
    "    for y in Y2:\n",
    "        if y == 1:\n",
    "            test_y.append(np.array([0,1]))\n",
    "        else:    \n",
    "            test_y.append(np.array([1,0]))\n",
    "\n",
    "    num_classes = 2\n",
    "    \n",
    "#     train_y = to_categorical(train_y,num_classes=num_classes)\n",
    "#     test_y = to_categorical(test_y,num_classes=num_classes)\n",
    "    nmodel = Sequential()\n",
    "    nmodel.add(Dense(units=num_classes, activation = 'relu', input_dim = np.array(train_x).shape[1]))\n",
    "    nmodel.add(Dropout(0.5))\n",
    "    nmodel.add(Dense(2, activation = 'relu'))\n",
    "    nmodel.add(Dropout(0.5))\n",
    "    # dropout:https://blog.csdn.net/program_developer/article/details/80737724\n",
    "    nmodel.add(Dense(2, activation = 'softmax'))\n",
    "    nmodel.compile(loss = 'categorical_crossentropy',\n",
    "                   optimizer = 'adam',\n",
    "                   metrics = ['accuracy'])\n",
    "    nmodel.fit(np.array(train_x),np.array(train_y),epochs=10, batch_size=5)\n",
    "    print(nmodel.evaluate(np.array(test_x),np.array(test_y), batch_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/21805 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 21805/21805 [00:00<00:00, 409323.55it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlVector==============\n",
      "(21805, 2)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-6337b1c8305c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;31m#     model.add(Dropout(dropout))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#     model.add(LSTM(neurons, return_sequences=True, activation=activation_function))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# rnn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM\n",
    "\n",
    "for vectorname in Xs.keys():\n",
    "    print(vectorname+'==============')\n",
    "    X1 = Xs[vectorname]\n",
    "    X = []\n",
    "    for idx in tqdm(range(0,len(X1))):\n",
    "        X.append([[X1[idx],1]])\n",
    "    X = np.array(X)\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    print(X.shape)\n",
    "    train_x,test_x,Y1,Y2 = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "\n",
    "    train_y = []\n",
    "    for y in Y1:\n",
    "        if y == 1:\n",
    "            train_y.append(np.array([0,1]))\n",
    "        else:    \n",
    "            train_y.append(np.array([1,0]))\n",
    "\n",
    "    test_y = []\n",
    "    for y in Y2:\n",
    "        if y == 1:\n",
    "            test_y.append(np.array([0,1]))\n",
    "        else:    \n",
    "            test_y.append(np.array([1,0]))\n",
    "\n",
    "    num_classes = 2\n",
    "    neurons = 2                 \n",
    "    activation_function = 'tanh'  \n",
    "    loss = 'mse'                  \n",
    "    optimizer=\"adam\"              \n",
    "    dropout = 0.25                 \n",
    "    batch_size = 12               \n",
    "    epochs = 53\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(2, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(LSTM(neurons, return_sequences=True, activation=activation_function))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(LSTM(neurons, activation=activation_function))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(Dense(units=1))\n",
    "#     model.add(Activation(activation_function))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    model.fit(np.array(train_x),np.array(train_y),epochs=10, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/21805 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 16837/21805 [00:59<00:21, 230.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 258/21805 [00:00<00:08, 2571.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 516/21805 [00:00<00:08, 2568.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 776/21805 [00:00<00:08, 2576.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 991/21805 [00:00<00:08, 2421.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 1240/21805 [00:00<00:08, 2439.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 1493/21805 [00:00<00:08, 2465.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 1774/21805 [00:00<00:07, 2558.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 2027/21805 [00:00<00:07, 2531.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 2267/21805 [00:00<00:07, 2471.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 2510/21805 [00:01<00:07, 2453.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 2755/21805 [00:01<00:07, 2450.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▎        | 2996/21805 [00:01<00:07, 2404.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 3234/21805 [00:01<00:07, 2335.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 3466/21805 [00:01<00:08, 2252.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 3697/21805 [00:01<00:07, 2268.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 3939/21805 [00:01<00:07, 2310.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 4178/21805 [00:01<00:07, 2331.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 4412/21805 [00:01<00:07, 2236.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██▏       | 4637/21805 [00:01<00:07, 2222.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 4860/21805 [00:02<00:07, 2178.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 5079/21805 [00:02<00:07, 2099.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 5318/21805 [00:02<00:07, 2175.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 5537/21805 [00:02<00:07, 2140.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▋       | 5774/21805 [00:02<00:07, 2202.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 6027/21805 [00:02<00:06, 2288.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 6258/21805 [00:02<00:07, 2204.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 6498/21805 [00:02<00:06, 2252.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 6725/21805 [00:02<00:06, 2246.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 6954/21805 [00:02<00:06, 2258.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 7183/21805 [00:03<00:06, 2267.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 7411/21805 [00:03<00:06, 2238.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 7650/21805 [00:03<00:06, 2281.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 7879/21805 [00:03<00:06, 2203.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 8101/21805 [00:03<00:06, 2163.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 8322/21805 [00:03<00:06, 2172.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 8554/21805 [00:03<00:05, 2212.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 8776/21805 [00:03<00:05, 2175.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████▏     | 9012/21805 [00:03<00:05, 2227.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 9236/21805 [00:04<00:05, 2228.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 9469/21805 [00:04<00:05, 2254.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 9695/21805 [00:04<00:05, 2239.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 9920/21805 [00:04<00:05, 2229.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 10164/21805 [00:04<00:05, 2287.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 10394/21805 [00:04<00:04, 2289.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▊     | 10624/21805 [00:04<00:04, 2245.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████▉     | 10861/21805 [00:04<00:04, 2279.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 11090/21805 [00:04<00:04, 2243.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 11315/21805 [00:04<00:04, 2183.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 11549/21805 [00:05<00:04, 2227.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 11794/21805 [00:05<00:04, 2287.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 12024/21805 [00:05<00:04, 2279.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▋    | 12290/21805 [00:05<00:03, 2380.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 12539/21805 [00:05<00:04, 2260.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 12884/21805 [00:05<00:03, 2521.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 13260/21805 [00:05<00:03, 2797.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 13558/21805 [00:05<00:03, 2711.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 13842/21805 [00:05<00:03, 2599.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 14204/21805 [00:06<00:02, 2837.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 14565/21805 [00:06<00:02, 3031.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 15000/21805 [00:06<00:02, 3334.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 15407/21805 [00:06<00:01, 3525.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 15776/21805 [00:06<00:01, 3394.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 16128/21805 [00:06<00:01, 3225.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 16505/21805 [00:06<00:01, 3370.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 16922/21805 [00:06<00:01, 3576.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 17329/21805 [00:06<00:01, 3709.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████▏ | 17748/21805 [00:06<00:01, 3838.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 18192/21805 [00:07<00:00, 4001.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 18632/21805 [00:07<00:00, 4111.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 19057/21805 [00:07<00:00, 4151.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 19476/21805 [00:07<00:00, 3879.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████▏| 19913/21805 [00:07<00:00, 4012.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 20343/21805 [00:07<00:00, 4090.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 20779/21805 [00:07<00:00, 4166.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 21214/21805 [00:07<00:00, 4218.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 21639/21805 [00:07<00:00, 4113.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 21805/21805 [00:07<00:00, 2736.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/109174 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 71062/109174 [00:00<00:00, 710613.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 109174/109174 [00:00<00:00, 698857.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlVector==============\n",
      "recall： 0.5030578287994746\n",
      "precision： 0.513431391040007\n",
      "f1_score 0.5081916773399072\n",
      "voting\n",
      "voting accuracy： 0.5166246273790415\n",
      "voting recall： 0.5032122199489782\n",
      "voting precision： 0.549419056501127\n",
      "voting f1_score 0.5253014788551367\n",
      "\n",
      "DsVector_rate==============\n",
      "recall： 0.5094570104316638\n",
      "precision： 0.5233958096404514\n",
      "f1_score 0.5163323549492113\n",
      "voting\n",
      "voting accuracy： 0.4916303600091722\n",
      "voting recall： 0.5033936584914079\n",
      "voting precision： 0.5120534259151274\n",
      "voting f1_score 0.5076866168070631\n",
      "\n",
      "SnVector==============\n",
      "recall： 0.5093620814494877\n",
      "precision： 0.5387957051147861\n",
      "f1_score 0.523665626208622\n",
      "voting\n",
      "voting accuracy： 0.5141022701215318\n",
      "voting recall： 0.5006688178289679\n",
      "voting precision： 0.5107294325810068\n",
      "voting f1_score 0.5056490875619778\n",
      "\n",
      "DsVector==============\n",
      "recall： 0.5063655039444992\n",
      "precision： 0.515294145015723\n",
      "f1_score 0.5107908092212332\n",
      "voting\n",
      "voting accuracy： 0.4916303600091722\n",
      "voting recall： 0.5031007678512793\n",
      "voting precision： 0.5097846402767316\n",
      "voting f1_score 0.5064206511495094\n",
      "\n",
      "PmiVector==============\n",
      "recall： 0.5531813838161872\n",
      "precision： 0.5953209237049305\n",
      "f1_score 0.5734780857351809\n",
      "voting\n",
      "voting accuracy： 0.5090575556065122\n",
      "voting recall： 0.5195735773281807\n",
      "voting precision： 0.5457994119472229\n",
      "voting f1_score 0.5323636995188143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# company Apple\n",
    "appledatas = [data for data in datas if data['company'] =='Apple Inc.']\n",
    "apple_count = train_sent_dict(appledatas)\n",
    "news2vector(appledatas,apple_count,bl_sent)\n",
    "\n",
    "apple_X1 = [data['DsVector'] for data in appledatas]\n",
    "apple_X2 = [data['SnVector'] for data in appledatas]\n",
    "apple_X3 = [data['BlVector'] for data in appledatas]\n",
    "apple_X4 = [data['PmiVector'] for data in appledatas]\n",
    "# X = [data['ContextVector'] for data in datas]\n",
    "apple_X5 = [data['DsVector_rate'] for data in appledatas]\n",
    "\n",
    "apple_Xs = {'DsVector':apple_X1,'SnVector':apple_X2,'BlVector':apple_X3,'PmiVector':apple_X4,'DsVector_rate':apple_X5}\n",
    "\n",
    "Y = [np.sign(data['rate']) for data in appledatas]\n",
    "\n",
    "for vectorname in apple_Xs.keys():\n",
    "    print(vectorname+'==============')\n",
    "    X = apple_Xs[vectorname]\n",
    "    train_x,test_x,train_y,test_y = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    \n",
    "    clf.fit(np.array(train_x), np.array(train_y))\n",
    "    predict_y = clf.predict(test_x)\n",
    "    recall = recall_score(test_y,clf.predict(test_x),average = 'macro')\n",
    "    precision = precision_score(test_y, clf.predict(test_x), average='macro')\n",
    "\n",
    "    print('recall：',recall)\n",
    "    print('precision：',precision)\n",
    "    print('f1_score',2*recall*precision/(recall+precision))\n",
    "\n",
    "    vote_predict_y = vote(predict_y,datas[-len(test_x):])\n",
    "    vote_recall = recall_score(test_y,vote_predict_y,average = 'macro')\n",
    "    vote_precision = precision_score(test_y, vote_predict_y, average='macro')\n",
    "\n",
    "    print('voting accuracy：',accuracy(vote_predict_y,test_y))\n",
    "    print('voting recall：',vote_recall)\n",
    "    print('voting precision：',vote_precision)\n",
    "    print('voting f1_score',2*vote_recall*vote_precision/(vote_recall+vote_precision))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlVector==============\n",
      "recall： 0.49998793967470384\n",
      "precision： 0.4271112946520752\n",
      "f1_score 0.4606853038336461\n",
      "voting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stocksentiment/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting accuracy： 0.5208952758506772\n",
      "voting recall： 0.5\n",
      "voting precision： 0.2604476379253386\n",
      "voting f1_score 0.34249253326092854\n",
      "\n",
      "DsVector_rate==============\n",
      "recall： 0.5009265814680646\n",
      "precision： 0.529247230471489\n",
      "f1_score 0.5146976225543572\n",
      "voting\n",
      "voting accuracy： 0.5209365708622399\n",
      "voting recall： 0.5001306865504281\n",
      "voting precision： 0.5153190319154286\n",
      "voting f1_score 0.507611271218312\n",
      "\n",
      "SnVector==============\n",
      "recall： 0.49985037680282735\n",
      "precision： 0.4860512225962932\n",
      "f1_score 0.4928542298913098\n",
      "voting\n",
      "voting accuracy： 0.5206199757735932\n",
      "voting recall： 0.4997530309590441\n",
      "voting precision： 0.41038596104575886\n",
      "voting f1_score 0.4506819940631131\n",
      "\n",
      "DsVector==============\n",
      "recall： 0.5044502805334159\n",
      "precision： 0.5429033077458991\n",
      "f1_score 0.522970902968638\n",
      "voting\n",
      "voting accuracy： 0.5286036780090299\n",
      "voting recall： 0.5084375949024605\n",
      "voting precision： 0.6253113955024281\n",
      "voting f1_score 0.5608504610545494\n",
      "\n",
      "PmiVector==============\n",
      "recall： 0.5061807470269806\n",
      "precision： 0.5518193638132911\n",
      "f1_score 0.5280157061176989\n",
      "voting\n",
      "voting accuracy： 0.5341922695738355\n",
      "voting recall： 0.5144704588811876\n",
      "voting precision： 0.6351529419026016\n",
      "voting f1_score 0.568477338331117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# company Apple Randomforest\n",
    "\n",
    "for vectorname in Xs.keys():\n",
    "    print(vectorname+'==============')\n",
    "    X = apple_Xs[vectorname]\n",
    "    train_x,test_x,train_y,test_y = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    \n",
    "    clf.fit(np.array(train_x), np.array(train_y))\n",
    "    predict_y = clf.predict(test_x)\n",
    "    recall = recall_score(test_y,clf.predict(test_x),average = 'macro')\n",
    "    precision = precision_score(test_y, clf.predict(test_x), average='macro')\n",
    "\n",
    "    print('recall：',recall)\n",
    "    print('precision：',precision)\n",
    "    print('f1_score',2*recall*precision/(recall+precision))\n",
    "\n",
    "    vote_predict_y = vote(predict_y,datas[-len(test_x):])\n",
    "    vote_recall = recall_score(test_y,vote_predict_y,average = 'macro')\n",
    "    vote_precision = precision_score(test_y, vote_predict_y, average='macro')\n",
    "\n",
    "    print('voting accuracy：',accuracy(vote_predict_y,test_y))\n",
    "    print('voting recall：',vote_recall)\n",
    "    print('voting precision：',vote_precision)\n",
    "    print('voting f1_score',2*vote_recall*vote_precision/(vote_recall+vote_precision))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlVector==============\n",
      "Epoch 1/10\n",
      "17444/17444 [==============================] - 7s 385us/step - loss: 0.7225 - acc: 0.4997\n",
      "Epoch 2/10\n",
      "17444/17444 [==============================] - 5s 315us/step - loss: 0.6961 - acc: 0.5000\n",
      "Epoch 3/10\n",
      "17444/17444 [==============================] - 6s 342us/step - loss: 0.6941 - acc: 0.5024\n",
      "Epoch 4/10\n",
      "17444/17444 [==============================] - 6s 344us/step - loss: 0.6934 - acc: 0.4985\n",
      "Epoch 5/10\n",
      "17444/17444 [==============================] - 6s 343us/step - loss: 0.6933 - acc: 0.4981\n",
      "Epoch 6/10\n",
      "17444/17444 [==============================] - 6s 349us/step - loss: 0.6933 - acc: 0.4915\n",
      "Epoch 7/10\n",
      "17444/17444 [==============================] - 6s 344us/step - loss: 0.6933 - acc: 0.5014\n",
      "Epoch 8/10\n",
      "17444/17444 [==============================] - 6s 337us/step - loss: 0.6933 - acc: 0.4981\n",
      "Epoch 9/10\n",
      "17444/17444 [==============================] - 6s 350us/step - loss: 0.6933 - acc: 0.5004\n",
      "Epoch 10/10\n",
      "17444/17444 [==============================] - 6s 330us/step - loss: 0.6933 - acc: 0.5010\n",
      "4361/4361 [==============================] - 1s 312us/step\n",
      "[0.6928118400906128, 0.5138729678035958]\n",
      "DsVector_rate==============\n",
      "Epoch 1/10\n",
      "17444/17444 [==============================] - 8s 448us/step - loss: 0.6940 - acc: 0.4972\n",
      "Epoch 2/10\n",
      "17444/17444 [==============================] - 6s 358us/step - loss: 0.6936 - acc: 0.5000\n",
      "Epoch 3/10\n",
      "17444/17444 [==============================] - 6s 353us/step - loss: 0.6935 - acc: 0.4970\n",
      "Epoch 4/10\n",
      "17444/17444 [==============================] - 6s 353us/step - loss: 0.6933 - acc: 0.5001\n",
      "Epoch 5/10\n",
      "17444/17444 [==============================] - 6s 362us/step - loss: 0.6934 - acc: 0.4917\n",
      "Epoch 6/10\n",
      "17444/17444 [==============================] - 6s 354us/step - loss: 0.6933 - acc: 0.4988\n",
      "Epoch 7/10\n",
      "17444/17444 [==============================] - 6s 348us/step - loss: 0.6933 - acc: 0.4987\n",
      "Epoch 8/10\n",
      "17444/17444 [==============================] - 6s 350us/step - loss: 0.6933 - acc: 0.5039\n",
      "Epoch 9/10\n",
      "17444/17444 [==============================] - 6s 365us/step - loss: 0.6933 - acc: 0.4993\n",
      "Epoch 10/10\n",
      "17444/17444 [==============================] - 6s 358us/step - loss: 0.6931 - acc: 0.5026\n",
      "4361/4361 [==============================] - 2s 351us/step\n",
      "[0.6938163645085433, 0.4861270376634663]\n",
      "SnVector==============\n",
      "Epoch 1/10\n",
      "17444/17444 [==============================] - 8s 436us/step - loss: 0.7217 - acc: 0.4982\n",
      "Epoch 2/10\n",
      "17444/17444 [==============================] - 6s 372us/step - loss: 0.7008 - acc: 0.4943\n",
      "Epoch 3/10\n",
      "17444/17444 [==============================] - 7s 427us/step - loss: 0.6963 - acc: 0.5002\n",
      "Epoch 4/10\n",
      "17444/17444 [==============================] - 8s 443us/step - loss: 0.6939 - acc: 0.5015\n",
      "Epoch 5/10\n",
      "17444/17444 [==============================] - 8s 447us/step - loss: 0.6934 - acc: 0.4982\n",
      "Epoch 6/10\n",
      "17444/17444 [==============================] - 8s 463us/step - loss: 0.6933 - acc: 0.5008\n",
      "Epoch 7/10\n",
      "17444/17444 [==============================] - 8s 447us/step - loss: 0.6933 - acc: 0.5005\n",
      "Epoch 8/10\n",
      "17444/17444 [==============================] - 8s 458us/step - loss: 0.6934 - acc: 0.4987\n",
      "Epoch 9/10\n",
      "17444/17444 [==============================] - 8s 453us/step - loss: 0.6933 - acc: 0.4978\n",
      "Epoch 10/10\n",
      "17444/17444 [==============================] - 8s 439us/step - loss: 0.6933 - acc: 0.4981\n",
      "4361/4361 [==============================] - 2s 447us/step\n",
      "[0.6928484585547059, 0.514560883429531]\n",
      "DsVector==============\n",
      "Epoch 1/10\n",
      "17444/17444 [==============================] - 10s 573us/step - loss: 0.6920 - acc: 0.5112\n",
      "Epoch 2/10\n",
      "17444/17444 [==============================] - 8s 458us/step - loss: 0.6920 - acc: 0.5175\n",
      "Epoch 3/10\n",
      "17444/17444 [==============================] - 8s 465us/step - loss: 0.6912 - acc: 0.5147\n",
      "Epoch 4/10\n",
      "17444/17444 [==============================] - 8s 468us/step - loss: 0.6912 - acc: 0.5221\n",
      "Epoch 5/10\n",
      "17444/17444 [==============================] - 8s 460us/step - loss: 0.6908 - acc: 0.5220\n",
      "Epoch 6/10\n",
      "17444/17444 [==============================] - 8s 461us/step - loss: 0.6895 - acc: 0.5242\n",
      "Epoch 7/10\n",
      "17444/17444 [==============================] - 8s 472us/step - loss: 0.6898 - acc: 0.5292\n",
      "Epoch 8/10\n",
      "17444/17444 [==============================] - 8s 456us/step - loss: 0.6897 - acc: 0.5291\n",
      "Epoch 9/10\n",
      "17444/17444 [==============================] - 8s 473us/step - loss: 0.6901 - acc: 0.5255\n",
      "Epoch 10/10\n",
      "17444/17444 [==============================] - 8s 470us/step - loss: 0.6905 - acc: 0.5171\n",
      "4361/4361 [==============================] - 2s 485us/step\n",
      "[0.6875273460670502, 0.5663838648544597]\n",
      "PmiVector==============\n",
      "Epoch 1/10\n",
      "17444/17444 [==============================] - 11s 610us/step - loss: 0.7061 - acc: 0.5118\n",
      "Epoch 2/10\n",
      "17444/17444 [==============================] - 8s 472us/step - loss: 0.6910 - acc: 0.5139\n",
      "Epoch 3/10\n",
      "17444/17444 [==============================] - 8s 463us/step - loss: 0.6893 - acc: 0.5253\n",
      "Epoch 4/10\n",
      "17444/17444 [==============================] - 8s 457us/step - loss: 0.6896 - acc: 0.5236\n",
      "Epoch 5/10\n",
      "17444/17444 [==============================] - 8s 459us/step - loss: 0.6899 - acc: 0.5225\n",
      "Epoch 6/10\n",
      "17444/17444 [==============================] - 8s 462us/step - loss: 0.6891 - acc: 0.5266\n",
      "Epoch 7/10\n",
      "17444/17444 [==============================] - 8s 466us/step - loss: 0.6870 - acc: 0.5316\n",
      "Epoch 8/10\n",
      "17444/17444 [==============================] - 8s 461us/step - loss: 0.6893 - acc: 0.5256\n",
      "Epoch 9/10\n",
      "17444/17444 [==============================] - 8s 472us/step - loss: 0.6887 - acc: 0.5261\n",
      "Epoch 10/10\n",
      "17444/17444 [==============================] - 8s 464us/step - loss: 0.6889 - acc: 0.5290\n",
      "4361/4361 [==============================] - 2s 466us/step\n",
      "[0.6846363094388223, 0.6124742137739293]\n"
     ]
    }
   ],
   "source": [
    "# company Apple DNN\n",
    "for vectorname in Xs.keys():\n",
    "    print(vectorname+'==============')\n",
    "    X = apple_Xs[vectorname]\n",
    "    train_x,test_x,Y1,Y2 = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "    # train_x = [data['DsVector_rate'] for data in datas]\n",
    "    # Y = [np.sign(data['rate']) for data in datas]\n",
    "    # test_x = [data['DsVector_rate'] for data in datas2[2000:]]\n",
    "    # Y2 = [np.sign(data['rate']) for data in datas2[2000:]]\n",
    "\n",
    "    train_y = []\n",
    "    for y in Y1:\n",
    "        if y == 1:\n",
    "            train_y.append(np.array([0,1]))\n",
    "        else:    \n",
    "            train_y.append(np.array([1,0]))\n",
    "\n",
    "    test_y = []\n",
    "    for y in Y2:\n",
    "        if y == 1:\n",
    "            test_y.append(np.array([0,1]))\n",
    "        else:    \n",
    "            test_y.append(np.array([1,0]))\n",
    "\n",
    "    num_classes = 2\n",
    "    \n",
    "#     train_y = to_categorical(train_y,num_classes=num_classes)\n",
    "#     test_y = to_categorical(test_y,num_classes=num_classes)\n",
    "    nmodel = Sequential()\n",
    "    nmodel.add(Dense(units=num_classes, activation = 'relu', input_dim = np.array(train_x).shape[1]))\n",
    "    nmodel.add(Dropout(0.5))\n",
    "    nmodel.add(Dense(2, activation = 'relu'))\n",
    "    nmodel.add(Dropout(0.5))\n",
    "    # dropout:https://blog.csdn.net/program_developer/article/details/80737724\n",
    "    nmodel.add(Dense(2, activation = 'softmax'))\n",
    "    nmodel.compile(loss = 'categorical_crossentropy',\n",
    "                   optimizer = 'adam',\n",
    "                   metrics = ['accuracy'])\n",
    "    nmodel.fit(np.array(train_x),np.array(train_y),epochs=10, batch_size=5)\n",
    "    print(nmodel.evaluate(np.array(test_x),np.array(test_y), batch_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Y = [np.sign(data['rate']) for data in datas]\n",
    "for vectorname in apple_Xs.keys():\n",
    "    print(vectorname+'==============')\n",
    "    X = apple_Xs[vectorname]\n",
    "    train_x,test_x,Y1,Y2 = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "    # train_x = [data['DsVector_rate'] for data in datas]\n",
    "    # Y = [np.sign(data['rate']) for data in datas]\n",
    "    # test_x = [data['DsVector_rate'] for data in datas2[2000:]]\n",
    "    # Y2 = [np.sign(data['rate']) for data in datas2[2000:]]\n",
    "\n",
    "    train_y = []\n",
    "    for y in Y1:\n",
    "        if y == 1:\n",
    "            train_y.append(np.array([0,1]))\n",
    "        else:    \n",
    "            train_y.append(np.array([1,0]))\n",
    "\n",
    "    test_y = []\n",
    "    for y in Y2:\n",
    "        if y == 1:\n",
    "            test_y.append(np.array([0,1]))\n",
    "        else:    \n",
    "            test_y.append(np.array([1,0]))\n",
    "\n",
    "    num_classes = 2\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, return_sequences=True, \n",
    "    input_shape=(inputs.shape[1], inputs.shape[2]), activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(neurons, return_sequences=True, activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(neurons, activation=activ_func))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(activ_func))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate\n",
    "dates = [data['date'] for data in datas]\n",
    "idxs = []\n",
    "for date in ['2016-12-29','2017-01-30','2017-02-28','2017-03-30','2017-04-28','2017-05-30','2017-06-30','2017-07-31','2017-08-30','2017-09-29','2017-10-30','2017-11-30','2017-12-29']:\n",
    "    idxs.append(dates.index(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[318950,\n",
       " 321630,\n",
       " 324727,\n",
       " 327362,\n",
       " 330253,\n",
       " 333254,\n",
       " 336362,\n",
       " 339416,\n",
       " 342353,\n",
       " 345361,\n",
       " 348815,\n",
       " 352002,\n",
       " 354236]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-29\n",
      "voting\n",
      "voting accuracy： 0.5055970149253731\n",
      "voting recall： 0.01664145234493192\n",
      "voting precision： 0.46808510638297873\n",
      "voting f1_score 0.03214024835646457\n",
      "\n",
      "===========\n",
      "2017-01-30\n",
      "voting\n",
      "voting accuracy： 0.4304165321278657\n",
      "voting recall： 0.038396386222473176\n",
      "voting precision： 0.5271317829457365\n",
      "voting f1_score 0.07157894736842105\n",
      "\n",
      "===========\n",
      "2017-02-28\n",
      "voting\n",
      "voting accuracy： 0.4990512333965844\n",
      "voting recall： 0.027169811320754716\n",
      "voting precision： 0.5373134328358209\n",
      "voting f1_score 0.05172413793103449\n",
      "\n",
      "===========\n",
      "2017-03-30\n",
      "voting\n",
      "voting accuracy： 0.5264614320304393\n",
      "voting recall： 0.019103600293901544\n",
      "voting precision： 0.43333333333333335\n",
      "voting f1_score 0.036593947923997186\n",
      "\n",
      "===========\n",
      "2017-04-28\n",
      "voting\n",
      "voting accuracy： 0.4828390536487837\n",
      "voting recall： 0.02192134107027724\n",
      "voting precision： 0.4927536231884058\n",
      "voting f1_score 0.04197530864197531\n",
      "\n",
      "===========\n",
      "2017-05-30\n",
      "voting\n",
      "voting accuracy： 0.4874517374517375\n",
      "voting recall： 0.029746835443037974\n",
      "voting precision： 0.4392523364485981\n",
      "voting f1_score 0.05572021339656194\n",
      "\n",
      "===========\n",
      "2017-06-30\n",
      "voting\n",
      "voting accuracy： 0.4764243614931238\n",
      "voting recall： 0.04086687306501548\n",
      "voting precision： 0.5689655172413793\n",
      "voting f1_score 0.07625649913344887\n",
      "\n",
      "===========\n",
      "2017-07-31\n",
      "voting\n",
      "voting accuracy： 0.5182158665304732\n",
      "voting recall： 0.02781740370898716\n",
      "voting precision： 0.42857142857142855\n",
      "voting f1_score 0.052243804420629605\n",
      "\n",
      "===========\n",
      "2017-08-30\n",
      "voting\n",
      "voting accuracy： 0.4587765957446808\n",
      "voting recall： 0.022208513263417645\n",
      "voting precision： 0.45569620253164556\n",
      "voting f1_score 0.042352941176470586\n",
      "\n",
      "===========\n",
      "2017-09-29\n",
      "voting\n",
      "voting accuracy： 0.4594672843080486\n",
      "voting recall： 0.022848034006376194\n",
      "voting precision： 0.6056338028169014\n",
      "voting f1_score 0.044034818228366614\n",
      "\n",
      "===========\n",
      "2017-10-30\n",
      "voting\n",
      "voting accuracy： 0.4694069657985566\n",
      "voting recall： 0.0551967116852613\n",
      "voting precision： 0.5340909090909091\n",
      "voting f1_score 0.10005321979776476\n",
      "\n",
      "===========\n",
      "2017-11-30\n",
      "voting\n",
      "voting accuracy： 0.5138764547896151\n",
      "voting recall： 0.23721340388007053\n",
      "voting precision： 0.5489795918367347\n",
      "voting f1_score 0.3312807881773399\n",
      "\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "new_counts = []\n",
    "new_datas_list = []\n",
    "date_list = ['2016-12-29','2017-01-30','2017-02-28','2017-03-30','2017-04-28','2017-05-30','2017-06-30','2017-07-31','2017-08-30','2017-09-29','2017-10-30','2017-11-30','2017-12-29']\n",
    "for i in range(0,len(idxs)-1):\n",
    "    new_count = train_sent_dict(datas[:idxs[i]])\n",
    "    new_datas = datas.copy()\n",
    "    news2vector2(new_datas,new_count,bl_sent)\n",
    "    print(date_list[i])\n",
    "    X = [data['DsVector_rate'] for data in new_datas]\n",
    "    Y = [np.sign(data['rate']) for data in new_datas]\n",
    "    train_x = X[0:idxs[i]]\n",
    "    train_y = Y[0:idxs[i]]\n",
    "    test_x = X[idxs[i]:idxs[i+1]] \n",
    "    test_y = Y[idxs[i]:idxs[i+1]]\n",
    "\n",
    "    clf.fit(np.array(train_x), np.array(train_y))\n",
    "    predict_y = clf.predict(test_x)\n",
    "    recall = recall_score(test_y,clf.predict(test_x),average = 'binary')\n",
    "    precision = precision_score(test_y, clf.predict(test_x), average='binary')\n",
    "    clf = GaussianNB()\n",
    "\n",
    "#     print('recall：',recall)\n",
    "#     print('precision：',precision)\n",
    "#     print('f1_score',2*recall*precision/(recall+precision))\n",
    "\n",
    "    vote_predict_y = vote(predict_y,datas[-len(test_x):])\n",
    "    vote_recall = recall_score(test_y,vote_predict_y,average = 'binary')\n",
    "    vote_precision = precision_score(test_y, vote_predict_y, average='binary')\n",
    "\n",
    "    print('voting accuracy：',accuracy(vote_predict_y,test_y))\n",
    "    print('voting recall：',vote_recall)\n",
    "    print('voting precision：',vote_precision)\n",
    "    print('voting f1_score',2*vote_recall*vote_precision/(vote_recall+vote_precision))\n",
    "    print('')\n",
    "    print('===========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [data['DsVector_rate'] for data in new_datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17790151913386223,\n",
       " 0.01895255343326669,\n",
       " 0.47347614740668953,\n",
       " 0.2625919648718393]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[2121]['DsVector_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "news2vector(datas,count,bl_sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
