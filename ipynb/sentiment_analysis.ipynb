{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import IPython\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from nltk import sent_tokenize\n",
    "from senticnet.senticnet import SenticNet\n",
    "\n",
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 534/72648 [00:25<20:03, 59.93it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "adj = ['JJ','JJR','JJS']\n",
    "adv = ['RB','RBR','RBS']\n",
    "vb = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "nn = ['NN','NNS']\n",
    "\n",
    "sn = SenticNet()\n",
    "\n",
    "def review_to_words(review_text): \n",
    "    if '(Reuters) -' in review_text:\n",
    "        review_text = review_text.split('(Reuters) -')[1]\n",
    "    if '*' in review_text:\n",
    "        review_text = review_text.split('*')[1]\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.split()                             \n",
    "    _stopwords = set(stopwords.words(\"english\"))\n",
    "    _stopwords = nltk.corpus.stopwords.words('english')\n",
    "    _stopwords.append('would')\n",
    "    _stopwords.append('kmh')\n",
    "    _stopwords.append('mph')\n",
    "    _stopwords.append('  ')\n",
    "    _stopwords.append('Reuters')\n",
    "    _stopwords.append('reuters')\n",
    "    # _stopwords = []\n",
    "    meaningful_words = [w for w in words if w not in _stopwords]\n",
    "    return meaningful_words\n",
    "\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "# import spacy\n",
    "# nlp = spacy.load('en')\n",
    "# def stem_and_check(word):\n",
    "#     word = nlp(word)\n",
    "#     return word[0].lemma_\n",
    "    # word = inf.singularize(word)\n",
    "    # word = nltk.PorterStemmer().stem(word)\n",
    "    # if d.check(word):\n",
    "    #    return word\n",
    "    # suggest_words = d.suggest(word)\n",
    "    # if len(suggest_words) == 0:\n",
    "    #    return word\n",
    "    # return suggest_words[0]\n",
    "\n",
    "def my_read(path):\n",
    "    file = open(path)\n",
    "    words = []\n",
    "    for line in file.readlines():\n",
    "        words.append(line.strip())\n",
    "    return words\n",
    "\n",
    "    \n",
    "def output_cloud(count,name):\n",
    "    # 云图\n",
    "    text = '' \n",
    "    for key,value in count.items():\n",
    "        text += (key+' ') * (value)\n",
    "    wc = WordCloud(\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        max_font_size=100,      #字体大小\n",
    "        min_font_size=10,\n",
    "        collocations=False, \n",
    "        max_words=1000\n",
    "    )\n",
    "    wc.generate(text)\n",
    "    wc.to_file(name+'.png') #图片保存\n",
    "\n",
    "\n",
    "def vote(results,datas):\n",
    "    if len(results) != len(datas):\n",
    "        raise Exception(\"error\")\n",
    "    new_results = []\n",
    "    print('voting')\n",
    "    for i in range(0,len(results)):\n",
    "        count = 0\n",
    "        company = datas[i]['company']\n",
    "        date = datas[i]['date']\n",
    "        for data in datas:\n",
    "            if data['company'] == company and data['date'] == date:\n",
    "                idx = datas.index(data)\n",
    "                # if idx!=i:\n",
    "                #     print(i)\n",
    "                #     print(idx)\n",
    "                #     print('========')\n",
    "                count += results[idx]\n",
    "        if count < 0:  \n",
    "            new_results.append(np.float64(-1))\n",
    "        else:\n",
    "            new_results.append(np.float64(1))\n",
    "    return np.array(new_results)\n",
    "\n",
    "def accuracy(y,y2):\n",
    "    if len(y) != len(y2):\n",
    "        raise Exception(\"error\")\n",
    "    count = 0\n",
    "    for i in range(0,len(y)):\n",
    "        if y[i] == y2[i]:\n",
    "            count += 1\n",
    "    return count/len(y2)\n",
    "\n",
    "# concept_info = sn.concept('love')\n",
    "# polarity_value = sn.polarity_value('love')\n",
    "# polarity_intense = sn.polarity_intense('love')\n",
    "# moodtags = sn.moodtags('love')\n",
    "# semantics = sn.semantics('love')\n",
    "# sentics = sn.sentics('love') \n",
    " \n",
    "def sentiment_score(text):\n",
    "    t = TextBlob(text)\n",
    "    score = t.sentiment.polarity\n",
    "    return score\n",
    "    # tokens = nltk.word_tokenize(text)\n",
    "    # pos_tags = nltk.pos_tag(tokens)\n",
    "    # score = 0\n",
    "    # count = 0\n",
    "    # for word,tag in pos_tags:\n",
    "    #   if word in sn.data.keys():\n",
    "    #    score += float(sn.polarity_intense(word))\n",
    "    #    count += 1\n",
    "    #    # print(word,sn.polarity_intense(word))\n",
    "    # if count == 0: #mid\n",
    "    #   return -1 \n",
    "    # return score/count\n",
    "    \n",
    "def load_data(path):\n",
    "    workbook = xlrd.open_workbook(path)\n",
    "    worksheet = workbook.sheet_by_index(0)\n",
    "    contents = worksheet.col_values(1)\n",
    "    companies = worksheet.col_values(2)\n",
    "    prices = worksheet.col_values(3)\n",
    "    dates = worksheet.col_values(4)\n",
    "    datas = []\n",
    "    for i in tqdm(range(0,len(contents))):\n",
    "    # if '*' not in contents[i]:\n",
    "        # if companies[i] != 'Apple Inc.':\n",
    "        #     continue\n",
    "        price_list = json.loads(prices[i])   \n",
    "        if price_list[2] == price_list[1]:\n",
    "            continue\n",
    "        data = {}\n",
    "        data['content'] = contents[i]\n",
    "        sents = sent_tokenize(data['content'])\n",
    "        data['tokens'] = []\n",
    "        data['tags'] = []\n",
    "        for sent in sents:\n",
    "            token = review_to_words(sent) # 去停用词会影响词性标注吗？？\n",
    "            data['tokens'].append(token)\n",
    "            data['tags'].append(nltk.pos_tag(token))\n",
    "        data['company'] = companies[i]\n",
    "        data['rate'] = (price_list[2]-price_list[1])/price_list[1]\n",
    "        data['date'] = dates[i]\n",
    "        datas.append(data)\n",
    "\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sent_dict(datas):\n",
    "    count = {}\n",
    "\n",
    "    POS = 0\n",
    "    NEG = 0\n",
    "\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "\n",
    "    N = 0 #len of tokens\n",
    "        \n",
    "    for data in tqdm(datas):\n",
    "        for tokens in data['tokens']: \n",
    "            N += len(tokens)\n",
    "            rate = data['rate'] # 选当天的股票变化判断涨跌，因为相关度当天的最高\n",
    "            if rate>0:\n",
    "                pos_count += 1\n",
    "                POS += len(tokens)\n",
    "                for token in tokens:\n",
    "                    if len(token) < 3:\n",
    "                        continue\n",
    "                    if 'not' in tokens:\n",
    "                        token = 'not_'+token\n",
    "                    if token in count.keys():\n",
    "                        count[token]['pos'] += 1\n",
    "                        count[token]['pos_rate'] += rate\n",
    "                    else:\n",
    "                        count[token] = {'pos':1,'neg':0,'pos_rate':rate,'neg_rate':0} \n",
    "\n",
    "            if rate<0:\n",
    "                neg_count += 1\n",
    "                NEG += len(tokens)\n",
    "                for token in tokens:\n",
    "                    if len(token) < 3:\n",
    "                        continue\n",
    "                    if 'not' in tokens:\n",
    "                        token = 'not_'+token\n",
    "                    if token in count.keys():\n",
    "                        count[token]['neg'] += 1\n",
    "                        count[token]['neg_rate'] -= rate\n",
    "                    else:\n",
    "                        count[token] = {'pos':0,'neg':1,'pos_rate':0,'neg_rate':-rate}\n",
    "                        ## freq\n",
    "    copy = count.copy()\n",
    "    sent_words = [] # PD>0.3情感值\n",
    "\n",
    "    freq_pos = pos_count/len(datas) \n",
    "    freq_neg = neg_count/len(datas)\n",
    "\n",
    "    # DS sent and PMI sent\n",
    "    for word,value in tqdm(copy.items()):\n",
    "        if value['pos']+value['neg']<len(datas)/100:\n",
    "            del count[word]\n",
    "            continue\n",
    "        freq_w_pos = value['pos']/len(datas)\n",
    "        freq_w_neg = value['neg']/len(datas)\n",
    "        freq_w = (value['pos']+value['neg'])/len(datas)\n",
    "        if freq_w_pos*N == 0:\n",
    "            PMI_w_pos = 0\n",
    "        else:\n",
    "            PMI_w_pos = np.log2(freq_w_pos*N/freq_w*freq_pos)\n",
    "        if freq_w_neg*N == 0:\n",
    "            PMI_w_neg = 0\n",
    "        else:\n",
    "            PMI_w_neg = np.log2(freq_w_neg*N/freq_w*freq_neg)\n",
    "        count[word]['PMI_sent'] = PMI_w_pos - PMI_w_neg\n",
    "\n",
    "        pos = value['pos']/len(datas)\n",
    "        neg = value['neg']/len(datas)\n",
    "        value['PD'] = (pos-neg)/(pos+neg) # polarity difference\n",
    "        if abs(value['PD']) > 0.3 and nltk.pos_tag([word])[0][1] in adj+adv:  \n",
    "            sent_words.append(word)\n",
    "        count[word]['sent'] = value['PD']*value['PD'] * np.sign(value['PD'])\n",
    "\n",
    "        pos_rate = value['pos_rate']/len(datas)\n",
    "        neg_rate = value['neg_rate']/len(datas)\n",
    "        value['PD_rate'] = (pos_rate-neg_rate)/(pos_rate+neg_rate) # polarity difference\n",
    "        count[word]['sent_rate'] = value['PD_rate']*value['PD_rate'] * np.sign(value['PD_rate'])\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news2vector(datas,count,bl_sent):\n",
    "    for data in tqdm(datas):\n",
    "        idx = datas.index(data)\n",
    "        tokens = data['tokens']\n",
    "        datas[idx]['DsVector'] = [0,0,0,0]\n",
    "        datas[idx]['DsVector_rate'] = [0,0,0,0]\n",
    "        datas[idx]['SnVector'] = [0,0,0,0]\n",
    "        datas[idx]['BlVector'] = [0,0,0,0]\n",
    "        datas[idx]['PmiVector'] = [0,0,0,0]\n",
    "        datas[idx]['ContextVector'] = [0,0,0,0]\n",
    "        \n",
    "        # for f in [token for token in tokens if token in sentiment_feature.keys()]:\n",
    "        #     for w in sentiment_feature[f].keys():\n",
    "        #         if w in tokens:\n",
    "        #             if abs(tokens.index(f)-tokens.index(w))<3 and ',' not in data['content'][min(data['content'].index(f),data['content'].index(w)):max(data['content'].index(f),data['content'].index(w))]:\n",
    "        #                 if tags[tokens.index(f)][1] in adj:\n",
    "        #                     if f in count.keys():\n",
    "        #                         datas[idx]['ContextVector'][0] += count[f]['sent']\n",
    "        #                 elif tags[tokens.index(f)][1] in adv:\n",
    "        #                     if f in count.keys():\n",
    "        #                         datas[idx]['ContextVector'][1] += count[f]['sent']\n",
    "        #                 if tags[tokens.index(w)][1] in nn:\n",
    "        #                     if w in count.keys():\n",
    "        #                         datas[idx]['ContextVector'][2] = count[w]['sent']\n",
    "        #                 elif tags[tokens.index(w)][1] in vb:\n",
    "        #                     if w in count.keys():\n",
    "        #                         datas[idx]['ContextVector'][3] = count[w]['sent']\n",
    "        #                 # print(sentiment_feature[f][w]['sent'])\n",
    "        for tags in data['tags']:\n",
    "            for word,tag in tags:\n",
    "                if tag in adj:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector'][0] += count[word]['sent']\n",
    "                        datas[idx]['DsVector_rate'][0] += count[word]['sent_rate']\n",
    "                        datas[idx]['PmiVector'][0] += count[word]['PMI_sent']\n",
    "                    if word in sn.data.keys():\n",
    "                        datas[idx]['SnVector'][0] += float(sn.polarity_intense(word))\n",
    "                    if word in bl_sent.keys():\n",
    "                        datas[idx]['BlVector'][0] += bl_sent[word]\n",
    "                elif tag in adv:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector'][1] += count[word]['sent']\n",
    "                        datas[idx]['DsVector_rate'][1] += count[word]['sent_rate']\n",
    "                        datas[idx]['PmiVector'][1] += count[word]['PMI_sent']\n",
    "                    if word in sn.data.keys():\n",
    "                        datas[idx]['SnVector'][1] += float(sn.polarity_intense(word))\n",
    "                    if word in bl_sent.keys():\n",
    "                        datas[idx]['BlVector'][1] += bl_sent[word]  \n",
    "                elif tag in nn:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector'][2] = count[word]['sent']\n",
    "                        datas[idx]['DsVector_rate'][2] += count[word]['sent_rate']\n",
    "                        datas[idx]['PmiVector'][2] += count[word]['PMI_sent']\n",
    "                    if word in sn.data.keys():\n",
    "                        datas[idx]['SnVector'][2] += float(sn.polarity_intense(word))\n",
    "                    if word in bl_sent.keys():\n",
    "                        datas[idx]['BlVector'][2] += bl_sent[word]\n",
    "                elif tag in vb:\n",
    "                    if word in count.keys():\n",
    "                        datas[idx]['DsVector'][3] = count[word]['sent']\n",
    "                        datas[idx]['DsVector_rate'][3] += count[word]['sent_rate']\n",
    "                        datas[idx]['PmiVector'][3] += count[word]['PMI_sent']\n",
    "                    if word in sn.data.keys():\n",
    "                        datas[idx]['SnVector'][3] += float(sn.polarity_intense(word))\n",
    "                    if word in bl_sent.keys():\n",
    "                        datas[idx]['BlVector'][3] += bl_sent[word]\n",
    "        # datas[idx]['DsVector'] = [adv_score,adv_score,noun_score,verb_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load finish\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "datas = pickle.load(open('/home/stocksentiment/Dynamic-Financial-News-Collection-and-Analysis/datas.pkl','rb'))\n",
    "count = pickle.load(open('/home/stocksentiment/Dynamic-Financial-News-Collection-and-Analysis/sent_dict.pkl', 'rb'))\n",
    "print('load finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2018 = r'/data/labeled_data2018.xls'\n",
    "path2019 = r'/data/labeled_data2019.xls'\n",
    "\n",
    "# path = 'labeled_data.xls'\n",
    "# datas = load_data(path)\n",
    "# count = train_sent_dict(datas)\n",
    "\n",
    "# import pickle\n",
    "# output = open('sent_dict.pkl', 'wb')\n",
    "# input = open('sent_dict.pkl', 'rb')\n",
    "# s = pickle.dump(count, output)\n",
    "# output.close()\n",
    "# clf2 = pickle.load(input)\n",
    "# input.close()\n",
    "# print clf2.predict(X[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/labeled_data2018.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2668b750aec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## 新闻情感值和股价的相关度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mworkbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2018\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mworksheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworksheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcompanies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworksheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/labeled_data2018.xls'"
     ]
    }
   ],
   "source": [
    "## 新闻情感值和股价的相关度\n",
    "workbook = xlrd.open_workbook(path2018)\n",
    "worksheet = workbook.sheet_by_index(0)\n",
    "contents = worksheet.col_values(1)\n",
    "companies = worksheet.col_values(2)\n",
    "prices = worksheet.col_values(3)\n",
    "dates = worksheet.col_values(4)\n",
    "\n",
    "rates = []\n",
    "score_list = []\n",
    "\n",
    "for i in tqdm(range(0,len(contents))):\n",
    "    rate = []\n",
    "    price_list = json.loads(prices[i])\n",
    "    for idx in range(0,6):\n",
    "      rate.append((price_list[idx+1]-price_list[idx])/price_list[idx])\n",
    "    rates.append(rate)\n",
    "    score_list.append(sentiment_score(contents[i]))\n",
    "\n",
    "## 合并一天新闻\n",
    "# for i in tqdm(range(0,len(contents))):\n",
    "#     rate = []\n",
    "#     price_list = json.loads(prices[i])\n",
    "#     for idx in range(0,6):\n",
    "#       rate.append((price_list[idx+1]-price_list[idx])/price_list[idx])\n",
    "#     if rate in rates:\n",
    "#         idx = rates.index(rate)\n",
    "#         score_list[idx] += sentiment_score(contents[i])\n",
    "#         continue\n",
    "#     rates.append(rate)\n",
    "#     score_list.append(sentiment_score(contents[i]))\n",
    "\n",
    "# 情感极性与六天内（包括）新闻涨跌比率的相关度\n",
    "fiveday_rate_list = []\n",
    "for i in range(0,6):\n",
    "   rate = [x[i] for x in rates]\n",
    "   data = {\n",
    "        'scores':score_list,\n",
    "        'rates':rate\n",
    "        }\n",
    "\n",
    "   df = pd.DataFrame(data)\n",
    "   # print(df)\n",
    "   print(df.corr(\"kendall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## neg pos 词\n",
    "# pos_words = {}\n",
    "# neg_words = {}\n",
    "# for word in sent_words:\n",
    "#    if count[word]['sent'] > 0:\n",
    "#       pos_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "#    else:\n",
    "#       neg_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "\n",
    "# output_cloud(pos_words,'pos')\n",
    "# output_cloud(neg_words,'neg')\n",
    "\n",
    "# datas = load_data(path2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## neg pos 词\n",
    "# pos_words = {}\n",
    "# neg_words = {}\n",
    "# for word in sent_words:\n",
    "#    if count[word]['sent'] > 0:\n",
    "#       pos_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "#    else:\n",
    "#       neg_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "\n",
    "# output_cloud(pos_words,'pos')\n",
    "# output_cloud(neg_words,'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 求于bl词典的覆盖率\n",
    "bl_sent = {}\n",
    "bl_pos = my_read('/home/stocksentiment/Dynamic-Financial-News-Collection-and-Analysis/sentiment_analysis/bl/positive.txt')  # 4783\n",
    "bl_neg = my_read('/home/stocksentiment/Dynamic-Financial-News-Collection-and-Analysis/sentiment_analysis/bl/negative.txt')  # 2006\n",
    "\n",
    "\n",
    "for word in bl_pos:\n",
    "    bl_sent[word] = 1\n",
    "for word in bl_pos:\n",
    "    bl_sent[word] = -1\n",
    "# pc = 0\n",
    "# for word in pos_words:\n",
    "#    if word in bl_pos:\n",
    "#       pc += 1\n",
    "# pos_accuracy = pc/len(pos_words)  # 0.2857142857142857\n",
    "\n",
    "# nc = 0\n",
    "# for word in neg_words:\n",
    "#    if word in bl_neg:\n",
    "#       nc += 1\n",
    "# neg_accuracy = nc/len(neg_words)  # 0.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## context sentiment dict\n",
    "# sent_words = [word.lower() for word in sent_words]\n",
    "# feature_words = {}\n",
    "# sentiment_feature = {}\n",
    "\n",
    "# for data in tqdm(datas):\n",
    "#     for tags in data['tags']:\n",
    "#         for word,tag in tags:\n",
    "#             if tag not in nn or len(word)<3: # vb+nn\n",
    "#                 continue\n",
    "#             # word = stem_and_check(word)\n",
    "#             if word not in feature_words.keys():\n",
    "#                 feature_words[word] = 1\n",
    "#             else:\n",
    "#                 feature_words[word] += 1\n",
    "\n",
    "# # avg_f = sum([item[1] for item in feature_words.items()])/len(feature_words.keys())\n",
    "# res = sorted(feature_words.items(),key=lambda feature_words:feature_words[1],reverse=True)\n",
    "# words = res[:400]\n",
    "# # for word,value in tqdm(copy.items()):\n",
    "# #     if value<avg_f+200:\n",
    "# #         del feature_words[word]\n",
    "\n",
    "# feature_words = [inf.singularize(word).lower() for word,freq in words]\n",
    "\n",
    "# sf_len = 0\n",
    "# for data in tqdm(datas):\n",
    "#     rate = data['rate']\n",
    "#     for tokens in data['tokens']:\n",
    "#         token_dict = {}\n",
    "#         for token in tokens:\n",
    "#             token_dict[inf.singularize(token).lower()] = token\n",
    "#         _tokens = [inf.singularize(token).lower() for token in tokens]\n",
    "\n",
    "#         for w in list(set(sent_words).intersection(set(_tokens))):\n",
    "#             for f in list(set(feature_words).intersection(set(_tokens))):\n",
    "#                 if f != w:\n",
    "#                     if abs(_tokens.index(w)-_tokens.index(f))<3 and ',' not in data['content'][min(data['content'].index(token_dict[f]),data['content'].index(token_dict[w])):max(data['content'].index(token_dict[f]),data['content'].index(token_dict[w]))]:\n",
    "#                         sf_len += 1\n",
    "#                         if f not in sentiment_feature.keys():\n",
    "#                             sentiment_feature[f] = {}\n",
    "#                             if rate > 0:\n",
    "#                                 sentiment_feature[f][w] = {'pos':1,'neg':0}\n",
    "#                             if rate < 0:\n",
    "#                                 sentiment_feature[f][w] = {'pos':0,'neg':1}\n",
    "#                         else:\n",
    "#                             if w not in sentiment_feature[f].keys():\n",
    "#                                 sentiment_feature[f][w] = {'pos':0,'neg':0}\n",
    "#                             if rate > 0:\n",
    "#                                 sentiment_feature[f][w]['pos'] += 1\n",
    "#                             if rate < 0:\n",
    "#                                 sentiment_feature[f][w]['neg'] += 1\n",
    "\n",
    "# avg_sf = sf_len/len(sentiment_feature.keys())\n",
    "# copy = sentiment_feature.copy()\n",
    "\n",
    "# for f,v in tqdm(sentiment_feature.items()):\n",
    "#     for w,value in v.items():\n",
    "#         if value['pos']+value['neg']<avg_sf: #avg_sf\n",
    "#             # print(f,w,value)\n",
    "#             # del sentiment_feature[f][w]\n",
    "#             sentiment_feature[f][w]['sent'] = 0\n",
    "#             continue\n",
    "#         pos = value['pos']/POS\n",
    "#         neg = value['neg']/NEG\n",
    "        \n",
    "#         value['PD'] = (pos-neg)/(pos+neg) # polarity difference\n",
    "#         sentiment_feature[f][w]['sent'] = value['PD'] * value['PD'] * np.sign(value['PD'])\n",
    "\n",
    "# res = sorted(sentiment_feature.items(),key=lambda sentiment_feature:sentiment_feature[1]['sent'],reverse=False)\n",
    "\n",
    "## company word\n",
    "# company_pos = {}\n",
    "# company_neg = {}\n",
    "# for key,value in sentiment_feature.items():\n",
    "#    if 'company' in key:\n",
    "#       if sentiment_feature[key]['sent'] > 0:\n",
    "#          company_pos[key.split('_')[0]] = sentiment_feature[key]['pos']\n",
    "#       else:\n",
    "#          company_neg[key.split('_')[0]] = sentiment_feature[key]['neg']\n",
    "\n",
    "# output_cloud(company_pos,'company_pos')\n",
    "# output_cloud(company_neg,'company_neg')\n",
    "\n",
    "\n",
    "# for r in res[:30]:\n",
    "#    print(r[0],r[1]['sent'],r[1]['pos']+r[1]['neg'])\n",
    "#    print(' ')\n",
    "\n",
    "# print(' ')\n",
    "# print('========================')\n",
    "\n",
    "# for r in res[-40:]:\n",
    "#    print(r[0],r[1]['sent'],r[1]['pos'],r[1]['neg'])\n",
    "#    print(' ')\n",
    "\n",
    "# 展示\n",
    "# pos_res = {}\n",
    "# for r in res:\n",
    "#     if r[1]['sent'] == 1.0:\n",
    "#         pos_res[r[0]] = r[1]['pos']+r[1]['neg']\n",
    "# pos_res = sorted(pos_res.items(),key=lambda pos_res:pos_res[1],reverse=True)\n",
    "# for r in pos_res[:20]:\n",
    "#     print(r[0],'1.0',r[1])\n",
    "#     print(' ')\n",
    "\n",
    "# print(' ')\n",
    "# print('========================')\n",
    "# print(' ')\n",
    "\n",
    "# neg_res = {}\n",
    "# for r in res:\n",
    "#     if r[1]['sent'] == -1.0:\n",
    "#         neg_res[r[0]] = r[1]['pos']+r[1]['neg']\n",
    "# neg_res = sorted(neg_res.items(),key=lambda neg_res:neg_res[1],reverse=True)\n",
    "# for r in neg_res[:20]:\n",
    "#     print(r[0],'-1.0',r[1])\n",
    "#     print(' ')\n",
    "# for sf,value in sentiment_feature.items():\n",
    "#    if value['pos']+value['neg'] > avg_sf:\n",
    "#       print(sf,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 363237/363237 [1:15:57<00:00, 27.32it/s] \n"
     ]
    }
   ],
   "source": [
    "news2vector(datas,count,bl_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('datas.pkl', 'wb')\n",
    "pickle.dump(datas, output)\n",
    "print('dump finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BlVector': [0, 0, -4, 0],\n",
       " 'ContextVector': [0, 0, 0, 0],\n",
       " 'DsVector': [0.009382414055471773,\n",
       "  0.003466796193975887,\n",
       "  0.00011491759888645986,\n",
       "  0.0008279551668078367],\n",
       " 'DsVector_rate': [0.04473495161500582,\n",
       "  0.01280244453155389,\n",
       "  0.2797177700726252,\n",
       "  0.10969579071672703],\n",
       " 'PmiVector': [2.3897680592562054,\n",
       "  0.9182778690101259,\n",
       "  12.363459195184863,\n",
       "  4.756092484475477],\n",
       " 'SnVector': [-0.6450000000000007,\n",
       "  -2.5869999999999997,\n",
       "  12.297000000000002,\n",
       "  1.055],\n",
       " 'company': 'Ameriprise Financial',\n",
       " 'content': 'NEW YORK  (Reuters) - H&R Block Inc will pay as much as $20.2 million to settle a New York lawsuit accusing it of fraudulently marketing retirement accounts that caused hundreds of thousands of mostly lower-income clients to lose money.    New York Attorney General Andrew Cuomo said the accord calls for the largest U.S. tax preparer to refund $11.4 million to $19.4 million of fees to customers nationwide who opened one of its Express IRAs, a type of individual retirement account. H&R Block will also pay $750,000 in fines and other costs to the state, and convert Express IRAs into new retirement accounts that do not charge fees, Cuomo said. The size of the refund depends on the number of claims made, he said. The attorney general said H&R Block also settled private class-action lawsuits based on the same allegations, and which were pending in the federal court in Kansas City, Missouri, where the company is based. Norman Siegel, a lawyer representing plaintiffs in the private litigation, in an e-mail declined to discuss terms of that settlement, but said he expects to file papers with the court in the next few days. New York had accused H&R Block of steering more than 600,000 customers to Express IRAs, without disclosing hidden fees that wiped out the interest that 85 percent of them could earn. Eliot Spitzer, Cuomo\\'s predecessor, had first sued H&R Block over the marketing of Express IRAs in March 2006. \"H&R Block\\'s aggressive peddling of fee-laden retirement accounts that were virtually guaranteed to lose money needlessly cost families across the country millions of their hard-earned dollars,\" Cuomo said in a statement on Monday. Gene King, an H&R Block spokesman, called the New York settlement \"satisfactory for all parties.\" He had no immediate comment on the class-action settlement. Spitzer originally sought $250 million of civil penalties and other remedies. His lawsuit had said the median Express IRA account had a $323 balance, too low for investors to offset such charges as $10 annual maintenance fees, $15 set-up fees, $15 \"re-contribution\" fees and $25 termination fees. Among the defendants in the New York case was H&R Block Financial Advisors Inc, which the company sold in 2008 to Ameriprise Financial Inc. Ameriprise did not return a call seeking comment. In late afternoon trading, H&R Block shares were up 7 cents at $22.69 on the New York Stock Exchange. The case is New York v. H&R Block Inc, New York State Supreme Court, No. 401110/2006. (Reporting by Jonathan Stempel; Editing by  Andre Grenon  and  Richard Chang )',\n",
       " 'date': '2010-01-04',\n",
       " 'rate': 0.023319966907406453,\n",
       " 'tags': [[('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('pay', 'NN'),\n",
       "   ('much', 'JJ'),\n",
       "   ('million', 'CD'),\n",
       "   ('settle', 'JJ'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('lawsuit', 'NN'),\n",
       "   ('accusing', 'VBG'),\n",
       "   ('fraudulently', 'RB'),\n",
       "   ('marketing', 'VBG'),\n",
       "   ('retirement', 'NN'),\n",
       "   ('accounts', 'NNS'),\n",
       "   ('caused', 'VBD'),\n",
       "   ('hundreds', 'NNS'),\n",
       "   ('thousands', 'NNS'),\n",
       "   ('mostly', 'RB'),\n",
       "   ('lower', 'JJR'),\n",
       "   ('income', 'NN'),\n",
       "   ('clients', 'NNS'),\n",
       "   ('lose', 'VB'),\n",
       "   ('money', 'NN')],\n",
       "  [('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('Attorney', 'NNP'),\n",
       "   ('General', 'NNP'),\n",
       "   ('Andrew', 'NNP'),\n",
       "   ('Cuomo', 'NNP'),\n",
       "   ('said', 'VBD'),\n",
       "   ('accord', 'NN'),\n",
       "   ('calls', 'VBZ'),\n",
       "   ('largest', 'JJS'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('tax', 'NN'),\n",
       "   ('preparer', 'NN'),\n",
       "   ('refund', 'NN'),\n",
       "   ('million', 'CD'),\n",
       "   ('million', 'CD'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('customers', 'NNS'),\n",
       "   ('nationwide', 'RB'),\n",
       "   ('opened', 'VBD'),\n",
       "   ('one', 'CD'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRAs', 'NNPS'),\n",
       "   ('type', 'NN'),\n",
       "   ('individual', 'JJ'),\n",
       "   ('retirement', 'NN'),\n",
       "   ('account', 'NN')],\n",
       "  [('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('also', 'RB'),\n",
       "   ('pay', 'VBP'),\n",
       "   ('fines', 'NNS'),\n",
       "   ('costs', 'NNS'),\n",
       "   ('state', 'NN'),\n",
       "   ('convert', 'NN'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRAs', 'NNP'),\n",
       "   ('new', 'JJ'),\n",
       "   ('retirement', 'NN'),\n",
       "   ('accounts', 'NNS'),\n",
       "   ('charge', 'NN'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('Cuomo', 'NNP'),\n",
       "   ('said', 'VBD')],\n",
       "  [('The', 'DT'),\n",
       "   ('size', 'NN'),\n",
       "   ('refund', 'NN'),\n",
       "   ('depends', 'VBZ'),\n",
       "   ('number', 'NN'),\n",
       "   ('claims', 'NNS'),\n",
       "   ('made', 'VBN'),\n",
       "   ('said', 'VBD')],\n",
       "  [('The', 'DT'),\n",
       "   ('attorney', 'NN'),\n",
       "   ('general', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('also', 'RB'),\n",
       "   ('settled', 'VBD'),\n",
       "   ('private', 'JJ'),\n",
       "   ('class', 'NN'),\n",
       "   ('action', 'NN'),\n",
       "   ('lawsuits', 'NNS'),\n",
       "   ('based', 'VBN'),\n",
       "   ('allegations', 'NNS'),\n",
       "   ('pending', 'VBG'),\n",
       "   ('federal', 'JJ'),\n",
       "   ('court', 'NN'),\n",
       "   ('Kansas', 'NNP'),\n",
       "   ('City', 'NNP'),\n",
       "   ('Missouri', 'NNP'),\n",
       "   ('company', 'NN'),\n",
       "   ('based', 'VBN')],\n",
       "  [('Norman', 'NNP'),\n",
       "   ('Siegel', 'NNP'),\n",
       "   ('lawyer', 'NN'),\n",
       "   ('representing', 'VBG'),\n",
       "   ('plaintiffs', 'NNS'),\n",
       "   ('private', 'JJ'),\n",
       "   ('litigation', 'NN'),\n",
       "   ('e', 'FW'),\n",
       "   ('mail', 'NN'),\n",
       "   ('declined', 'VBD'),\n",
       "   ('discuss', 'JJ'),\n",
       "   ('terms', 'NNS'),\n",
       "   ('settlement', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('expects', 'VBZ'),\n",
       "   ('file', 'JJ'),\n",
       "   ('papers', 'NNS'),\n",
       "   ('court', 'NN'),\n",
       "   ('next', 'JJ'),\n",
       "   ('days', 'NNS')],\n",
       "  [('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('accused', 'VBD'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('steering', 'VBG'),\n",
       "   ('customers', 'NNS'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRAs', 'NNP'),\n",
       "   ('without', 'IN'),\n",
       "   ('disclosing', 'VBG'),\n",
       "   ('hidden', 'NN'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('wiped', 'VBD'),\n",
       "   ('interest', 'NN'),\n",
       "   ('percent', 'NN'),\n",
       "   ('could', 'MD'),\n",
       "   ('earn', 'VB')],\n",
       "  [('Eliot', 'NNP'),\n",
       "   ('Spitzer', 'NNP'),\n",
       "   ('Cuomo', 'NNP'),\n",
       "   ('predecessor', 'NN'),\n",
       "   ('first', 'RB'),\n",
       "   ('sued', 'VBN'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('marketing', 'NN'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRAs', 'NNP'),\n",
       "   ('March', 'NNP')],\n",
       "  [('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('aggressive', 'JJ'),\n",
       "   ('peddling', 'NN'),\n",
       "   ('fee', 'NN'),\n",
       "   ('laden', 'JJ'),\n",
       "   ('retirement', 'NN'),\n",
       "   ('accounts', 'NNS'),\n",
       "   ('virtually', 'RB'),\n",
       "   ('guaranteed', 'VBP'),\n",
       "   ('lose', 'JJ'),\n",
       "   ('money', 'NN'),\n",
       "   ('needlessly', 'RB'),\n",
       "   ('cost', 'VBN'),\n",
       "   ('families', 'NNS'),\n",
       "   ('across', 'IN'),\n",
       "   ('country', 'NN'),\n",
       "   ('millions', 'NNS'),\n",
       "   ('hard', 'RB'),\n",
       "   ('earned', 'VBD'),\n",
       "   ('dollars', 'NNS'),\n",
       "   ('Cuomo', 'NNP'),\n",
       "   ('said', 'VBD'),\n",
       "   ('statement', 'NN'),\n",
       "   ('Monday', 'NNP')],\n",
       "  [('Gene', 'NNP'),\n",
       "   ('King', 'NNP'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('spokesman', 'NN'),\n",
       "   ('called', 'VBD'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('settlement', 'NN'),\n",
       "   ('satisfactory', 'NN'),\n",
       "   ('parties', 'NNS')],\n",
       "  [('He', 'PRP'),\n",
       "   ('immediate', 'JJ'),\n",
       "   ('comment', 'NN'),\n",
       "   ('class', 'NN'),\n",
       "   ('action', 'NN'),\n",
       "   ('settlement', 'NN')],\n",
       "  [('Spitzer', 'NNP'),\n",
       "   ('originally', 'RB'),\n",
       "   ('sought', 'VBD'),\n",
       "   ('million', 'CD'),\n",
       "   ('civil', 'JJ'),\n",
       "   ('penalties', 'NNS'),\n",
       "   ('remedies', 'NNS')],\n",
       "  [('His', 'PRP$'),\n",
       "   ('lawsuit', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('median', 'JJ'),\n",
       "   ('Express', 'NNP'),\n",
       "   ('IRA', 'NNP'),\n",
       "   ('account', 'NN'),\n",
       "   ('balance', 'NN'),\n",
       "   ('low', 'JJ'),\n",
       "   ('investors', 'NNS'),\n",
       "   ('offset', 'VBD'),\n",
       "   ('charges', 'NNS'),\n",
       "   ('annual', 'JJ'),\n",
       "   ('maintenance', 'NN'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('set', 'VBP'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('contribution', 'NN'),\n",
       "   ('fees', 'NNS'),\n",
       "   ('termination', 'NN'),\n",
       "   ('fees', 'NNS')],\n",
       "  [('Among', 'IN'),\n",
       "   ('defendants', 'NNS'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('case', 'NN'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('Financial', 'NNP'),\n",
       "   ('Advisors', 'NNPS'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('company', 'NN'),\n",
       "   ('sold', 'VBD'),\n",
       "   ('Ameriprise', 'NNP'),\n",
       "   ('Financial', 'NNP'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('Ameriprise', 'NNP'),\n",
       "   ('return', 'NN'),\n",
       "   ('call', 'NN'),\n",
       "   ('seeking', 'VBG'),\n",
       "   ('comment', 'NN')],\n",
       "  [('In', 'IN'),\n",
       "   ('late', 'JJ'),\n",
       "   ('afternoon', 'NN'),\n",
       "   ('trading', 'NN'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('shares', 'NNS'),\n",
       "   ('cents', 'NNS'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('Stock', 'NNP'),\n",
       "   ('Exchange', 'NNP')],\n",
       "  [('The', 'DT'),\n",
       "   ('case', 'NN'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('v', 'NN'),\n",
       "   ('H', 'NNP'),\n",
       "   ('R', 'NNP'),\n",
       "   ('Block', 'NNP'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('New', 'NNP'),\n",
       "   ('York', 'NNP'),\n",
       "   ('State', 'NNP'),\n",
       "   ('Supreme', 'NNP'),\n",
       "   ('Court', 'NNP'),\n",
       "   ('No', 'NNP')],\n",
       "  [],\n",
       "  [('Reporting', 'VBG'),\n",
       "   ('Jonathan', 'NNP'),\n",
       "   ('Stempel', 'NNP'),\n",
       "   ('Editing', 'NNP'),\n",
       "   ('Andre', 'NNP'),\n",
       "   ('Grenon', 'NNP'),\n",
       "   ('Richard', 'NNP'),\n",
       "   ('Chang', 'NNP')]],\n",
       " 'tokens': [['H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'Inc',\n",
       "   'pay',\n",
       "   'much',\n",
       "   'million',\n",
       "   'settle',\n",
       "   'New',\n",
       "   'York',\n",
       "   'lawsuit',\n",
       "   'accusing',\n",
       "   'fraudulently',\n",
       "   'marketing',\n",
       "   'retirement',\n",
       "   'accounts',\n",
       "   'caused',\n",
       "   'hundreds',\n",
       "   'thousands',\n",
       "   'mostly',\n",
       "   'lower',\n",
       "   'income',\n",
       "   'clients',\n",
       "   'lose',\n",
       "   'money'],\n",
       "  ['New',\n",
       "   'York',\n",
       "   'Attorney',\n",
       "   'General',\n",
       "   'Andrew',\n",
       "   'Cuomo',\n",
       "   'said',\n",
       "   'accord',\n",
       "   'calls',\n",
       "   'largest',\n",
       "   'U',\n",
       "   'S',\n",
       "   'tax',\n",
       "   'preparer',\n",
       "   'refund',\n",
       "   'million',\n",
       "   'million',\n",
       "   'fees',\n",
       "   'customers',\n",
       "   'nationwide',\n",
       "   'opened',\n",
       "   'one',\n",
       "   'Express',\n",
       "   'IRAs',\n",
       "   'type',\n",
       "   'individual',\n",
       "   'retirement',\n",
       "   'account'],\n",
       "  ['H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'also',\n",
       "   'pay',\n",
       "   'fines',\n",
       "   'costs',\n",
       "   'state',\n",
       "   'convert',\n",
       "   'Express',\n",
       "   'IRAs',\n",
       "   'new',\n",
       "   'retirement',\n",
       "   'accounts',\n",
       "   'charge',\n",
       "   'fees',\n",
       "   'Cuomo',\n",
       "   'said'],\n",
       "  ['The', 'size', 'refund', 'depends', 'number', 'claims', 'made', 'said'],\n",
       "  ['The',\n",
       "   'attorney',\n",
       "   'general',\n",
       "   'said',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'also',\n",
       "   'settled',\n",
       "   'private',\n",
       "   'class',\n",
       "   'action',\n",
       "   'lawsuits',\n",
       "   'based',\n",
       "   'allegations',\n",
       "   'pending',\n",
       "   'federal',\n",
       "   'court',\n",
       "   'Kansas',\n",
       "   'City',\n",
       "   'Missouri',\n",
       "   'company',\n",
       "   'based'],\n",
       "  ['Norman',\n",
       "   'Siegel',\n",
       "   'lawyer',\n",
       "   'representing',\n",
       "   'plaintiffs',\n",
       "   'private',\n",
       "   'litigation',\n",
       "   'e',\n",
       "   'mail',\n",
       "   'declined',\n",
       "   'discuss',\n",
       "   'terms',\n",
       "   'settlement',\n",
       "   'said',\n",
       "   'expects',\n",
       "   'file',\n",
       "   'papers',\n",
       "   'court',\n",
       "   'next',\n",
       "   'days'],\n",
       "  ['New',\n",
       "   'York',\n",
       "   'accused',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'steering',\n",
       "   'customers',\n",
       "   'Express',\n",
       "   'IRAs',\n",
       "   'without',\n",
       "   'disclosing',\n",
       "   'hidden',\n",
       "   'fees',\n",
       "   'wiped',\n",
       "   'interest',\n",
       "   'percent',\n",
       "   'could',\n",
       "   'earn'],\n",
       "  ['Eliot',\n",
       "   'Spitzer',\n",
       "   'Cuomo',\n",
       "   'predecessor',\n",
       "   'first',\n",
       "   'sued',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'marketing',\n",
       "   'Express',\n",
       "   'IRAs',\n",
       "   'March'],\n",
       "  ['H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'aggressive',\n",
       "   'peddling',\n",
       "   'fee',\n",
       "   'laden',\n",
       "   'retirement',\n",
       "   'accounts',\n",
       "   'virtually',\n",
       "   'guaranteed',\n",
       "   'lose',\n",
       "   'money',\n",
       "   'needlessly',\n",
       "   'cost',\n",
       "   'families',\n",
       "   'across',\n",
       "   'country',\n",
       "   'millions',\n",
       "   'hard',\n",
       "   'earned',\n",
       "   'dollars',\n",
       "   'Cuomo',\n",
       "   'said',\n",
       "   'statement',\n",
       "   'Monday'],\n",
       "  ['Gene',\n",
       "   'King',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'spokesman',\n",
       "   'called',\n",
       "   'New',\n",
       "   'York',\n",
       "   'settlement',\n",
       "   'satisfactory',\n",
       "   'parties'],\n",
       "  ['He', 'immediate', 'comment', 'class', 'action', 'settlement'],\n",
       "  ['Spitzer',\n",
       "   'originally',\n",
       "   'sought',\n",
       "   'million',\n",
       "   'civil',\n",
       "   'penalties',\n",
       "   'remedies'],\n",
       "  ['His',\n",
       "   'lawsuit',\n",
       "   'said',\n",
       "   'median',\n",
       "   'Express',\n",
       "   'IRA',\n",
       "   'account',\n",
       "   'balance',\n",
       "   'low',\n",
       "   'investors',\n",
       "   'offset',\n",
       "   'charges',\n",
       "   'annual',\n",
       "   'maintenance',\n",
       "   'fees',\n",
       "   'set',\n",
       "   'fees',\n",
       "   'contribution',\n",
       "   'fees',\n",
       "   'termination',\n",
       "   'fees'],\n",
       "  ['Among',\n",
       "   'defendants',\n",
       "   'New',\n",
       "   'York',\n",
       "   'case',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'Financial',\n",
       "   'Advisors',\n",
       "   'Inc',\n",
       "   'company',\n",
       "   'sold',\n",
       "   'Ameriprise',\n",
       "   'Financial',\n",
       "   'Inc',\n",
       "   'Ameriprise',\n",
       "   'return',\n",
       "   'call',\n",
       "   'seeking',\n",
       "   'comment'],\n",
       "  ['In',\n",
       "   'late',\n",
       "   'afternoon',\n",
       "   'trading',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'shares',\n",
       "   'cents',\n",
       "   'New',\n",
       "   'York',\n",
       "   'Stock',\n",
       "   'Exchange'],\n",
       "  ['The',\n",
       "   'case',\n",
       "   'New',\n",
       "   'York',\n",
       "   'v',\n",
       "   'H',\n",
       "   'R',\n",
       "   'Block',\n",
       "   'Inc',\n",
       "   'New',\n",
       "   'York',\n",
       "   'State',\n",
       "   'Supreme',\n",
       "   'Court',\n",
       "   'No'],\n",
       "  [],\n",
       "  ['Reporting',\n",
       "   'Jonathan',\n",
       "   'Stempel',\n",
       "   'Editing',\n",
       "   'Andre',\n",
       "   'Grenon',\n",
       "   'Richard',\n",
       "   'Chang']]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DsVector==============\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-50ef817bece8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mpredict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \"\"\"\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \"\"\"\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X1 = [data['DsVector'] for data in datas]\n",
    "X2 = [data['SnVector'] for data in datas]\n",
    "X3 = [data['BlVector'] for data in datas]\n",
    "X4 = [data['PmiVector'] for data in datas]\n",
    "# X = [data['ContextVector'] for data in datas]\n",
    "X5 = [data['DsVector_rate'] for data in datas]\n",
    "\n",
    "Xs = {'DsVector':X1,'SnVector':X2,'BlVector':X3,'PmiVector':X4,'DsVector_rate':X5}\n",
    "\n",
    "Y = [np.sign(data['rate']) for data in datas]\n",
    "# Y = [data['rate'] for data in datas]\n",
    "\n",
    "# Ds_rate GaussianNB\n",
    "\n",
    "for vectorname in Xs.keys():\n",
    "    print(vectorname+'==============')\n",
    "    X = Xs[vectorname]\n",
    "    train_x,test_x,train_y,test_y = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "    clf.fit(np.array(train_x), np.array(train_y))\n",
    "\n",
    "    predict_y = clf.predict(test_x)\n",
    "    recall = recall_score(test_y,clf.predict(test_x),average = 'macro')\n",
    "    precision = precision_score(test_y, clf.predict(test_x), average='macro')\n",
    "\n",
    "    print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "    print('召回率：',recall)\n",
    "    print('精确率：',precision)\n",
    "    print('f1_score',2*recall*precision/(recall+precision))\n",
    "\n",
    "    vote_predict_y = vote(predict_y,datas[-len(test_x):])\n",
    "    print('投票算法准确率：',accuracy(vote_predict_y,test_y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dnn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "train_x,test_x,train_y,test_y = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "\n",
    "# train_x = [data['DsVector_rate'] for data in datas]\n",
    "# Y = [np.sign(data['rate']) for data in datas]\n",
    "# test_x = [data['DsVector_rate'] for data in datas2[2000:]]\n",
    "# Y2 = [np.sign(data['rate']) for data in datas2[2000:]]\n",
    "\n",
    "# train_y = []\n",
    "# for y in Y:\n",
    "#     if y == 1:\n",
    "#         train_y.append(np.array([0,1]))\n",
    "#     else:    \n",
    "#         train_y.append(np.array([1,0]))\n",
    "\n",
    "# test_y = []\n",
    "# for y in Y2:\n",
    "#     if y == 1:\n",
    "#         test_y.append(np.array([0,1]))\n",
    "#     else:    \n",
    "#         test_y.append(np.array([1,0]))\n",
    "\n",
    "test_y = to_categorical(Y,num_classes=num_classes)\n",
    "\n",
    "nmodel = Sequential()\n",
    "nmodel.add(Dense(units=num_classes, activation = 'relu', input_dim = np.array(train_x).shape[1]))\n",
    "nmodel.add(Dropout(0.5))\n",
    "nmodel.add(Dense(2, activation = 'relu'))\n",
    "nmodel.add(Dropout(0.5))\n",
    "# dropout:https://blog.csdn.net/program_developer/article/details/80737724\n",
    "nmodel.add(Dense(2, activation = 'softmax'))\n",
    "nmodel.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics = ['accuracy'])\n",
    "nmodel.fit(np.array(train_x),np.array(train_y),epochs=10, batch_size=5)\n",
    "nmodel.evaluate(np.array(test_x),np.array(test_y), batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company Apple\n",
    "train_x = [data['DsVector_rate'] for data in datas]\n",
    "train_y = [np.sign(data['rate']) for data in datas]\n",
    "test_x = [data['DsVector_rate'] for data in datas2 if data['company'] =='Apple Inc.']\n",
    "test_y = [np.sign(data['rate']) for data in datas2 if data['company'] =='Apple Inc.']\n",
    "clf = GaussianNB()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))\n",
    "vote_predict_y = vote(predict_y,datas[-len(test_x):])\n",
    "print('投票算法准确率：',accuracy(vote_predict_y,test_y)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
