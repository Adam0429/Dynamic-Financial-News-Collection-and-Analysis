{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senticnet.senticnet import SenticNet\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import IPython\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import inflection as inf\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models import Word2Vec\n",
    "import enchant\n",
    "import time\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "adj = ['JJ','JJR','JJS']\n",
    "adv = ['RB','RBR','RBS']\n",
    "vb = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "nn = ['NN','NNS']\n",
    "\n",
    "\n",
    "def review_to_words(review_text): \n",
    "    if '(Reuters) -' in review_text:\n",
    "        review_text = review_text.split('(Reuters) -')[1]\n",
    "    if '*' in review_text:\n",
    "        review_text = review_text.split('*')[1]\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.split()                             \n",
    "    _stopwords = set(stopwords.words(\"english\"))\n",
    "    _stopwords = nltk.corpus.stopwords.words('english')\n",
    "    _stopwords.append('would')\n",
    "    _stopwords.append('kmh')\n",
    "    _stopwords.append('mph')\n",
    "    _stopwords.append('  ')\n",
    "    _stopwords.append('Reuters')\n",
    "    _stopwords.append('reuters')\n",
    "    # _stopwords = []\n",
    "    meaningful_words = [w for w in words if w not in _stopwords]\n",
    "    return meaningful_words\n",
    "\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def stem_and_check(word):\n",
    "    word = nlp(word)\n",
    "    return word[0].lemma_\n",
    "    # word = inf.singularize(word)\n",
    "    # word = nltk.PorterStemmer().stem(word)\n",
    "    # if d.check(word):\n",
    "    #    return word\n",
    "    # suggest_words = d.suggest(word)\n",
    "    # if len(suggest_words) == 0:\n",
    "    #    return word\n",
    "    # return suggest_words[0]\n",
    "\n",
    "def my_read(path):\n",
    "    file = open(path)\n",
    "    words = []\n",
    "    for line in file.readlines():\n",
    "        words.append(line.strip())\n",
    "    return words\n",
    "\n",
    "def output_cloud(count,name):\n",
    "    # 云图\n",
    "    text = '' \n",
    "    for key,value in count.items():\n",
    "        text += (key+' ') * (value)\n",
    "    wc = WordCloud(\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        max_font_size=100,      #字体大小\n",
    "        min_font_size=10,\n",
    "        collocations=False, \n",
    "        max_words=1000\n",
    "    )\n",
    "    wc.generate(text)\n",
    "    wc.to_file(name+'.png') #图片保存\n",
    "\n",
    "sn = SenticNet()\n",
    "# concept_info = sn.concept('love')\n",
    "# polarity_value = sn.polarity_value('love')\n",
    "# polarity_intense = sn.polarity_intense('love')\n",
    "# moodtags = sn.moodtags('love')\n",
    "# semantics = sn.semantics('love')\n",
    "# sentics = sn.sentics('love') \n",
    " \n",
    "def sentiment_score(text):\n",
    "    t = TextBlob(text)\n",
    "    score = t.sentiment.polarity\n",
    "    return score\n",
    "    # tokens = nltk.word_tokenize(text)\n",
    "    # pos_tags = nltk.pos_tag(tokens)\n",
    "    # score = 0\n",
    "    # count = 0\n",
    "    # for word,tag in pos_tags:\n",
    "    #   if word in sn.data.keys():\n",
    "    #    score += float(sn.polarity_intense(word))\n",
    "    #    count += 1\n",
    "    #    # print(word,sn.polarity_intense(word))\n",
    "    # if count == 0: #mid\n",
    "    #   return -1 \n",
    "    # return score/count\n",
    "\n",
    "# test sentiment_score accuracy\n",
    "# scores = []\n",
    "# workbook = pd.read_csv(u'sentiment.csv',encoding='ISO-8859-1')\n",
    "# correct = 0\n",
    "# pos_count = 0\n",
    "# neg_count = 0\n",
    "# pos_correct = 0\n",
    "# neg_correct = 0\n",
    "\n",
    "# for i in tqdm(range(0,10000)):\n",
    "#   i = int(random.random()*1599999)\n",
    "#   if workbook.loc[i][0] == 0:\n",
    "#    neg_count += 1\n",
    "#    if workbook.loc[i][0] == sentiment_score_list(workbook.loc[i][5]):\n",
    "#     neg_correct += 1\n",
    "#   elif workbook.loc[i][0] == 4:\n",
    "#    pos_count += 1\n",
    "#    if workbook.loc[i][0] == sentiment_score_list(workbook.loc[i][5]):\n",
    "#     pos_correct += 1\n",
    "# print('pos',pos_correct/pos_count,pos_count,pos_correct)\n",
    "# print('neg',neg_correct/neg_count,neg_count,neg_correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/Users/wangfeihong/Desktop/Dynamic-Financial-News-Collection-and-Analysis/data/labeled_data.xls'\n",
    "\n",
    "workbook = xlrd.open_workbook(path)\n",
    "worksheet = workbook.sheet_by_index(0)\n",
    "contents = worksheet.col_values(1)\n",
    "companies = worksheet.col_values(2)\n",
    "prices = worksheet.col_values(3)\n",
    "dates = worksheet.col_values(4)\n",
    "rates = []\n",
    "score_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20961/20961 [10:03<00:00, 34.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.3 (v3.6.3:2c5fed86e0, Oct  3 2017, 00:32:08) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n",
      "In [1]: exit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datas = []\n",
    "for i in tqdm(range(0,len(contents))):\n",
    "    # if '*' not in contents[i]:\n",
    "        # if companies[i] != 'Apple Inc.':\n",
    "        #     continue\n",
    "    data = {}\n",
    "    data['content'] = contents[i]\n",
    "    sents = sent_tokenize(data['content'])\n",
    "    data['tokens'] = []\n",
    "    data['tags'] = []\n",
    "    for sent in sents:\n",
    "        token = review_to_words(sent) # 去停用词会影响词性标注吗？？\n",
    "        data['tokens'].append(token)\n",
    "        data['tags'].append(nltk.pos_tag(token))\n",
    "    data['company'] = companies[i]\n",
    "    price_list = json.loads(prices[i])       \n",
    "    data['rate'] = (price_list[2]-price_list[1])/price_list[1]\n",
    "    data['date'] = dates[i]\n",
    "    datas.append(data)\n",
    "\n",
    "## sort by time\n",
    "# dates = {}\n",
    "# for i in tqdm(range(0,len(_datas))):\n",
    "#     dates[i] = int(time.mktime(time.strptime(_datas[i]['date'], \"%Y-%m-%d\")))\n",
    "# res = sorted(dates.items(),key=lambda dates:dates[1],reverse=False)\n",
    "# datas = []\n",
    "# for idx,date in res:\n",
    "#     datas.append(_datas[idx])\n",
    "\n",
    "count = {}\n",
    "\n",
    "POS = 0\n",
    "NEG = 0\n",
    "\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "\n",
    "N = 0 #len of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20961/20961 [00:09<00:00, 2153.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(datas):\n",
    "    for tokens in data['tokens']: \n",
    "        N += len(tokens)\n",
    "        rate = data['rate'] # 选当天的股票变化判断涨跌，因为相关度当天的最高\n",
    "        if rate>0:\n",
    "            pos_count += 1\n",
    "            POS += len(tokens)\n",
    "            for token in tokens:\n",
    "                if len(token) < 3:\n",
    "                    continue\n",
    "                if 'not' in tokens:\n",
    "                    token = 'not_'+token\n",
    "                if token in count.keys():\n",
    "                    count[token]['pos'] += 1\n",
    "                    count[token]['pos_rate'] += rate\n",
    "                else:\n",
    "                    count[token] = {'pos':1,'neg':0,'pos_rate':rate,'neg_rate':0} \n",
    "\n",
    "        if rate<0:\n",
    "            neg_count += 1\n",
    "            NEG += len(tokens)\n",
    "            for token in tokens:\n",
    "                if len(token) < 3:\n",
    "                    continue\n",
    "                if 'not' in tokens:\n",
    "                    token = 'not_'+token\n",
    "                if token in count.keys():\n",
    "                    count[token]['neg'] += 1\n",
    "                    count[token]['neg_rate'] -= rate\n",
    "                else:\n",
    "                    count[token] = {'pos':0,'neg':1,'pos_rate':0,'neg_rate':-rate}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59244/59244 [00:00<00:00, 80290.33it/s]\n"
     ]
    }
   ],
   "source": [
    "## freq\n",
    "copy = count.copy()\n",
    "sent_words = [] # PD>0.3情感值\n",
    "\n",
    "freq_pos = pos_count/len(datas) \n",
    "freq_neg = neg_count/len(datas)\n",
    "\n",
    "# DS sent and PMI sent\n",
    "for word,value in tqdm(copy.items()):\n",
    "    if value['pos']+value['neg']<20:\n",
    "        del count[word]\n",
    "        continue\n",
    "    freq_w_pos = value['pos']/len(datas)\n",
    "    freq_w_neg = value['neg']/len(datas)\n",
    "    freq_w = (value['pos']+value['neg'])/len(datas)\n",
    "    if freq_w_pos*N == 0:\n",
    "        PMI_w_pos = 0\n",
    "    else:\n",
    "        PMI_w_pos = np.log2(freq_w_pos*N/freq_w*freq_pos)\n",
    "    if freq_w_neg*N == 0:\n",
    "        PMI_w_neg = 0\n",
    "    else:\n",
    "        PMI_w_neg = np.log2(freq_w_neg*N/freq_w*freq_neg)\n",
    "    count[word]['PMI_sent'] = PMI_w_pos - PMI_w_neg\n",
    "\n",
    "    pos = value['pos']/len(datas)\n",
    "    neg = value['neg']/len(datas)\n",
    "    value['PD'] = (pos-neg)/(pos+neg) # polarity difference\n",
    "    if abs(value['PD']) > 0.3 and nltk.pos_tag([word])[0][1] in adj+adv:  \n",
    "        sent_words.append(word)\n",
    "    count[word]['sent'] = value['PD']*value['PD'] * np.sign(value['PD'])\n",
    "\n",
    "    pos_rate = value['pos_rate']/len(datas)\n",
    "    neg_rate = value['neg_rate']/len(datas)\n",
    "    value['PD_rate'] = (pos_rate-neg_rate)/(pos_rate+neg_rate) # polarity difference\n",
    "    count[word]['sent_rate'] = value['PD_rate']*value['PD_rate'] * np.sign(value['PD_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOC -1.0\n",
      "Liveris -1.0\n",
      "Signa -1.0\n",
      "Kaufhof -1.0\n",
      "Eurosport -1.0\n",
      "Ciarallo -1.0\n",
      "Morneau -1.0\n",
      "Contrafund -1.0\n",
      "Solodyn -1.0\n",
      "TCH -1.0\n",
      "Tenable -1.0\n",
      "CITs -1.0\n",
      "Players -1.0\n",
      "Havilland -1.0\n",
      "ITEP -1.0\n",
      "Genuine -1.0\n",
      "Subsea -1.0\n",
      "GAZ -1.0\n",
      "SoCalGas -1.0\n",
      "GreenSky -1.0\n",
      "reconstitution -1.0\n",
      "Ontex -1.0\n",
      "Pick -1.0\n",
      "Energen -1.0\n",
      "PBGC -1.0\n",
      "Cristi -1.0\n",
      "Multichoice -1.0\n",
      "Marketo -1.0\n",
      "Brii -1.0\n",
      "Yao -1.0\n",
      "Binladin -1.0\n",
      "implanted -1.0\n",
      "Hetrick -1.0\n",
      "Wasp -1.0\n",
      "Perryman -1.0\n",
      "Stimwave -1.0\n",
      "neuromodulation -1.0\n",
      "Skinsei -1.0\n",
      "Ghosn -0.87890625\n",
      "Tianjin -0.8751300728407907\n",
      "Shave -0.8622448979591836\n",
      "Cineworld -0.8573388203017833\n",
      "Springer -0.8573388203017833\n",
      "Halo -0.8463999999999998\n",
      "McKenzie -0.8402777777777777\n",
      "Encana -0.8336483931947068\n",
      "Celltrion -0.8336483931947068\n",
      "rats -0.8264462809917354\n",
      "Temenos -0.8185941043083901\n",
      "PJM -0.8185941043083901\n",
      "Essendant -0.8185941043083901\n",
      "Goh -0.8099999999999998\n",
      "Newfield -0.7954711468224982\n",
      "Catalyst -0.7809626825310979\n",
      "bows -0.7656249999999998\n",
      "tournament -0.7585848074921955\n",
      "Marubeni -0.7585848074921955\n",
      "Ingraham -0.7585848074921955\n",
      "EASA -0.7346938775510203\n",
      "Fidessa -0.7256515775034292\n",
      "Walgreen -0.7256515775034292\n",
      "Acxiom -0.7199265381083562\n",
      "Lexus -0.7159763313609465\n",
      "Diamondback -0.7159763313609465\n",
      "StoneCo -0.7159763313609465\n",
      "restraining -0.7055999999999998\n",
      "Itochu -0.7055999999999998\n",
      "Samarco -0.7055999999999998\n",
      "isolation -0.6944444444444443\n",
      "exploit -0.6865306122448979\n",
      "Aliso -0.6824196597353496\n",
      "communique -0.6824196597353496\n",
      "gravity -0.6782006920415224\n",
      "NOVEMBER -0.6760493827160492\n",
      "shekels -0.6694214876033059\n",
      "Blanco -0.6694214876033057\n",
      "Delek -0.6625202812330991\n",
      "portrayed -0.6553287981859413\n",
      "KRX -0.6553287981859413\n",
      "Ramos -0.6478286734086853\n",
      "Burnaby -0.6399999999999999\n",
      "equivalence -0.6399999999999999\n",
      "Gateway -0.6399999999999999\n",
      "unfold -0.6399999999999999\n",
      "Brauer -0.6399999999999999\n",
      "marches -0.6399999999999999\n",
      "Maricunga -0.6290130796670629\n",
      "Salar -0.6173469387755101\n",
      "motive -0.6173469387755101\n",
      "Zions -0.6173469387755101\n",
      "NIO -0.6164196434669762\n",
      "Impax -0.6096828673297052\n",
      "Via -0.6049382716049381\n",
      "DWS -0.6049382716049381\n",
      "Conrath -0.600079349335449\n",
      "DJT -0.5984336062655746\n",
      "ski -0.5917159763313609\n",
      "tunnel -0.5917159763313609\n",
      "Madness -0.5917159763313609\n",
      "Tavares -0.5817293881068656\n",
      "tribe 1.0\n",
      "Gawker 1.0\n",
      "Noto 1.0\n",
      "SoFi 1.0\n",
      "Miles 1.0\n",
      "ArcSight 1.0\n",
      "FSTEC 1.0\n",
      "Bauer 1.0\n",
      "Tung 1.0\n",
      "Iancu 1.0\n",
      "Aon 1.0\n",
      "Venucia 1.0\n",
      "Seki 1.0\n",
      "IFT 1.0\n",
      "Makhzoomi 1.0\n",
      "Kubasik 1.0\n",
      "Televisa 1.0\n",
      "Scotch 1.0\n",
      "NRATV 1.0\n",
      "Tenaciou 1.0\n",
      "UOL 1.0\n",
      "Nexa 1.0\n",
      "Netshoes 1.0\n",
      "Ying 1.0\n",
      "LTSE 1.0\n",
      "Breland 1.0\n",
      "Allogene 1.0\n",
      "Leucadia 1.0\n",
      "IDA 1.0\n",
      "Cordis 1.0\n",
      "Rajaratnam 1.0\n",
      "Marqeta 1.0\n",
      "Rothman 1.0\n",
      "Aimovig 0.9518143961927426\n",
      "pea 0.9000657462195923\n",
      "ESMA 0.8668252080856124\n",
      "Shaheen 0.8520710059171597\n",
      "McAfee 0.8433985839233651\n",
      "Kalydeco 0.8402777777777777\n",
      "twitter 0.8402777777777777\n",
      "ATTN 0.8336483931947068\n",
      "paradigm 0.8264462809917354\n",
      "Hartford 0.8185941043083901\n",
      "migrants 0.8185941043083901\n",
      "Jung 0.8185941043083899\n",
      "Inter 0.8099999999999998\n",
      "Boao 0.8099999999999998\n",
      "Kass 0.8053911900065744\n",
      "shell 0.7785467128027681\n",
      "ADM 0.7767882792301063\n",
      "Harbour 0.7722681359044994\n",
      "AveXis 0.7621567145376668\n",
      "genetically 0.7346938775510203\n",
      "Loxo 0.7256515775034292\n",
      "downs 0.7055999999999998\n",
      "Arabic 0.6944444444444443\n",
      "admissions 0.6944444444444443\n",
      "assemblers 0.6944444444444443\n",
      "Neill 0.6885468537799907\n",
      "whiskey 0.6824196597353496\n",
      "McEwan 0.6782006920415224\n",
      "artist 0.6694214876033059\n",
      "Sen 0.6694214876033059\n",
      "clobbered 0.6694214876033059\n",
      "larotrectinib 0.6694214876033059\n",
      "nailed 0.6694214876033059\n",
      "lightweight 0.6553287981859413\n",
      "untreated 0.6553287981859413\n",
      "compatible 0.6553287981859413\n",
      "Polster 0.6512773160972606\n",
      "whisky 0.6503642039542142\n",
      "infringement 0.6399999999999999\n",
      "Lin 0.6399999999999999\n",
      "Hun 0.6399999999999999\n",
      "Fiserv 0.6399999999999999\n",
      "Grainger 0.6399999999999999\n",
      "Memory 0.6318211702827087\n",
      "Nektar 0.6290130796670629\n",
      "Fortnite 0.6173469387755101\n",
      "Rogers 0.6124763705103968\n",
      "Cherokee 0.6049382716049381\n",
      "Ichikawa 0.6049382716049381\n",
      "TRI 0.5917159763313609\n",
      "Ziff 0.5847750865051901\n",
      "mutations 0.580498866213152\n",
      "JUMP 0.5776\n",
      "Babytree 0.5739210284664829\n",
      "Greenberg 0.5625000000000002\n",
      "Ipreo 0.5625000000000002\n",
      "clears 0.5625\n",
      "Sonos 0.5577631789777281\n",
      "deceptively 0.5529257067718608\n",
      "Wanda 0.5504682622268472\n",
      "Shufu 0.5504682622268472\n",
      "Santos 0.5486968449931412\n",
      "tribal 0.5463137996219283\n",
      "Dealmakers 0.5463137996219283\n",
      "refrigerated 0.5463137996219283\n",
      "TRADING 0.5377777777777779\n",
      "amenable 0.5325054784514245\n"
     ]
    }
   ],
   "source": [
    "res = sorted(count.items(),key=lambda count:count[1]['sent'],reverse=False)\n",
    "for r in res[:100]:\n",
    "    print(r[0],r[1]['sent'])\n",
    "res = sorted(count.items(),key=lambda count:count[1]['sent'],reverse=True)  \n",
    "for r in res[:100]:\n",
    "    print(r[0],r[1]['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = sorted(count.items(),key=lambda count:count[1]['PD'],reverse=True)\n",
    "# print(res)\n",
    "\n",
    "## neg pos 词\n",
    "# pos_words = {}\n",
    "# neg_words = {}\n",
    "# for word in sent_words:\n",
    "#    if count[word]['sent'] > 0:\n",
    "#       pos_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "#    else:\n",
    "#       neg_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "\n",
    "# output_cloud(pos_words,'pos')\n",
    "# output_cloud(neg_words,'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 求于bl词典的覆盖率\n",
    "bl_sent = {}\n",
    "bl_pos = my_read(r'/Users/wangfeihong/Desktop/Dynamic-Financial-News-Collection-and-Analysis/sentiment_analysis/bl/positive.txt')  # 4783\n",
    "bl_neg = my_read(r'/Users/wangfeihong/Desktop/Dynamic-Financial-News-Collection-and-Analysis/sentiment_analysis/bl/negative.txt')  # 2006\n",
    "\n",
    "\n",
    "for word in bl_pos:\n",
    "    bl_sent[word] = 1\n",
    "for word in bl_pos:\n",
    "    bl_sent[word] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc = 0\n",
    "# for word in pos_words:\n",
    "#    if word in bl_pos:\n",
    "#       pc += 1\n",
    "# pos_accuracy = pc/len(pos_words)  # 0.2857142857142857\n",
    "\n",
    "# nc = 0\n",
    "# for word in neg_words:\n",
    "#    if word in bl_neg:\n",
    "#       nc += 1\n",
    "# neg_accuracy = nc/len(neg_words)  # 0.18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20961/20961 [00:02<00:00, 8076.35it/s]\n"
     ]
    }
   ],
   "source": [
    "## context sentiment dict\n",
    "sent_words = [word.lower() for word in sent_words]\n",
    "feature_words = {}\n",
    "sentiment_feature = {}\n",
    "\n",
    "for data in tqdm(datas):\n",
    "    for tags in data['tags']:\n",
    "        for word,tag in tags:\n",
    "            if tag not in nn or len(word)<3: # vb+nn\n",
    "                continue\n",
    "            # word = stem_and_check(word)\n",
    "            if word not in feature_words.keys():\n",
    "                feature_words[word] = 1\n",
    "            else:\n",
    "                feature_words[word] += 1\n",
    "\n",
    "# avg_f = sum([item[1] for item in feature_words.items()])/len(feature_words.keys())\n",
    "res = sorted(feature_words.items(),key=lambda feature_words:feature_words[1],reverse=True)\n",
    "words = res[:400]\n",
    "# for word,value in tqdm(copy.items()):\n",
    "#     if value<avg_f+200:\n",
    "#         del feature_words[word]\n",
    "\n",
    "feature_words = [inf.singularize(word).lower() for word,freq in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 736/20961 [00:15<07:09, 47.07it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2544029d3891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtoken_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mtoken_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0m_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/inflection.py\u001b[0m in \u001b[0;36msingularize\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSINGULARS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/re.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    181\u001b[0m     a match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sf_len = 0\n",
    "for data in tqdm(datas):\n",
    "    rate = data['rate']\n",
    "    for tokens in data['tokens']:\n",
    "        token_dict = {}\n",
    "        for token in tokens:\n",
    "            token_dict[inf.singularize(token).lower()] = token\n",
    "        _tokens = [inf.singularize(token).lower() for token in tokens]\n",
    "\n",
    "        for w in list(set(sent_words).intersection(set(_tokens))):\n",
    "            for f in list(set(feature_words).intersection(set(_tokens))):\n",
    "                if f != w:\n",
    "                    if abs(_tokens.index(w)-_tokens.index(f))<3 and ',' not in data['content'][min(data['content'].index(token_dict[f]),data['content'].index(token_dict[w])):max(data['content'].index(token_dict[f]),data['content'].index(token_dict[w]))]:\n",
    "                        sf_len += 1\n",
    "                        if f not in sentiment_feature.keys():\n",
    "                            sentiment_feature[f] = {}\n",
    "                            if rate > 0:\n",
    "                                sentiment_feature[f][w] = {'pos':1,'neg':0}\n",
    "                            if rate < 0:\n",
    "                                sentiment_feature[f][w] = {'pos':0,'neg':1}\n",
    "                        else:\n",
    "                            if w not in sentiment_feature[f].keys():\n",
    "                                sentiment_feature[f][w] = {'pos':0,'neg':0}\n",
    "                            if rate > 0:\n",
    "                                sentiment_feature[f][w]['pos'] += 1\n",
    "                            if rate < 0:\n",
    "                                sentiment_feature[f][w]['neg'] += 1\n",
    "\n",
    "avg_sf = sf_len/len(sentiment_feature.keys())\n",
    "copy = sentiment_feature.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20961 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 99/20961 [00:00<00:21, 987.11it/s]\u001b[A\n",
      "  1%|▏         | 264/20961 [00:00<00:18, 1121.49it/s]\u001b[A\n",
      "  2%|▏         | 428/20961 [00:00<00:16, 1238.35it/s]\u001b[A\n",
      "  3%|▎         | 642/20961 [00:00<00:14, 1417.39it/s]\u001b[A\n",
      "  4%|▍         | 854/20961 [00:00<00:12, 1571.21it/s]\u001b[A\n",
      "  5%|▍         | 1029/20961 [00:00<00:12, 1618.54it/s]\u001b[A\n",
      "  6%|▌         | 1212/20961 [00:00<00:11, 1674.85it/s]\u001b[A\n",
      "  7%|▋         | 1410/20961 [00:00<00:11, 1754.04it/s]\u001b[A\n",
      "  8%|▊         | 1588/20961 [00:00<00:11, 1761.12it/s]\u001b[A\n",
      "  8%|▊         | 1765/20961 [00:01<00:11, 1724.96it/s]\u001b[A\n",
      "  9%|▉         | 1939/20961 [00:01<00:11, 1598.47it/s]\u001b[A\n",
      " 10%|█         | 2148/20961 [00:01<00:10, 1718.86it/s]\u001b[A\n",
      " 11%|█         | 2356/20961 [00:01<00:10, 1811.16it/s]\u001b[A\n",
      " 12%|█▏        | 2542/20961 [00:01<00:10, 1813.23it/s]\u001b[A\n",
      " 13%|█▎        | 2760/20961 [00:01<00:09, 1907.38it/s]\u001b[A\n",
      " 14%|█▍        | 2960/20961 [00:01<00:09, 1927.94it/s]\u001b[A\n",
      " 15%|█▌        | 3156/20961 [00:01<00:09, 1872.65it/s]\u001b[A\n",
      " 16%|█▌        | 3360/20961 [00:01<00:09, 1916.90it/s]\u001b[A\n",
      " 17%|█▋        | 3557/20961 [00:01<00:09, 1932.08it/s]\u001b[A\n",
      " 18%|█▊        | 3759/20961 [00:02<00:08, 1955.69it/s]\u001b[A\n",
      "  4%|▎         | 736/20961 [00:30<07:09, 47.07it/s]/s]\u001b[A\n",
      " 20%|█▉        | 4156/20961 [00:02<00:08, 1918.80it/s]\u001b[A\n",
      " 21%|██        | 4349/20961 [00:02<00:09, 1766.88it/s]\u001b[A\n",
      " 22%|██▏       | 4529/20961 [00:02<00:09, 1724.71it/s]\u001b[A\n",
      " 22%|██▏       | 4704/20961 [00:02<00:09, 1673.88it/s]\u001b[A\n",
      " 23%|██▎       | 4876/20961 [00:02<00:09, 1687.14it/s]\u001b[A\n",
      " 24%|██▍       | 5047/20961 [00:02<00:09, 1638.83it/s]\u001b[A\n",
      " 25%|██▌       | 5241/20961 [00:02<00:09, 1717.26it/s]\u001b[A\n",
      " 26%|██▌       | 5415/20961 [00:03<00:09, 1573.00it/s]\u001b[A\n",
      " 27%|██▋       | 5598/20961 [00:03<00:09, 1640.52it/s]\u001b[A\n",
      " 28%|██▊       | 5805/20961 [00:03<00:08, 1748.69it/s]\u001b[A\n",
      " 29%|██▊       | 5985/20961 [00:03<00:08, 1733.43it/s]\u001b[A\n",
      " 29%|██▉       | 6176/20961 [00:03<00:08, 1780.52it/s]\u001b[A\n",
      " 30%|███       | 6357/20961 [00:03<00:08, 1723.86it/s]\u001b[A\n",
      " 31%|███       | 6532/20961 [00:04<00:28, 514.46it/s] \u001b[A\n",
      " 32%|███▏      | 6660/20961 [00:04<00:23, 617.48it/s]\u001b[A\n",
      " 32%|███▏      | 6812/20961 [00:04<00:18, 750.93it/s]\u001b[A\n",
      " 33%|███▎      | 6982/20961 [00:04<00:15, 901.40it/s]\u001b[A\n",
      " 34%|███▍      | 7126/20961 [00:04<00:13, 1001.65it/s]\u001b[A\n",
      " 35%|███▍      | 7268/20961 [00:05<00:12, 1054.16it/s]\u001b[A\n",
      " 35%|███▌      | 7403/20961 [00:05<00:12, 1054.41it/s]\u001b[A\n",
      " 36%|███▌      | 7529/20961 [00:05<00:12, 1087.01it/s]\u001b[A\n",
      " 37%|███▋      | 7658/20961 [00:05<00:11, 1138.31it/s]\u001b[A\n",
      " 37%|███▋      | 7787/20961 [00:05<00:11, 1179.07it/s]\u001b[A\n",
      " 38%|███▊      | 7958/20961 [00:05<00:10, 1299.14it/s]\u001b[A\n",
      " 39%|███▊      | 8110/20961 [00:05<00:09, 1358.30it/s]\u001b[A\n",
      " 39%|███▉      | 8254/20961 [00:05<00:09, 1344.51it/s]\u001b[A\n",
      " 40%|████      | 8394/20961 [00:05<00:09, 1280.69it/s]\u001b[A\n",
      " 41%|████      | 8564/20961 [00:05<00:08, 1382.75it/s]\u001b[A\n",
      " 42%|████▏     | 8708/20961 [00:06<00:08, 1364.30it/s]\u001b[A\n",
      " 42%|████▏     | 8859/20961 [00:06<00:08, 1402.24it/s]\u001b[A\n",
      " 43%|████▎     | 9003/20961 [00:06<00:09, 1309.11it/s]\u001b[A\n",
      " 44%|████▎     | 9153/20961 [00:06<00:08, 1358.74it/s]\u001b[A\n",
      " 44%|████▍     | 9292/20961 [00:06<00:09, 1267.76it/s]\u001b[A\n",
      " 45%|████▍     | 9422/20961 [00:06<00:09, 1265.94it/s]\u001b[A\n",
      " 46%|████▌     | 9584/20961 [00:06<00:08, 1353.54it/s]\u001b[A\n",
      " 46%|████▋     | 9731/20961 [00:06<00:08, 1382.35it/s]\u001b[A\n",
      " 47%|████▋     | 9872/20961 [00:06<00:08, 1317.42it/s]\u001b[A\n",
      " 48%|████▊     | 10007/20961 [00:07<00:08, 1325.60it/s]\u001b[A\n",
      " 48%|████▊     | 10144/20961 [00:07<00:08, 1336.07it/s]\u001b[A\n",
      " 49%|████▉     | 10285/20961 [00:07<00:07, 1354.75it/s]\u001b[A\n",
      " 50%|████▉     | 10422/20961 [00:07<00:07, 1355.05it/s]\u001b[A\n",
      " 50%|█████     | 10582/20961 [00:07<00:07, 1416.97it/s]\u001b[A\n",
      " 51%|█████     | 10725/20961 [00:07<00:07, 1417.90it/s]\u001b[A\n",
      " 52%|█████▏    | 10882/20961 [00:07<00:06, 1455.79it/s]\u001b[A\n",
      " 53%|█████▎    | 11046/20961 [00:07<00:06, 1506.16it/s]\u001b[A\n",
      " 53%|█████▎    | 11207/20961 [00:07<00:06, 1533.82it/s]\u001b[A\n",
      " 54%|█████▍    | 11362/20961 [00:07<00:06, 1533.57it/s]\u001b[A\n",
      " 55%|█████▍    | 11522/20961 [00:08<00:06, 1552.73it/s]\u001b[A\n",
      " 56%|█████▌    | 11686/20961 [00:08<00:05, 1577.88it/s]\u001b[A\n",
      " 57%|█████▋    | 11845/20961 [00:08<00:06, 1485.73it/s]\u001b[A\n",
      " 57%|█████▋    | 11995/20961 [00:08<00:06, 1479.60it/s]\u001b[A\n",
      " 58%|█████▊    | 12146/20961 [00:08<00:05, 1487.86it/s]\u001b[A\n",
      " 59%|█████▊    | 12296/20961 [00:08<00:06, 1436.29it/s]\u001b[A\n",
      " 59%|█████▉    | 12441/20961 [00:08<00:06, 1408.79it/s]\u001b[A\n",
      " 60%|██████    | 12583/20961 [00:08<00:06, 1330.94it/s]\u001b[A\n",
      " 61%|██████    | 12718/20961 [00:08<00:06, 1333.29it/s]\u001b[A\n",
      " 61%|██████▏   | 12884/20961 [00:09<00:05, 1416.62it/s]\u001b[A\n",
      " 62%|██████▏   | 13028/20961 [00:09<00:05, 1347.49it/s]\u001b[A\n",
      " 63%|██████▎   | 13165/20961 [00:09<00:06, 1230.86it/s]\u001b[A\n",
      " 63%|██████▎   | 13292/20961 [00:09<00:06, 1138.55it/s]\u001b[A\n",
      " 64%|██████▍   | 13410/20961 [00:09<00:07, 1064.15it/s]\u001b[A\n",
      " 65%|██████▍   | 13521/20961 [00:09<00:07, 992.10it/s] \u001b[A\n",
      " 65%|██████▌   | 13629/20961 [00:09<00:07, 1016.08it/s]\u001b[A\n",
      " 66%|██████▌   | 13734/20961 [00:09<00:07, 1009.62it/s]\u001b[A\n",
      " 66%|██████▌   | 13837/20961 [00:10<00:07, 911.86it/s] \u001b[A\n",
      " 66%|██████▋   | 13932/20961 [00:10<00:08, 872.72it/s]\u001b[A\n",
      " 67%|██████▋   | 14027/20961 [00:10<00:07, 893.77it/s]\u001b[A\n",
      " 67%|██████▋   | 14119/20961 [00:10<00:07, 878.05it/s]\u001b[A\n",
      " 68%|██████▊   | 14209/20961 [00:10<00:07, 874.55it/s]\u001b[A\n",
      " 68%|██████▊   | 14298/20961 [00:10<00:07, 870.26it/s]\u001b[A\n",
      " 69%|██████▊   | 14408/20961 [00:10<00:07, 927.84it/s]\u001b[A\n",
      " 69%|██████▉   | 14524/20961 [00:10<00:06, 986.25it/s]\u001b[A\n",
      " 70%|██████▉   | 14627/20961 [00:10<00:06, 996.32it/s]\u001b[A\n",
      " 70%|███████   | 14729/20961 [00:10<00:06, 976.73it/s]\u001b[A\n",
      " 71%|███████   | 14828/20961 [00:11<00:06, 905.65it/s]\u001b[A\n",
      " 71%|███████   | 14921/20961 [00:11<00:08, 689.33it/s]\u001b[A\n",
      " 72%|███████▏  | 14999/20961 [00:11<00:08, 709.30it/s]\u001b[A\n",
      " 72%|███████▏  | 15080/20961 [00:11<00:07, 735.41it/s]\u001b[A\n",
      " 73%|███████▎  | 15198/20961 [00:11<00:06, 828.34it/s]\u001b[A\n",
      " 73%|███████▎  | 15292/20961 [00:11<00:06, 857.13it/s]\u001b[A\n",
      " 73%|███████▎  | 15396/20961 [00:11<00:06, 904.42it/s]\u001b[A\n",
      " 74%|███████▍  | 15491/20961 [00:11<00:06, 891.15it/s]\u001b[A\n",
      " 74%|███████▍  | 15584/20961 [00:12<00:06, 892.32it/s]\u001b[A\n",
      " 75%|███████▍  | 15676/20961 [00:12<00:05, 893.37it/s]\u001b[A\n",
      " 75%|███████▌  | 15772/20961 [00:12<00:05, 910.76it/s]\u001b[A\n",
      " 76%|███████▌  | 15869/20961 [00:12<00:05, 924.56it/s]\u001b[A\n",
      " 76%|███████▌  | 15966/20961 [00:12<00:05, 935.56it/s]\u001b[A\n",
      " 77%|███████▋  | 16067/20961 [00:12<00:05, 956.47it/s]\u001b[A\n",
      " 77%|███████▋  | 16164/20961 [00:12<00:04, 960.34it/s]\u001b[A\n",
      " 78%|███████▊  | 16285/20961 [00:12<00:04, 1023.36it/s]\u001b[A\n",
      " 78%|███████▊  | 16414/20961 [00:12<00:04, 1089.54it/s]\u001b[A\n",
      " 79%|███████▉  | 16526/20961 [00:12<00:04, 1096.24it/s]\u001b[A\n",
      " 79%|███████▉  | 16638/20961 [00:13<00:04, 1013.23it/s]\u001b[A\n",
      " 80%|███████▉  | 16742/20961 [00:13<00:04, 965.34it/s] \u001b[A\n",
      " 80%|████████  | 16858/20961 [00:13<00:04, 1015.87it/s]\u001b[A\n",
      " 81%|████████  | 16966/20961 [00:13<00:03, 1031.30it/s]\u001b[A\n",
      " 81%|████████▏ | 17071/20961 [00:13<00:03, 1007.27it/s]\u001b[A\n",
      " 82%|████████▏ | 17178/20961 [00:13<00:03, 1024.14it/s]\u001b[A\n",
      " 82%|████████▏ | 17282/20961 [00:13<00:03, 1023.61it/s]\u001b[A\n",
      " 83%|████████▎ | 17386/20961 [00:13<00:03, 1017.19it/s]\u001b[A\n",
      " 83%|████████▎ | 17489/20961 [00:13<00:03, 1015.38it/s]\u001b[A\n",
      " 84%|████████▍ | 17591/20961 [00:14<00:03, 981.15it/s] \u001b[A\n",
      " 84%|████████▍ | 17690/20961 [00:14<00:03, 964.13it/s]\u001b[A\n",
      " 85%|████████▍ | 17809/20961 [00:14<00:03, 1021.69it/s]\u001b[A\n",
      " 85%|████████▌ | 17913/20961 [00:14<00:03, 977.20it/s] \u001b[A\n",
      " 86%|████████▌ | 18012/20961 [00:14<00:03, 958.93it/s]\u001b[A\n",
      " 86%|████████▋ | 18109/20961 [00:14<00:02, 951.46it/s]\u001b[A\n",
      " 87%|████████▋ | 18206/20961 [00:14<00:02, 955.56it/s]\u001b[A\n",
      " 87%|████████▋ | 18334/20961 [00:14<00:02, 1032.46it/s]\u001b[A\n",
      " 88%|████████▊ | 18440/20961 [00:14<00:02, 1016.94it/s]\u001b[A\n",
      " 89%|████████▊ | 18555/20961 [00:14<00:02, 1051.04it/s]\u001b[A\n",
      " 89%|████████▉ | 18662/20961 [00:15<00:02, 964.77it/s] \u001b[A\n",
      " 90%|████████▉ | 18761/20961 [00:15<00:02, 924.38it/s]\u001b[A\n",
      " 90%|████████▉ | 18861/20961 [00:15<00:02, 943.35it/s]\u001b[A\n",
      " 90%|█████████ | 18957/20961 [00:15<00:02, 940.43it/s]\u001b[A\n",
      " 91%|█████████ | 19053/20961 [00:15<00:02, 905.84it/s]\u001b[A\n",
      " 91%|█████████▏| 19157/20961 [00:15<00:01, 942.23it/s]\u001b[A\n",
      " 92%|█████████▏| 19253/20961 [00:15<00:01, 916.28it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 19348/20961 [00:15<00:01, 925.18it/s]\u001b[A\n",
      " 93%|█████████▎| 19442/20961 [00:15<00:01, 918.38it/s]\u001b[A\n",
      " 93%|█████████▎| 19537/20961 [00:16<00:01, 925.58it/s]\u001b[A\n",
      " 94%|█████████▎| 19644/20961 [00:16<00:01, 963.33it/s]\u001b[A\n",
      " 94%|█████████▍| 19741/20961 [00:16<00:01, 949.79it/s]\u001b[A\n",
      " 95%|█████████▍| 19840/20961 [00:16<00:01, 961.25it/s]\u001b[A\n",
      " 95%|█████████▌| 19948/20961 [00:16<00:01, 993.70it/s]\u001b[A\n",
      " 96%|█████████▌| 20048/20961 [00:16<00:00, 956.27it/s]\u001b[A\n",
      " 96%|█████████▌| 20154/20961 [00:16<00:00, 985.03it/s]\u001b[A\n",
      " 97%|█████████▋| 20256/20961 [00:16<00:00, 994.62it/s]\u001b[A\n",
      " 97%|█████████▋| 20359/20961 [00:16<00:00, 1004.48it/s]\u001b[A\n",
      " 98%|█████████▊| 20463/20961 [00:16<00:00, 1013.94it/s]\u001b[A\n",
      " 98%|█████████▊| 20565/20961 [00:17<00:00, 964.23it/s] \u001b[A\n",
      " 99%|█████████▊| 20663/20961 [00:17<00:00, 944.29it/s]\u001b[A\n",
      " 99%|█████████▉| 20759/20961 [00:17<00:00, 935.16it/s]\u001b[A\n",
      "100%|█████████▉| 20861/20961 [00:17<00:00, 958.59it/s]\u001b[A\n",
      "100%|█████████▉| 20958/20961 [00:17<00:00, 913.61it/s]\u001b[A\n",
      "100%|██████████| 20961/20961 [00:17<00:00, 1195.75it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "# content to vector\n",
    "\n",
    "for data in tqdm(datas):\n",
    "    idx = datas.index(data)\n",
    "    tokens = data['tokens']\n",
    "    datas[idx]['DsVector'] = [0,0,0,0]\n",
    "    datas[idx]['DsVector_rate'] = [0,0,0,0]\n",
    "    datas[idx]['SnVector'] = [0,0,0,0]\n",
    "    datas[idx]['BlVector'] = [0,0,0,0]\n",
    "    datas[idx]['PmiVector'] = [0,0,0,0]\n",
    "    datas[idx]['ContextVector'] = [0,0,0,0]\n",
    "    \n",
    "    # for f in [token for token in tokens if token in sentiment_feature.keys()]:\n",
    "    #     for w in sentiment_feature[f].keys():\n",
    "    #         if w in tokens:\n",
    "    #             if abs(tokens.index(f)-tokens.index(w))<3 and ',' not in data['content'][min(data['content'].index(f),data['content'].index(w)):max(data['content'].index(f),data['content'].index(w))]:\n",
    "    #                 if tags[tokens.index(f)][1] in adj:\n",
    "    #                     if f in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][0] += count[f]['sent']\n",
    "    #                 elif tags[tokens.index(f)][1] in adv:\n",
    "    #                     if f in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][1] += count[f]['sent']\n",
    "    #                 if tags[tokens.index(w)][1] in nn:\n",
    "    #                     if w in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][2] = count[w]['sent']\n",
    "    #                 elif tags[tokens.index(w)][1] in vb:\n",
    "    #                     if w in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][3] = count[w]['sent']\n",
    "    #                 # print(sentiment_feature[f][w]['sent'])\n",
    "    for tags in data['tags']:\n",
    "        for word,tag in tags:\n",
    "            if tag in adj:\n",
    "                if word in count.keys():\n",
    "                    datas[idx]['DsVector'][0] += count[word]['sent']\n",
    "                    datas[idx]['DsVector_rate'][0] += count[word]['sent_rate']\n",
    "                    datas[idx]['PmiVector'][0] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas[idx]['SnVector'][0] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas[idx]['BlVector'][0] += bl_sent[word]\n",
    "            elif tag in adv:\n",
    "                if word in count.keys():\n",
    "                    datas[idx]['SnVector'][1] += count[word]['sent']\n",
    "                    datas[idx]['DsVector_rate'][1] += count[word]['sent_rate']\n",
    "                    datas[idx]['PmiVector'][1] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas[idx]['SnVector'][1] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas[idx]['BlVector'][1] += bl_sent[word]  \n",
    "            elif tag in nn:\n",
    "                if word in count.keys():\n",
    "                    datas[idx]['DsVector'][2] = count[word]['sent']\n",
    "                    datas[idx]['DsVector_rate'][2] += count[word]['sent_rate']\n",
    "                    datas[idx]['PmiVector'][2] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas[idx]['SnVector'][2] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas[idx]['BlVector'][2] += bl_sent[word]\n",
    "            elif tag in vb:\n",
    "                if word in count.keys():\n",
    "                    datas[idx]['DsVector'][3] = count[word]['sent']\n",
    "                    datas[idx]['DsVector_rate'][3] += count[word]['sent_rate']\n",
    "                    datas[idx]['PmiVector'][3] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas[idx]['SnVector'][3] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas[idx]['BlVector'][3] += bl_sent[word]\n",
    "    # datas[idx]['DsVector'] = [adv_score,adv_score,noun_score,verb_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.6670641545432864\n",
      "召回率： 0.44918354553004747\n",
      "精确率： 0.4459919380917163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [data['DsVector'] for data in datas]\n",
    "# X = [data['SnVector'] for data in datas]\n",
    "# X = [data['BlVector'] for data in datas]\n",
    "# X = [data['PmiVector'] for data in datas]\n",
    "# X = [data['ContextVector'] for data in datas]\n",
    "X = [data['DsVector_rate'] for data in datas]\n",
    "Y = [np.sign(data['rate']) for data in datas]\n",
    "# Y = [data['rate'] for data in datas]\n",
    "\n",
    "train_x,test_x,train_y,test_y = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "# # x_train = X[:2500]\n",
    "# # y_train = Y[:2500]\n",
    "# # x_test = X[-300:]\n",
    "# # y_test = Y[-300:]\n",
    "clf = GaussianNB()\n",
    "# clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.6565704746005246\n",
      "召回率： 0.44413870593941013\n",
      "精确率： 0.4477928072686903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "X = [data['DsVector'] for data in datas]\n",
    "clf = GaussianNB()\n",
    "# clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.6670641545432864\n",
      "召回率： 0.44918354553004747\n",
      "精确率： 0.4459919380917163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
