{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senticnet.senticnet import SenticNet\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import IPython\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import inflection as inf\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models import Word2Vec\n",
    "import enchant\n",
    "import time\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "adj = ['JJ','JJR','JJS']\n",
    "adv = ['RB','RBR','RBS']\n",
    "vb = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "nn = ['NN','NNS']\n",
    "\n",
    "\n",
    "def review_to_words(review_text): \n",
    "    if '(Reuters) -' in review_text:\n",
    "        review_text = review_text.split('(Reuters) -')[1]\n",
    "    if '*' in review_text:\n",
    "        review_text = review_text.split('*')[1]\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.split()                             \n",
    "    _stopwords = set(stopwords.words(\"english\"))\n",
    "    _stopwords = nltk.corpus.stopwords.words('english')\n",
    "    _stopwords.append('would')\n",
    "    _stopwords.append('kmh')\n",
    "    _stopwords.append('mph')\n",
    "    _stopwords.append('  ')\n",
    "    _stopwords.append('Reuters')\n",
    "    _stopwords.append('reuters')\n",
    "    # _stopwords = []\n",
    "    meaningful_words = [w for w in words if w not in _stopwords]\n",
    "    return meaningful_words\n",
    "\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def stem_and_check(word):\n",
    "    word = nlp(word)\n",
    "    return word[0].lemma_\n",
    "    # word = inf.singularize(word)\n",
    "    # word = nltk.PorterStemmer().stem(word)\n",
    "    # if d.check(word):\n",
    "    #    return word\n",
    "    # suggest_words = d.suggest(word)\n",
    "    # if len(suggest_words) == 0:\n",
    "    #    return word\n",
    "    # return suggest_words[0]\n",
    "\n",
    "def my_read(path):\n",
    "    file = open(path)\n",
    "    words = []\n",
    "    for line in file.readlines():\n",
    "        words.append(line.strip())\n",
    "    return words\n",
    "\n",
    "def output_cloud(count,name):\n",
    "    # 云图\n",
    "    text = '' \n",
    "    for key,value in count.items():\n",
    "        text += (key+' ') * (value)\n",
    "    wc = WordCloud(\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        max_font_size=100,      #字体大小\n",
    "        min_font_size=10,\n",
    "        collocations=False, \n",
    "        max_words=1000\n",
    "    )\n",
    "    wc.generate(text)\n",
    "    wc.to_file(name+'.png') #图片保存\n",
    "\n",
    "sn = SenticNet()\n",
    "# concept_info = sn.concept('love')\n",
    "# polarity_value = sn.polarity_value('love')\n",
    "# polarity_intense = sn.polarity_intense('love')\n",
    "# moodtags = sn.moodtags('love')\n",
    "# semantics = sn.semantics('love')\n",
    "# sentics = sn.sentics('love') \n",
    " \n",
    "def sentiment_score(text):\n",
    "    t = TextBlob(text)\n",
    "    score = t.sentiment.polarity\n",
    "    return score\n",
    "    # tokens = nltk.word_tokenize(text)\n",
    "    # pos_tags = nltk.pos_tag(tokens)\n",
    "    # score = 0\n",
    "    # count = 0\n",
    "    # for word,tag in pos_tags:\n",
    "    #   if word in sn.data.keys():\n",
    "    #    score += float(sn.polarity_intense(word))\n",
    "    #    count += 1\n",
    "    #    # print(word,sn.polarity_intense(word))\n",
    "    # if count == 0: #mid\n",
    "    #   return -1 \n",
    "    # return score/count\n",
    "\n",
    "# test sentiment_score accuracy\n",
    "# scores = []\n",
    "# workbook = pd.read_csv(u'sentiment.csv',encoding='ISO-8859-1')\n",
    "# correct = 0\n",
    "# pos_count = 0\n",
    "# neg_count = 0\n",
    "# pos_correct = 0\n",
    "# neg_correct = 0\n",
    "\n",
    "# for i in tqdm(range(0,10000)):\n",
    "#   i = int(random.random()*1599999)\n",
    "#   if workbook.loc[i][0] == 0:\n",
    "#    neg_count += 1\n",
    "#    if workbook.loc[i][0] == sentiment_score_list(workbook.loc[i][5]):\n",
    "#     neg_correct += 1\n",
    "#   elif workbook.loc[i][0] == 4:\n",
    "#    pos_count += 1\n",
    "#    if workbook.loc[i][0] == sentiment_score_list(workbook.loc[i][5]):\n",
    "#     pos_correct += 1\n",
    "# print('pos',pos_correct/pos_count,pos_count,pos_correct)\n",
    "# print('neg',neg_correct/neg_count,neg_count,neg_correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    path2018 = r'/Users/wangfeihong/Desktop/Dynamic-Financial-News-Collection-and-Analysis/data/labeled_data2018.xls'\n",
    "    path2019 = r'/Users/wangfeihong/Desktop/Dynamic-Financial-News-Collection-and-Analysis/data/labeled_data2019.xls'\n",
    "\n",
    "    workbook = xlrd.open_workbook(path2018)\n",
    "    worksheet = workbook.sheet_by_index(0)\n",
    "    contents = worksheet.col_values(1)\n",
    "    companies = worksheet.col_values(2)\n",
    "    prices = worksheet.col_values(3)\n",
    "    dates = worksheet.col_values(4)\n",
    "    rates = []\n",
    "    score_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20961/20961 [10:41<00:00, 32.66it/s]\n"
     ]
    }
   ],
   "source": [
    "datas = []\n",
    "for i in tqdm(range(0,len(contents))):\n",
    "    # if '*' not in contents[i]:\n",
    "        # if companies[i] != 'Apple Inc.':\n",
    "        #     continue\n",
    "    price_list = json.loads(prices[i])   \n",
    "    if price_list[2] == price_list[1]:\n",
    "        continue\n",
    "    data = {}\n",
    "    data['content'] = contents[i]\n",
    "    sents = sent_tokenize(data['content'])\n",
    "    data['tokens'] = []\n",
    "    data['tags'] = []\n",
    "    for sent in sents:\n",
    "        token = review_to_words(sent) # 去停用词会影响词性标注吗？？\n",
    "        data['tokens'].append(token)\n",
    "        data['tags'].append(nltk.pos_tag(token))\n",
    "    data['company'] = companies[i]\n",
    "    data['rate'] = (price_list[2]-price_list[1])/price_list[1]\n",
    "    data['date'] = dates[i]\n",
    "    datas.append(data)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4803/4803 [02:26<00:00, 32.87it/s]\n"
     ]
    }
   ],
   "source": [
    "workbook = xlrd.open_workbook(path2019)\n",
    "worksheet = workbook.sheet_by_index(0)\n",
    "contents = worksheet.col_values(1)\n",
    "companies = worksheet.col_values(2)\n",
    "prices = worksheet.col_values(3)\n",
    "dates = worksheet.col_values(4)\n",
    "rates = []\n",
    "score_list = []\n",
    "datas2 = []\n",
    "for i in tqdm(range(0,len(contents))):\n",
    "    # if '*' not in contents[i]:\n",
    "        # if companies[i] != 'Apple Inc.':\n",
    "        #     continue\n",
    "    price_list = json.loads(prices[i]) \n",
    "    if price_list[2] == price_list[1]:\n",
    "        continue\n",
    "    data = {}\n",
    "    data['content'] = contents[i]\n",
    "    sents = sent_tokenize(data['content'])\n",
    "    data['tokens'] = []\n",
    "    data['tags'] = []\n",
    "    for sent in sents:\n",
    "        token = review_to_words(sent) # 去停用词会影响词性标注吗？？\n",
    "        data['tokens'].append(token)\n",
    "        data['tags'].append(nltk.pos_tag(token))\n",
    "    data['company'] = companies[i]       \n",
    "    data['rate'] = (price_list[2]-price_list[1])/price_list[1]\n",
    "    data['date'] = dates[i]\n",
    "    datas2.append(data)\n",
    "\n",
    "## sort by time\n",
    "# dates = {}\n",
    "# for i in tqdm(range(0,len(_datas))):\n",
    "#     dates[i] = int(time.mktime(time.strptime(_datas[i]['date'], \"%Y-%m-%d\")))\n",
    "# res = sorted(dates.items(),key=lambda dates:dates[1],reverse=False)\n",
    "# datas = []\n",
    "# for idx,date in res:\n",
    "#     datas.append(_datas[idx])\n",
    "\n",
    "count = {}\n",
    "\n",
    "POS = 0\n",
    "NEG = 0\n",
    "\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "\n",
    "N = 0 #len of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20819/20819 [00:07<00:00, 2831.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(datas):\n",
    "    for tokens in data['tokens']: \n",
    "        N += len(tokens)\n",
    "        rate = data['rate'] # 选当天的股票变化判断涨跌，因为相关度当天的最高\n",
    "        if rate>0:\n",
    "            pos_count += 1\n",
    "            POS += len(tokens)\n",
    "            for token in tokens:\n",
    "                if len(token) < 3:\n",
    "                    continue\n",
    "                if 'not' in tokens:\n",
    "                    token = 'not_'+token\n",
    "                if token in count.keys():\n",
    "                    count[token]['pos'] += 1\n",
    "                    count[token]['pos_rate'] += rate\n",
    "                else:\n",
    "                    count[token] = {'pos':1,'neg':0,'pos_rate':rate,'neg_rate':0} \n",
    "\n",
    "        if rate<0:\n",
    "            neg_count += 1\n",
    "            NEG += len(tokens)\n",
    "            for token in tokens:\n",
    "                if len(token) < 3:\n",
    "                    continue\n",
    "                if 'not' in tokens:\n",
    "                    token = 'not_'+token\n",
    "                if token in count.keys():\n",
    "                    count[token]['neg'] += 1\n",
    "                    count[token]['neg_rate'] -= rate\n",
    "                else:\n",
    "                    count[token] = {'pos':0,'neg':1,'pos_rate':0,'neg_rate':-rate}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59244/59244 [00:00<00:00, 80496.98it/s]\n"
     ]
    }
   ],
   "source": [
    "## freq\n",
    "copy = count.copy()\n",
    "sent_words = [] # PD>0.3情感值\n",
    "\n",
    "freq_pos = pos_count/len(datas) \n",
    "freq_neg = neg_count/len(datas)\n",
    "\n",
    "# DS sent and PMI sent\n",
    "for word,value in tqdm(copy.items()):\n",
    "    if value['pos']+value['neg']<20:\n",
    "        del count[word]\n",
    "        continue\n",
    "    freq_w_pos = value['pos']/len(datas)\n",
    "    freq_w_neg = value['neg']/len(datas)\n",
    "    freq_w = (value['pos']+value['neg'])/len(datas)\n",
    "    if freq_w_pos*N == 0:\n",
    "        PMI_w_pos = 0\n",
    "    else:\n",
    "        PMI_w_pos = np.log2(freq_w_pos*N/freq_w*freq_pos)\n",
    "    if freq_w_neg*N == 0:\n",
    "        PMI_w_neg = 0\n",
    "    else:\n",
    "        PMI_w_neg = np.log2(freq_w_neg*N/freq_w*freq_neg)\n",
    "    count[word]['PMI_sent'] = PMI_w_pos - PMI_w_neg\n",
    "\n",
    "    pos = value['pos']/len(datas)\n",
    "    neg = value['neg']/len(datas)\n",
    "    value['PD'] = (pos-neg)/(pos+neg) # polarity difference\n",
    "    if abs(value['PD']) > 0.3 and nltk.pos_tag([word])[0][1] in adj+adv:  \n",
    "        sent_words.append(word)\n",
    "    count[word]['sent'] = value['PD']*value['PD'] * np.sign(value['PD'])\n",
    "\n",
    "    pos_rate = value['pos_rate']/len(datas)\n",
    "    neg_rate = value['neg_rate']/len(datas)\n",
    "    value['PD_rate'] = (pos_rate-neg_rate)/(pos_rate+neg_rate) # polarity difference\n",
    "    count[word]['sent_rate'] = value['PD_rate']*value['PD_rate'] * np.sign(value['PD_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOC -1.0\n",
      "Liveris -1.0\n",
      "Signa -1.0\n",
      "Kaufhof -1.0\n",
      "Eurosport -1.0\n",
      "Ciarallo -1.0\n",
      "Morneau -1.0\n",
      "Contrafund -1.0\n",
      "Solodyn -1.0\n",
      "TCH -1.0\n",
      "Tenable -1.0\n",
      "CITs -1.0\n",
      "Players -1.0\n",
      "Havilland -1.0\n",
      "ITEP -1.0\n",
      "Genuine -1.0\n",
      "Subsea -1.0\n",
      "GAZ -1.0\n",
      "SoCalGas -1.0\n",
      "GreenSky -1.0\n",
      "reconstitution -1.0\n",
      "Ontex -1.0\n",
      "Pick -1.0\n",
      "Energen -1.0\n",
      "PBGC -1.0\n",
      "Cristi -1.0\n",
      "Multichoice -1.0\n",
      "Marketo -1.0\n",
      "Brii -1.0\n",
      "Yao -1.0\n",
      "Binladin -1.0\n",
      "implanted -1.0\n",
      "Hetrick -1.0\n",
      "Wasp -1.0\n",
      "Perryman -1.0\n",
      "Stimwave -1.0\n",
      "neuromodulation -1.0\n",
      "Skinsei -1.0\n",
      "Ghosn -0.87890625\n",
      "Tianjin -0.8751300728407907\n",
      "Shave -0.8622448979591836\n",
      "Cineworld -0.8573388203017833\n",
      "Springer -0.8573388203017833\n",
      "Halo -0.8463999999999998\n",
      "McKenzie -0.8402777777777777\n",
      "Encana -0.8336483931947068\n",
      "Celltrion -0.8336483931947068\n",
      "rats -0.8264462809917354\n",
      "Temenos -0.8185941043083901\n",
      "PJM -0.8185941043083901\n",
      "Essendant -0.8185941043083901\n",
      "Goh -0.8099999999999998\n",
      "Newfield -0.795471146822498\n",
      "Catalyst -0.7809626825310977\n",
      "bows -0.7656249999999998\n",
      "tournament -0.7585848074921955\n",
      "Marubeni -0.7585848074921955\n",
      "Ingraham -0.7585848074921955\n",
      "EASA -0.7346938775510203\n",
      "Fidessa -0.7256515775034292\n",
      "Walgreen -0.7256515775034292\n",
      "Acxiom -0.7199265381083564\n",
      "Lexus -0.7159763313609465\n",
      "Diamondback -0.7159763313609465\n",
      "StoneCo -0.7159763313609465\n",
      "restraining -0.7055999999999998\n",
      "Itochu -0.7055999999999998\n",
      "Samarco -0.7055999999999998\n",
      "isolation -0.6944444444444443\n",
      "exploit -0.6865306122448979\n",
      "Aliso -0.6824196597353496\n",
      "communique -0.6824196597353496\n",
      "gravity -0.6782006920415224\n",
      "NOVEMBER -0.6760493827160493\n",
      "shekels -0.6694214876033059\n",
      "Blanco -0.6694214876033057\n",
      "Delek -0.6625202812330988\n",
      "portrayed -0.6553287981859413\n",
      "KRX -0.6553287981859413\n",
      "Ramos -0.6478286734086852\n",
      "Burnaby -0.6400000000000001\n",
      "equivalence -0.6400000000000001\n",
      "unfold -0.6400000000000001\n",
      "Brauer -0.6400000000000001\n",
      "marches -0.6400000000000001\n",
      "Gateway -0.6399999999999999\n",
      "Maricunga -0.6290130796670629\n",
      "Salar -0.6173469387755101\n",
      "motive -0.6173469387755101\n",
      "Zions -0.6173469387755101\n",
      "NIO -0.6164196434669762\n",
      "Impax -0.6096828673297052\n",
      "Via -0.6049382716049381\n",
      "DWS -0.6049382716049381\n",
      "Conrath -0.6000793493354495\n",
      "DJT -0.5984336062655746\n",
      "ski -0.5917159763313609\n",
      "tunnel -0.5917159763313609\n",
      "Madness -0.5917159763313609\n",
      "Tavares -0.5817293881068659\n",
      "tribe 1.0\n",
      "Gawker 1.0\n",
      "Noto 1.0\n",
      "SoFi 1.0\n",
      "Miles 1.0\n",
      "ArcSight 1.0\n",
      "FSTEC 1.0\n",
      "Bauer 1.0\n",
      "Tung 1.0\n",
      "Iancu 1.0\n",
      "Aon 1.0\n",
      "Venucia 1.0\n",
      "Seki 1.0\n",
      "IFT 1.0\n",
      "Makhzoomi 1.0\n",
      "Kubasik 1.0\n",
      "Televisa 1.0\n",
      "Scotch 1.0\n",
      "NRATV 1.0\n",
      "Tenaciou 1.0\n",
      "UOL 1.0\n",
      "Nexa 1.0\n",
      "Netshoes 1.0\n",
      "Ying 1.0\n",
      "LTSE 1.0\n",
      "Breland 1.0\n",
      "Allogene 1.0\n",
      "Leucadia 1.0\n",
      "IDA 1.0\n",
      "Cordis 1.0\n",
      "Rajaratnam 1.0\n",
      "Marqeta 1.0\n",
      "Rothman 1.0\n",
      "Aimovig 0.9518143961927423\n",
      "pea 0.9000657462195923\n",
      "ESMA 0.8668252080856124\n",
      "Shaheen 0.8520710059171597\n",
      "McAfee 0.8433985839233651\n",
      "Kalydeco 0.8402777777777777\n",
      "twitter 0.8402777777777777\n",
      "ATTN 0.8336483931947068\n",
      "paradigm 0.8264462809917354\n",
      "Jung 0.8185941043083902\n",
      "Hartford 0.8185941043083901\n",
      "migrants 0.8185941043083901\n",
      "Inter 0.8099999999999998\n",
      "Boao 0.8099999999999998\n",
      "Kass 0.8053911900065744\n",
      "shell 0.7785467128027681\n",
      "ADM 0.7767882792301061\n",
      "Harbour 0.7722681359044994\n",
      "AveXis 0.7621567145376668\n",
      "genetically 0.7346938775510203\n",
      "Loxo 0.7256515775034292\n",
      "downs 0.7055999999999998\n",
      "Arabic 0.6944444444444443\n",
      "admissions 0.6944444444444443\n",
      "assemblers 0.6944444444444443\n",
      "Neill 0.6885468537799907\n",
      "whiskey 0.6824196597353496\n",
      "McEwan 0.6782006920415224\n",
      "artist 0.6694214876033059\n",
      "Sen 0.6694214876033059\n",
      "clobbered 0.6694214876033059\n",
      "larotrectinib 0.6694214876033059\n",
      "nailed 0.6694214876033059\n",
      "lightweight 0.6553287981859413\n",
      "untreated 0.6553287981859413\n",
      "compatible 0.6553287981859413\n",
      "Polster 0.6512773160972608\n",
      "whisky 0.6503642039542141\n",
      "infringement 0.6400000000000001\n",
      "Lin 0.6400000000000001\n",
      "Hun 0.6400000000000001\n",
      "Grainger 0.6400000000000001\n",
      "Fiserv 0.6399999999999999\n",
      "Memory 0.6318211702827085\n",
      "Nektar 0.6290130796670629\n",
      "Fortnite 0.6173469387755101\n",
      "Rogers 0.612476370510397\n",
      "Cherokee 0.6049382716049381\n",
      "Ichikawa 0.6049382716049381\n",
      "TRI 0.5917159763313609\n",
      "Ziff 0.5847750865051901\n",
      "mutations 0.580498866213152\n",
      "JUMP 0.5776\n",
      "Babytree 0.5739210284664832\n",
      "clears 0.5625\n",
      "Greenberg 0.5624999999999999\n",
      "Ipreo 0.5624999999999999\n",
      "Sonos 0.5577631789777281\n",
      "deceptively 0.5529257067718607\n",
      "Wanda 0.5504682622268472\n",
      "Shufu 0.5504682622268472\n",
      "Santos 0.5486968449931412\n",
      "tribal 0.5463137996219283\n",
      "Dealmakers 0.5463137996219283\n",
      "refrigerated 0.5463137996219283\n",
      "TRADING 0.5377777777777777\n",
      "amenable 0.5325054784514245\n"
     ]
    }
   ],
   "source": [
    "res = sorted(count.items(),key=lambda count:count[1]['sent'],reverse=False)\n",
    "for r in res[:100]:\n",
    "    print(r[0],r[1]['sent'])\n",
    "res = sorted(count.items(),key=lambda count:count[1]['sent'],reverse=True)  \n",
    "for r in res[:100]:\n",
    "    print(r[0],r[1]['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = sorted(count.items(),key=lambda count:count[1]['PD'],reverse=True)\n",
    "# print(res)\n",
    "\n",
    "## neg pos 词\n",
    "# pos_words = {}\n",
    "# neg_words = {}\n",
    "# for word in sent_words:\n",
    "#    if count[word]['sent'] > 0:\n",
    "#       pos_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "#    else:\n",
    "#       neg_words[word.lower()] = count[word]['pos']+count[word]['neg']\n",
    "\n",
    "# output_cloud(pos_words,'pos')\n",
    "# output_cloud(neg_words,'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 求于bl词典的覆盖率\n",
    "bl_sent = {}\n",
    "bl_pos = my_read(r'/Users/wangfeihong/Desktop/Dynamic-Financial-News-Collection-and-Analysis/sentiment_analysis/bl/positive.txt')  # 4783\n",
    "bl_neg = my_read(r'/Users/wangfeihong/Desktop/Dynamic-Financial-News-Collection-and-Analysis/sentiment_analysis/bl/negative.txt')  # 2006\n",
    "\n",
    "\n",
    "for word in bl_pos:\n",
    "    bl_sent[word] = 1\n",
    "for word in bl_pos:\n",
    "    bl_sent[word] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc = 0\n",
    "# for word in pos_words:\n",
    "#    if word in bl_pos:\n",
    "#       pc += 1\n",
    "# pos_accuracy = pc/len(pos_words)  # 0.2857142857142857\n",
    "\n",
    "# nc = 0\n",
    "# for word in neg_words:\n",
    "#    if word in bl_neg:\n",
    "#       nc += 1\n",
    "# neg_accuracy = nc/len(neg_words)  # 0.18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20819/20819 [00:01<00:00, 11628.05it/s]\n"
     ]
    }
   ],
   "source": [
    "## context sentiment dict\n",
    "sent_words = [word.lower() for word in sent_words]\n",
    "feature_words = {}\n",
    "sentiment_feature = {}\n",
    "\n",
    "for data in tqdm(datas):\n",
    "    for tags in data['tags']:\n",
    "        for word,tag in tags:\n",
    "            if tag not in nn or len(word)<3: # vb+nn\n",
    "                continue\n",
    "            # word = stem_and_check(word)\n",
    "            if word not in feature_words.keys():\n",
    "                feature_words[word] = 1\n",
    "            else:\n",
    "                feature_words[word] += 1\n",
    "\n",
    "# avg_f = sum([item[1] for item in feature_words.items()])/len(feature_words.keys())\n",
    "res = sorted(feature_words.items(),key=lambda feature_words:feature_words[1],reverse=True)\n",
    "words = res[:400]\n",
    "# for word,value in tqdm(copy.items()):\n",
    "#     if value<avg_f+200:\n",
    "#         del feature_words[word]\n",
    "\n",
    "feature_words = [inf.singularize(word).lower() for word,freq in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sf_len = 0\n",
    "# for data in tqdm(datas):\n",
    "#     rate = data['rate']\n",
    "#     for tokens in data['tokens']:\n",
    "#         token_dict = {}\n",
    "#         for token in tokens:\n",
    "#             token_dict[inf.singularize(token).lower()] = token\n",
    "#         _tokens = [inf.singularize(token).lower() for token in tokens]\n",
    "\n",
    "#         for w in list(set(sent_words).intersection(set(_tokens))):\n",
    "#             for f in list(set(feature_words).intersection(set(_tokens))):\n",
    "#                 if f != w:\n",
    "#                     if abs(_tokens.index(w)-_tokens.index(f))<3 and ',' not in data['content'][min(data['content'].index(token_dict[f]),data['content'].index(token_dict[w])):max(data['content'].index(token_dict[f]),data['content'].index(token_dict[w]))]:\n",
    "#                         sf_len += 1\n",
    "#                         if f not in sentiment_feature.keys():\n",
    "#                             sentiment_feature[f] = {}\n",
    "#                             if rate > 0:\n",
    "#                                 sentiment_feature[f][w] = {'pos':1,'neg':0}\n",
    "#                             if rate < 0:\n",
    "#                                 sentiment_feature[f][w] = {'pos':0,'neg':1}\n",
    "#                         else:\n",
    "#                             if w not in sentiment_feature[f].keys():\n",
    "#                                 sentiment_feature[f][w] = {'pos':0,'neg':0}\n",
    "#                             if rate > 0:\n",
    "#                                 sentiment_feature[f][w]['pos'] += 1\n",
    "#                             if rate < 0:\n",
    "#                                 sentiment_feature[f][w]['neg'] += 1\n",
    "\n",
    "# avg_sf = sf_len/len(sentiment_feature.keys())\n",
    "# copy = sentiment_feature.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20819/20819 [00:34<00:00, 595.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "# content to vector\n",
    "\n",
    "for data in tqdm(datas):\n",
    "    idx = datas.index(data)\n",
    "    tokens = data['tokens']\n",
    "    datas[idx]['DsVector'] = [0,0,0,0]\n",
    "    datas[idx]['DsVector_rate'] = [0,0,0,0]\n",
    "    datas[idx]['SnVector'] = [0,0,0,0]\n",
    "    datas[idx]['BlVector'] = [0,0,0,0]\n",
    "    datas[idx]['PmiVector'] = [0,0,0,0]\n",
    "    datas[idx]['ContextVector'] = [0,0,0,0]\n",
    "    \n",
    "    # for f in [token for token in tokens if token in sentiment_feature.keys()]:\n",
    "    #     for w in sentiment_feature[f].keys():\n",
    "    #         if w in tokens:\n",
    "    #             if abs(tokens.index(f)-tokens.index(w))<3 and ',' not in data['content'][min(data['content'].index(f),data['content'].index(w)):max(data['content'].index(f),data['content'].index(w))]:\n",
    "    #                 if tags[tokens.index(f)][1] in adj:\n",
    "    #                     if f in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][0] += count[f]['sent']\n",
    "    #                 elif tags[tokens.index(f)][1] in adv:\n",
    "    #                     if f in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][1] += count[f]['sent']\n",
    "    #                 if tags[tokens.index(w)][1] in nn:\n",
    "    #                     if w in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][2] = count[w]['sent']\n",
    "    #                 elif tags[tokens.index(w)][1] in vb:\n",
    "    #                     if w in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][3] = count[w]['sent']\n",
    "    #                 # print(sentiment_feature[f][w]['sent'])\n",
    "    for tags in data['tags']:\n",
    "        for word,tag in tags:\n",
    "            if tag in adj:\n",
    "                if word in count.keys():\n",
    "                    datas[idx]['DsVector'][0] += count[word]['sent']\n",
    "                    datas[idx]['DsVector_rate'][0] += count[word]['sent_rate']\n",
    "                    datas[idx]['PmiVector'][0] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas[idx]['SnVector'][0] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas[idx]['BlVector'][0] += bl_sent[word]\n",
    "            elif tag in adv:\n",
    "                if word in count.keys():\n",
    "                    datas[idx]['SnVector'][1] += count[word]['sent']\n",
    "                    datas[idx]['DsVector_rate'][1] += count[word]['sent_rate']\n",
    "                    datas[idx]['PmiVector'][1] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas[idx]['SnVector'][1] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas[idx]['BlVector'][1] += bl_sent[word]  \n",
    "            elif tag in nn:\n",
    "                if word in count.keys():\n",
    "                    datas[idx]['DsVector'][2] = count[word]['sent']\n",
    "                    datas[idx]['DsVector_rate'][2] += count[word]['sent_rate']\n",
    "                    datas[idx]['PmiVector'][2] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas[idx]['SnVector'][2] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas[idx]['BlVector'][2] += bl_sent[word]\n",
    "            elif tag in vb:\n",
    "                if word in count.keys():\n",
    "                    datas[idx]['DsVector'][3] = count[word]['sent']\n",
    "                    datas[idx]['DsVector_rate'][3] += count[word]['sent_rate']\n",
    "                    datas[idx]['PmiVector'][3] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas[idx]['SnVector'][3] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas[idx]['BlVector'][3] += bl_sent[word]\n",
    "    # datas[idx]['DsVector'] = [adv_score,adv_score,noun_score,verb_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4776/4776 [00:04<00:00, 1132.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "# content to vector\n",
    "\n",
    "for data in tqdm(datas2):\n",
    "    idx = datas2.index(data)\n",
    "    tokens = data['tokens']\n",
    "    datas2[idx]['DsVector'] = [0,0,0,0]\n",
    "    datas2[idx]['DsVector_rate'] = [0,0,0,0]\n",
    "    datas2[idx]['SnVector'] = [0,0,0,0]\n",
    "    datas2[idx]['BlVector'] = [0,0,0,0]\n",
    "    datas2[idx]['PmiVector'] = [0,0,0,0]\n",
    "    datas2[idx]['ContextVector'] = [0,0,0,0]\n",
    "    \n",
    "    # for f in [token for token in tokens if token in sentiment_feature.keys()]:\n",
    "    #     for w in sentiment_feature[f].keys():\n",
    "    #         if w in tokens:\n",
    "    #             if abs(tokens.index(f)-tokens.index(w))<3 and ',' not in data['content'][min(data['content'].index(f),data['content'].index(w)):max(data['content'].index(f),data['content'].index(w))]:\n",
    "    #                 if tags[tokens.index(f)][1] in adj:\n",
    "    #                     if f in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][0] += count[f]['sent']\n",
    "    #                 elif tags[tokens.index(f)][1] in adv:\n",
    "    #                     if f in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][1] += count[f]['sent']\n",
    "    #                 if tags[tokens.index(w)][1] in nn:\n",
    "    #                     if w in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][2] = count[w]['sent']\n",
    "    #                 elif tags[tokens.index(w)][1] in vb:\n",
    "    #                     if w in count.keys():\n",
    "    #                         datas[idx]['ContextVector'][3] = count[w]['sent']\n",
    "    #                 # print(sentiment_feature[f][w]['sent'])\n",
    "    for tags in data['tags']:\n",
    "        for word,tag in tags:\n",
    "            if tag in adj:\n",
    "                if word in count.keys():\n",
    "                    datas2[idx]['DsVector'][0] += count[word]['sent']\n",
    "                    datas2[idx]['DsVector_rate'][0] += count[word]['sent_rate']\n",
    "                    datas2[idx]['PmiVector'][0] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas2[idx]['SnVector'][0] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas2[idx]['BlVector'][0] += bl_sent[word]\n",
    "            elif tag in adv:\n",
    "                if word in count.keys():\n",
    "                    datas2[idx]['SnVector'][1] += count[word]['sent']\n",
    "                    datas2[idx]['DsVector_rate'][1] += count[word]['sent_rate']\n",
    "                    datas2[idx]['PmiVector'][1] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas2[idx]['SnVector'][1] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas2[idx]['BlVector'][1] += bl_sent[word]  \n",
    "            elif tag in nn:\n",
    "                if word in count.keys():\n",
    "                    datas2[idx]['DsVector'][2] = count[word]['sent']\n",
    "                    datas2[idx]['DsVector_rate'][2] += count[word]['sent_rate']\n",
    "                    datas2[idx]['PmiVector'][2] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas2[idx]['SnVector'][2] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas2[idx]['BlVector'][2] += bl_sent[word]\n",
    "            elif tag in vb:\n",
    "                if word in count.keys():\n",
    "                    datas2[idx]['DsVector'][3] = count[word]['sent']\n",
    "                    datas2[idx]['DsVector_rate'][3] += count[word]['sent_rate']\n",
    "                    datas2[idx]['PmiVector'][3] += count[word]['PMI_sent']\n",
    "                if word in sn.data.keys():\n",
    "                    datas2[idx]['SnVector'][3] += float(sn.polarity_intense(word))\n",
    "                if word in bl_sent.keys():\n",
    "                    datas2[idx]['BlVector'][3] += bl_sent[word]\n",
    "    # datas[idx]['DsVector'] = [adv_score,adv_score,noun_score,verb_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.6621037463976945\n",
      "召回率： 0.6664047951582868\n",
      "精确率： 0.6783703703703703\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [data['DsVector'] for data in datas]\n",
    "# X = [data['SnVector'] for data in datas]\n",
    "# X = [data['BlVector'] for data in datas]\n",
    "# X = [data['PmiVector'] for data in datas]\n",
    "# X = [data['ContextVector'] for data in datas]\n",
    "X = [data['DsVector_rate'] for data in datas]\n",
    "Y = [np.sign(data['rate']) for data in datas]\n",
    "\n",
    "# Y = [data['rate'] for data in datas]\n",
    "\n",
    "train_x,test_x,train_y,test_y = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "# # x_train = X[:2500]\n",
    "# # y_train = Y[:2500]\n",
    "# # x_test = X[-300:]\n",
    "# # y_test = Y[-300:]\n",
    "clf = GaussianNB()\n",
    "# clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.6083093179634966\n",
      "召回率： 0.6018721734503858\n",
      "精确率： 0.6226475550920627\n"
     ]
    }
   ],
   "source": [
    "X = [data['DsVector'] for data in datas]\n",
    "train_x,test_x,train_y,test_y = model_selection.train_test_split(X,Y,test_size=0.2,shuffle=False)\n",
    "clf = GaussianNB()\n",
    "# clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.654418828049952\n",
      "召回率： 0.6548713642812805\n",
      "精确率： 0.6547700396219609\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-03 2019-01-22 2018-12-31 2019-04-11\n"
     ]
    }
   ],
   "source": [
    "print(datas[0]['date'],datas[-1]['date'],datas2[0]['date'],datas2[-1]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.5259365994236311\n",
      "召回率： 0.4958963157435996\n",
      "精确率： 0.49491717717352257\n"
     ]
    }
   ],
   "source": [
    "# 用18年数据训练，测试19年数据\n",
    "train_x = [data['DsVector_rate'] for data in datas]\n",
    "train_y = [np.sign(data['rate']) for data in datas]\n",
    "test_x = [data['DsVector_rate'] for data in datas2[2000:]]\n",
    "test_y = [np.sign(data['rate']) for data in datas2[2000:]]\n",
    "# clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "clf = GaussianNB()\n",
    "# clf = LinearRegression()\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.4989193083573487\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass-multioutput and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-42dc2accca42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'准确率：'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'召回率：'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'精确率：'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mrecall_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1365\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1368\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "clf.fit(np.array(train_x), np.array(train_y))\n",
    "predict_y = clf.predict(test_x)\n",
    "print('准确率：',clf.score(np.array(test_x), np.array(test_y))) \n",
    "print('召回率：',recall_score(test_y,clf.predict(test_x),average = 'macro'))\n",
    "print('精确率：',precision_score(test_y, clf.predict(test_x), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20819/20819 [==============================] - 2s 90us/step - loss: 0.6878 - acc: 0.5282: 0s - loss: 0.6882 - ac\n",
      "Epoch 2/10\n",
      "20819/20819 [==============================] - 2s 89us/step - loss: 0.6865 - acc: 0.5299\n",
      "Epoch 3/10\n",
      "20819/20819 [==============================] - 2s 89us/step - loss: 0.6863 - acc: 0.5324: 0s - loss: 0.6863 \n",
      "Epoch 4/10\n",
      "20819/20819 [==============================] - 2s 88us/step - loss: 0.6843 - acc: 0.5361: 0s - loss: 0\n",
      "Epoch 5/10\n",
      "20819/20819 [==============================] - 2s 93us/step - loss: 0.6849 - acc: 0.5366\n",
      "Epoch 6/10\n",
      "20819/20819 [==============================] - 2s 88us/step - loss: 0.6855 - acc: 0.5327\n",
      "Epoch 7/10\n",
      "20819/20819 [==============================] - 2s 97us/step - loss: 0.6843 - acc: 0.5352\n",
      "Epoch 8/10\n",
      "20819/20819 [==============================] - 2s 89us/step - loss: 0.6834 - acc: 0.5395\n",
      "Epoch 9/10\n",
      "20819/20819 [==============================] - 2s 88us/step - loss: 0.6845 - acc: 0.5365\n",
      "Epoch 10/10\n",
      "20819/20819 [==============================] - 2s 90us/step - loss: 0.6866 - acc: 0.5329: 0s - loss: 0.6864 - acc: 0\n",
      "2776/2776 [==============================] - 0s 36us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6949739774168396, 0.5108069263946169]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dnn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "train_x = [data['DsVector_rate'] for data in datas]\n",
    "Y = [np.sign(data['rate']) for data in datas]\n",
    "test_x = [data['DsVector_rate'] for data in datas2[2000:]]\n",
    "Y2 = [np.sign(data['rate']) for data in datas2[2000:]]\n",
    "\n",
    "train_y = []\n",
    "for y in Y:\n",
    "    if y == 1:\n",
    "        train_y.append(np.array([0,1]))\n",
    "    else:    \n",
    "        train_y.append(np.array([1,0]))\n",
    "\n",
    "test_y = []\n",
    "for y in Y2:\n",
    "    if y == 1:\n",
    "        test_y.append(np.array([0,1]))\n",
    "    else:    \n",
    "        test_y.append(np.array([1,0]))\n",
    "\n",
    "num_classes = 2\n",
    "# test_y = to_categorical(Y,num_classes=3)\n",
    "\n",
    "nmodel = Sequential()\n",
    "nmodel.add(Dense(units=num_classes, activation = 'relu', input_dim = np.array(train_x).shape[1]))\n",
    "nmodel.add(Dropout(0.5))\n",
    "nmodel.add(Dense(2, activation = 'relu'))\n",
    "nmodel.add(Dropout(0.5))\n",
    "# dropout:https://blog.csdn.net/program_developer/article/details/80737724\n",
    "nmodel.add(Dense(2, activation = 'softmax'))\n",
    "nmodel.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics = ['accuracy'])\n",
    "nmodel.fit(np.array(train_x),np.array(train_y),epochs=10, batch_size=5)\n",
    "nmodel.evaluate(np.array(test_x),np.array(test_y), batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-01-29'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas2[2000]['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'By Sijia Jiang\\nHONG KONG (Reuters) - Huawei Technologies Co Ltd\\'s [HWT.UL] planned deal with U.S. carrier AT&T Inc (T.N) to sell its smartphones in the United States has collapsed at the 11th hour because of security concerns, people with knowledge of the matter said, in a blow to the Chinese firm\\'s global ambitions.\\nAT&T was pressured to drop the deal after members of the U.S. Senate and House intelligence committees sent a letter on Dec. 20 to the U.S. Federal Communications Commission (FCC) citing concerns about Huawei\\'s plans to launch consumer products through a major U.S. telecom carrier.\\nThe letter to FCC Chairman Ajit Pai, which was signed by 18 lawmakers, noted concerns about Chinese companies in the U.S. telecommunications industry.\\nThe letter notes the committee\\'s concerns \"about Chinese espionage in general, and Huawei\\'s role in that espionage in particular.\" A copy of the letter was seen by Reuters.\\nHuawei said in a statement on Tuesday that its flagship smartphone Mate 10 Pro - Huawei\\'s challenger to the iPhone - will be sold in the United States only through open channels.\\n\"The U.S. market presents unique challenges for Huawei, and while the HUAWEI Mate 10 Pro will not be sold by U.S. carriers, we remain committed to this market now and in the future,\" it said.\\nHuawei\\'s Washington-based spokesman William Plummer said on Tuesday that \"privacy and security are always our first priority.\"\\n\"We are compliant with the world\\'s most stringent privacy protection frameworks and requirements and have gained the trust of over 150 million customers in the past year alone,\" Plummer said in an email.\\n\\nREJECTION OF DEALS\\nHuawei is the world\\'s third largest smartphone vendor by volume after Samsung Electronics (005930.KS) and Apple Inc (AAPL.O), but it has a mere 0.5 percent share of the U.S. smartphone market, compared with 39 percent for Apple and 18 percent for Samsung, according to industry tracker Canalys. \\nWashington began to have concerns about Chinese investment in the United States before President Donald Trump took office last year, and those concerns have heightened.\\nBecause of this skepticism, including the U.S. government\\'s rejection of several Chinese deals, Chinese investment in the United States fell to $25 billion last year from $50 billion in 2016, according to Derek Scissors, a China expert at the American Enterprise Institute.\\nAmong the deals killed recently by the multi-agency Committee on Foreign Investment in the United States (CFIUS) were Ant Financial\\'s plan to buy U.S. money transfer company MoneyGram International Inc (MGI.O), the purchase by China-backed Canyon Bridge Capital Partners LLC of a U.S. chip maker and plans by Zhongwang USA, backed by a Chinese aluminum tycoon, to buy a U.S. aluminum maker.\\nIn 2012, Huawei and ZTE Corp (000063.SZ) (0763.HK) were the subject of a U.S. investigation into whether their equipment provided an opportunity for foreign espionage and threatened critical U.S. infrastructure - a link that Huawei has consistently denied.    \\nIn the United States, where telecom carriers dominate the distribution channel by typically providing subsidies and special package deals, Huawei had been unable to make any significant inroads due to national security concerns. \\nAT&T declined to comment. \\nThe flagship Mate 10 Pro that was to be introduced is Huawei\\'s most high-end product to date, equipped with its own AI-powered chips that Huawei says process data much faster than those used by Apple and Samsung. It was launched in Europe in October with a price tag of 799 euros ($955). \\n\"This makes it very difficult for Huawei to get significant in the U.S. as the open channels account for only about 10-11 percent of the market,\" said Canalys analyst Mo Jia, referring to sales channels outside telecom carriers and vendors\\' own stores.\\nHe said Huawei\\'s proprietary mobile chips may have presented a bigger regulatory hurdle for its U.S. market entry in the current political climate, compared with other Chinese vendors\\' entry strategy that relies on U.S. chip suppliers.  \\n\\n ($1 = 0.8363 euro)\\n\\n',\n",
       " 'tokens': [['Huawei',\n",
       "   'Technologies',\n",
       "   'Co',\n",
       "   'Ltd',\n",
       "   'HWT',\n",
       "   'UL',\n",
       "   'planned',\n",
       "   'deal',\n",
       "   'U',\n",
       "   'S',\n",
       "   'carrier',\n",
       "   'AT',\n",
       "   'T',\n",
       "   'Inc',\n",
       "   'T',\n",
       "   'N',\n",
       "   'sell',\n",
       "   'smartphones',\n",
       "   'United',\n",
       "   'States',\n",
       "   'collapsed',\n",
       "   'th',\n",
       "   'hour',\n",
       "   'security',\n",
       "   'concerns',\n",
       "   'people',\n",
       "   'knowledge',\n",
       "   'matter',\n",
       "   'said',\n",
       "   'blow',\n",
       "   'Chinese',\n",
       "   'firm',\n",
       "   'global',\n",
       "   'ambitions'],\n",
       "  ['AT',\n",
       "   'T',\n",
       "   'pressured',\n",
       "   'drop',\n",
       "   'deal',\n",
       "   'members',\n",
       "   'U',\n",
       "   'S',\n",
       "   'Senate',\n",
       "   'House',\n",
       "   'intelligence',\n",
       "   'committees',\n",
       "   'sent',\n",
       "   'letter',\n",
       "   'Dec',\n",
       "   'U',\n",
       "   'S',\n",
       "   'Federal',\n",
       "   'Communications',\n",
       "   'Commission',\n",
       "   'FCC',\n",
       "   'citing',\n",
       "   'concerns',\n",
       "   'Huawei',\n",
       "   'plans',\n",
       "   'launch',\n",
       "   'consumer',\n",
       "   'products',\n",
       "   'major',\n",
       "   'U',\n",
       "   'S',\n",
       "   'telecom',\n",
       "   'carrier'],\n",
       "  ['The',\n",
       "   'letter',\n",
       "   'FCC',\n",
       "   'Chairman',\n",
       "   'Ajit',\n",
       "   'Pai',\n",
       "   'signed',\n",
       "   'lawmakers',\n",
       "   'noted',\n",
       "   'concerns',\n",
       "   'Chinese',\n",
       "   'companies',\n",
       "   'U',\n",
       "   'S',\n",
       "   'telecommunications',\n",
       "   'industry'],\n",
       "  ['The',\n",
       "   'letter',\n",
       "   'notes',\n",
       "   'committee',\n",
       "   'concerns',\n",
       "   'Chinese',\n",
       "   'espionage',\n",
       "   'general',\n",
       "   'Huawei',\n",
       "   'role',\n",
       "   'espionage',\n",
       "   'particular'],\n",
       "  ['A', 'copy', 'letter', 'seen'],\n",
       "  ['Huawei',\n",
       "   'said',\n",
       "   'statement',\n",
       "   'Tuesday',\n",
       "   'flagship',\n",
       "   'smartphone',\n",
       "   'Mate',\n",
       "   'Pro',\n",
       "   'Huawei',\n",
       "   'challenger',\n",
       "   'iPhone',\n",
       "   'sold',\n",
       "   'United',\n",
       "   'States',\n",
       "   'open',\n",
       "   'channels'],\n",
       "  ['The',\n",
       "   'U',\n",
       "   'S',\n",
       "   'market',\n",
       "   'presents',\n",
       "   'unique',\n",
       "   'challenges',\n",
       "   'Huawei',\n",
       "   'HUAWEI',\n",
       "   'Mate',\n",
       "   'Pro',\n",
       "   'sold',\n",
       "   'U',\n",
       "   'S',\n",
       "   'carriers',\n",
       "   'remain',\n",
       "   'committed',\n",
       "   'market',\n",
       "   'future',\n",
       "   'said'],\n",
       "  ['Huawei',\n",
       "   'Washington',\n",
       "   'based',\n",
       "   'spokesman',\n",
       "   'William',\n",
       "   'Plummer',\n",
       "   'said',\n",
       "   'Tuesday',\n",
       "   'privacy',\n",
       "   'security',\n",
       "   'always',\n",
       "   'first',\n",
       "   'priority'],\n",
       "  ['We',\n",
       "   'compliant',\n",
       "   'world',\n",
       "   'stringent',\n",
       "   'privacy',\n",
       "   'protection',\n",
       "   'frameworks',\n",
       "   'requirements',\n",
       "   'gained',\n",
       "   'trust',\n",
       "   'million',\n",
       "   'customers',\n",
       "   'past',\n",
       "   'year',\n",
       "   'alone',\n",
       "   'Plummer',\n",
       "   'said',\n",
       "   'email'],\n",
       "  ['REJECTION',\n",
       "   'OF',\n",
       "   'DEALS',\n",
       "   'Huawei',\n",
       "   'world',\n",
       "   'third',\n",
       "   'largest',\n",
       "   'smartphone',\n",
       "   'vendor',\n",
       "   'volume',\n",
       "   'Samsung',\n",
       "   'Electronics',\n",
       "   'KS',\n",
       "   'Apple',\n",
       "   'Inc',\n",
       "   'AAPL',\n",
       "   'O',\n",
       "   'mere',\n",
       "   'percent',\n",
       "   'share',\n",
       "   'U',\n",
       "   'S',\n",
       "   'smartphone',\n",
       "   'market',\n",
       "   'compared',\n",
       "   'percent',\n",
       "   'Apple',\n",
       "   'percent',\n",
       "   'Samsung',\n",
       "   'according',\n",
       "   'industry',\n",
       "   'tracker',\n",
       "   'Canalys'],\n",
       "  ['Washington',\n",
       "   'began',\n",
       "   'concerns',\n",
       "   'Chinese',\n",
       "   'investment',\n",
       "   'United',\n",
       "   'States',\n",
       "   'President',\n",
       "   'Donald',\n",
       "   'Trump',\n",
       "   'took',\n",
       "   'office',\n",
       "   'last',\n",
       "   'year',\n",
       "   'concerns',\n",
       "   'heightened'],\n",
       "  ['Because',\n",
       "   'skepticism',\n",
       "   'including',\n",
       "   'U',\n",
       "   'S',\n",
       "   'government',\n",
       "   'rejection',\n",
       "   'several',\n",
       "   'Chinese',\n",
       "   'deals',\n",
       "   'Chinese',\n",
       "   'investment',\n",
       "   'United',\n",
       "   'States',\n",
       "   'fell',\n",
       "   'billion',\n",
       "   'last',\n",
       "   'year',\n",
       "   'billion',\n",
       "   'according',\n",
       "   'Derek',\n",
       "   'Scissors',\n",
       "   'China',\n",
       "   'expert',\n",
       "   'American',\n",
       "   'Enterprise',\n",
       "   'Institute'],\n",
       "  ['Among',\n",
       "   'deals',\n",
       "   'killed',\n",
       "   'recently',\n",
       "   'multi',\n",
       "   'agency',\n",
       "   'Committee',\n",
       "   'Foreign',\n",
       "   'Investment',\n",
       "   'United',\n",
       "   'States',\n",
       "   'CFIUS',\n",
       "   'Ant',\n",
       "   'Financial',\n",
       "   'plan',\n",
       "   'buy',\n",
       "   'U',\n",
       "   'S',\n",
       "   'money',\n",
       "   'transfer',\n",
       "   'company',\n",
       "   'MoneyGram',\n",
       "   'International',\n",
       "   'Inc',\n",
       "   'MGI',\n",
       "   'O',\n",
       "   'purchase',\n",
       "   'China',\n",
       "   'backed',\n",
       "   'Canyon',\n",
       "   'Bridge',\n",
       "   'Capital',\n",
       "   'Partners',\n",
       "   'LLC',\n",
       "   'U',\n",
       "   'S',\n",
       "   'chip',\n",
       "   'maker',\n",
       "   'plans',\n",
       "   'Zhongwang',\n",
       "   'USA',\n",
       "   'backed',\n",
       "   'Chinese',\n",
       "   'aluminum',\n",
       "   'tycoon',\n",
       "   'buy',\n",
       "   'U',\n",
       "   'S',\n",
       "   'aluminum',\n",
       "   'maker'],\n",
       "  ['In',\n",
       "   'Huawei',\n",
       "   'ZTE',\n",
       "   'Corp',\n",
       "   'SZ',\n",
       "   'HK',\n",
       "   'subject',\n",
       "   'U',\n",
       "   'S',\n",
       "   'investigation',\n",
       "   'whether',\n",
       "   'equipment',\n",
       "   'provided',\n",
       "   'opportunity',\n",
       "   'foreign',\n",
       "   'espionage',\n",
       "   'threatened',\n",
       "   'critical',\n",
       "   'U',\n",
       "   'S',\n",
       "   'infrastructure',\n",
       "   'link',\n",
       "   'Huawei',\n",
       "   'consistently',\n",
       "   'denied'],\n",
       "  ['In',\n",
       "   'United',\n",
       "   'States',\n",
       "   'telecom',\n",
       "   'carriers',\n",
       "   'dominate',\n",
       "   'distribution',\n",
       "   'channel',\n",
       "   'typically',\n",
       "   'providing',\n",
       "   'subsidies',\n",
       "   'special',\n",
       "   'package',\n",
       "   'deals',\n",
       "   'Huawei',\n",
       "   'unable',\n",
       "   'make',\n",
       "   'significant',\n",
       "   'inroads',\n",
       "   'due',\n",
       "   'national',\n",
       "   'security',\n",
       "   'concerns'],\n",
       "  ['AT', 'T', 'declined', 'comment'],\n",
       "  ['The',\n",
       "   'flagship',\n",
       "   'Mate',\n",
       "   'Pro',\n",
       "   'introduced',\n",
       "   'Huawei',\n",
       "   'high',\n",
       "   'end',\n",
       "   'product',\n",
       "   'date',\n",
       "   'equipped',\n",
       "   'AI',\n",
       "   'powered',\n",
       "   'chips',\n",
       "   'Huawei',\n",
       "   'says',\n",
       "   'process',\n",
       "   'data',\n",
       "   'much',\n",
       "   'faster',\n",
       "   'used',\n",
       "   'Apple',\n",
       "   'Samsung'],\n",
       "  ['It', 'launched', 'Europe', 'October', 'price', 'tag', 'euros'],\n",
       "  ['This',\n",
       "   'makes',\n",
       "   'difficult',\n",
       "   'Huawei',\n",
       "   'get',\n",
       "   'significant',\n",
       "   'U',\n",
       "   'S',\n",
       "   'open',\n",
       "   'channels',\n",
       "   'account',\n",
       "   'percent',\n",
       "   'market',\n",
       "   'said',\n",
       "   'Canalys',\n",
       "   'analyst',\n",
       "   'Mo',\n",
       "   'Jia',\n",
       "   'referring',\n",
       "   'sales',\n",
       "   'channels',\n",
       "   'outside',\n",
       "   'telecom',\n",
       "   'carriers',\n",
       "   'vendors',\n",
       "   'stores'],\n",
       "  ['He',\n",
       "   'said',\n",
       "   'Huawei',\n",
       "   'proprietary',\n",
       "   'mobile',\n",
       "   'chips',\n",
       "   'may',\n",
       "   'presented',\n",
       "   'bigger',\n",
       "   'regulatory',\n",
       "   'hurdle',\n",
       "   'U',\n",
       "   'S',\n",
       "   'market',\n",
       "   'entry',\n",
       "   'current',\n",
       "   'political',\n",
       "   'climate',\n",
       "   'compared',\n",
       "   'Chinese',\n",
       "   'vendors',\n",
       "   'entry',\n",
       "   'strategy',\n",
       "   'relies',\n",
       "   'U',\n",
       "   'S',\n",
       "   'chip',\n",
       "   'suppliers'],\n",
       "  ['euro']],\n",
       " 'tags': [[('Huawei', 'NNP'),\n",
       "   ('Technologies', 'NNPS'),\n",
       "   ('Co', 'NNP'),\n",
       "   ('Ltd', 'NNP'),\n",
       "   ('HWT', 'NNP'),\n",
       "   ('UL', 'NNP'),\n",
       "   ('planned', 'VBD'),\n",
       "   ('deal', 'NN'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('carrier', 'NN'),\n",
       "   ('AT', 'NNP'),\n",
       "   ('T', 'NNP'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('T', 'NNP'),\n",
       "   ('N', 'NNP'),\n",
       "   ('sell', 'VB'),\n",
       "   ('smartphones', 'NNS'),\n",
       "   ('United', 'NNP'),\n",
       "   ('States', 'NNPS'),\n",
       "   ('collapsed', 'VBD'),\n",
       "   ('th', 'JJ'),\n",
       "   ('hour', 'NN'),\n",
       "   ('security', 'NN'),\n",
       "   ('concerns', 'NNS'),\n",
       "   ('people', 'NNS'),\n",
       "   ('knowledge', 'VBP'),\n",
       "   ('matter', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('blow', 'IN'),\n",
       "   ('Chinese', 'JJ'),\n",
       "   ('firm', 'NN'),\n",
       "   ('global', 'JJ'),\n",
       "   ('ambitions', 'NNS')],\n",
       "  [('AT', 'NNP'),\n",
       "   ('T', 'NNP'),\n",
       "   ('pressured', 'VBD'),\n",
       "   ('drop', 'NN'),\n",
       "   ('deal', 'NN'),\n",
       "   ('members', 'NNS'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('Senate', 'NNP'),\n",
       "   ('House', 'NNP'),\n",
       "   ('intelligence', 'NN'),\n",
       "   ('committees', 'NNS'),\n",
       "   ('sent', 'VBD'),\n",
       "   ('letter', 'NN'),\n",
       "   ('Dec', 'NNP'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('Federal', 'NNP'),\n",
       "   ('Communications', 'NNP'),\n",
       "   ('Commission', 'NNP'),\n",
       "   ('FCC', 'NNP'),\n",
       "   ('citing', 'VBG'),\n",
       "   ('concerns', 'NNS'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('plans', 'VBZ'),\n",
       "   ('launch', 'JJ'),\n",
       "   ('consumer', 'NN'),\n",
       "   ('products', 'NNS'),\n",
       "   ('major', 'JJ'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('telecom', 'NN'),\n",
       "   ('carrier', 'NN')],\n",
       "  [('The', 'DT'),\n",
       "   ('letter', 'NN'),\n",
       "   ('FCC', 'NNP'),\n",
       "   ('Chairman', 'NNP'),\n",
       "   ('Ajit', 'NNP'),\n",
       "   ('Pai', 'NNP'),\n",
       "   ('signed', 'VBD'),\n",
       "   ('lawmakers', 'NNS'),\n",
       "   ('noted', 'VBD'),\n",
       "   ('concerns', 'NNS'),\n",
       "   ('Chinese', 'JJ'),\n",
       "   ('companies', 'NNS'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('telecommunications', 'NNS'),\n",
       "   ('industry', 'NN')],\n",
       "  [('The', 'DT'),\n",
       "   ('letter', 'NN'),\n",
       "   ('notes', 'VBZ'),\n",
       "   ('committee', 'NN'),\n",
       "   ('concerns', 'NNS'),\n",
       "   ('Chinese', 'JJ'),\n",
       "   ('espionage', 'NN'),\n",
       "   ('general', 'JJ'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('role', 'NN'),\n",
       "   ('espionage', 'NN'),\n",
       "   ('particular', 'JJ')],\n",
       "  [('A', 'DT'), ('copy', 'NN'), ('letter', 'NN'), ('seen', 'VBN')],\n",
       "  [('Huawei', 'NNP'),\n",
       "   ('said', 'VBD'),\n",
       "   ('statement', 'NN'),\n",
       "   ('Tuesday', 'NNP'),\n",
       "   ('flagship', 'VBZ'),\n",
       "   ('smartphone', 'JJ'),\n",
       "   ('Mate', 'NNP'),\n",
       "   ('Pro', 'NNP'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('challenger', 'NN'),\n",
       "   ('iPhone', 'NN'),\n",
       "   ('sold', 'VBN'),\n",
       "   ('United', 'NNP'),\n",
       "   ('States', 'NNPS'),\n",
       "   ('open', 'JJ'),\n",
       "   ('channels', 'NNS')],\n",
       "  [('The', 'DT'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('market', 'NN'),\n",
       "   ('presents', 'NNS'),\n",
       "   ('unique', 'JJ'),\n",
       "   ('challenges', 'NNS'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('HUAWEI', 'NNP'),\n",
       "   ('Mate', 'NNP'),\n",
       "   ('Pro', 'NNP'),\n",
       "   ('sold', 'VBD'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('carriers', 'NNS'),\n",
       "   ('remain', 'VBP'),\n",
       "   ('committed', 'JJ'),\n",
       "   ('market', 'NN'),\n",
       "   ('future', 'NN'),\n",
       "   ('said', 'VBD')],\n",
       "  [('Huawei', 'NNP'),\n",
       "   ('Washington', 'NNP'),\n",
       "   ('based', 'VBN'),\n",
       "   ('spokesman', 'NN'),\n",
       "   ('William', 'NNP'),\n",
       "   ('Plummer', 'NNP'),\n",
       "   ('said', 'VBD'),\n",
       "   ('Tuesday', 'NNP'),\n",
       "   ('privacy', 'NN'),\n",
       "   ('security', 'NN'),\n",
       "   ('always', 'RB'),\n",
       "   ('first', 'RB'),\n",
       "   ('priority', 'NN')],\n",
       "  [('We', 'PRP'),\n",
       "   ('compliant', 'VBP'),\n",
       "   ('world', 'NN'),\n",
       "   ('stringent', 'NN'),\n",
       "   ('privacy', 'NN'),\n",
       "   ('protection', 'NN'),\n",
       "   ('frameworks', 'VBZ'),\n",
       "   ('requirements', 'NNS'),\n",
       "   ('gained', 'VBD'),\n",
       "   ('trust', 'JJ'),\n",
       "   ('million', 'CD'),\n",
       "   ('customers', 'NNS'),\n",
       "   ('past', 'JJ'),\n",
       "   ('year', 'NN'),\n",
       "   ('alone', 'RB'),\n",
       "   ('Plummer', 'NNP'),\n",
       "   ('said', 'VBD'),\n",
       "   ('email', 'NN')],\n",
       "  [('REJECTION', 'NN'),\n",
       "   ('OF', 'IN'),\n",
       "   ('DEALS', 'NNP'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('world', 'NN'),\n",
       "   ('third', 'RB'),\n",
       "   ('largest', 'JJS'),\n",
       "   ('smartphone', 'NN'),\n",
       "   ('vendor', 'NN'),\n",
       "   ('volume', 'NN'),\n",
       "   ('Samsung', 'NNP'),\n",
       "   ('Electronics', 'NNP'),\n",
       "   ('KS', 'NNP'),\n",
       "   ('Apple', 'NNP'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('AAPL', 'NNP'),\n",
       "   ('O', 'NNP'),\n",
       "   ('mere', 'FW'),\n",
       "   ('percent', 'NN'),\n",
       "   ('share', 'NN'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('smartphone', 'NN'),\n",
       "   ('market', 'NN'),\n",
       "   ('compared', 'VBN'),\n",
       "   ('percent', 'NN'),\n",
       "   ('Apple', 'NNP'),\n",
       "   ('percent', 'NN'),\n",
       "   ('Samsung', 'NNP'),\n",
       "   ('according', 'VBG'),\n",
       "   ('industry', 'NN'),\n",
       "   ('tracker', 'NN'),\n",
       "   ('Canalys', 'NNP')],\n",
       "  [('Washington', 'NNP'),\n",
       "   ('began', 'VBD'),\n",
       "   ('concerns', 'NNS'),\n",
       "   ('Chinese', 'JJ'),\n",
       "   ('investment', 'NN'),\n",
       "   ('United', 'NNP'),\n",
       "   ('States', 'NNPS'),\n",
       "   ('President', 'NNP'),\n",
       "   ('Donald', 'NNP'),\n",
       "   ('Trump', 'NNP'),\n",
       "   ('took', 'VBD'),\n",
       "   ('office', 'NN'),\n",
       "   ('last', 'JJ'),\n",
       "   ('year', 'NN'),\n",
       "   ('concerns', 'NNS'),\n",
       "   ('heightened', 'VBD')],\n",
       "  [('Because', 'IN'),\n",
       "   ('skepticism', 'NN'),\n",
       "   ('including', 'VBG'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('government', 'NN'),\n",
       "   ('rejection', 'NN'),\n",
       "   ('several', 'JJ'),\n",
       "   ('Chinese', 'JJ'),\n",
       "   ('deals', 'NNS'),\n",
       "   ('Chinese', 'JJ'),\n",
       "   ('investment', 'NN'),\n",
       "   ('United', 'NNP'),\n",
       "   ('States', 'NNPS'),\n",
       "   ('fell', 'VBD'),\n",
       "   ('billion', 'CD'),\n",
       "   ('last', 'JJ'),\n",
       "   ('year', 'NN'),\n",
       "   ('billion', 'CD'),\n",
       "   ('according', 'VBG'),\n",
       "   ('Derek', 'NNP'),\n",
       "   ('Scissors', 'NNP'),\n",
       "   ('China', 'NNP'),\n",
       "   ('expert', 'JJ'),\n",
       "   ('American', 'NNP'),\n",
       "   ('Enterprise', 'NNP'),\n",
       "   ('Institute', 'NNP')],\n",
       "  [('Among', 'IN'),\n",
       "   ('deals', 'NNS'),\n",
       "   ('killed', 'VBD'),\n",
       "   ('recently', 'RB'),\n",
       "   ('multi', 'JJ'),\n",
       "   ('agency', 'NN'),\n",
       "   ('Committee', 'NNP'),\n",
       "   ('Foreign', 'NNP'),\n",
       "   ('Investment', 'NNP'),\n",
       "   ('United', 'NNP'),\n",
       "   ('States', 'NNPS'),\n",
       "   ('CFIUS', 'NNP'),\n",
       "   ('Ant', 'NNP'),\n",
       "   ('Financial', 'NNP'),\n",
       "   ('plan', 'NN'),\n",
       "   ('buy', 'VB'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('money', 'NN'),\n",
       "   ('transfer', 'NN'),\n",
       "   ('company', 'NN'),\n",
       "   ('MoneyGram', 'NNP'),\n",
       "   ('International', 'NNP'),\n",
       "   ('Inc', 'NNP'),\n",
       "   ('MGI', 'NNP'),\n",
       "   ('O', 'NNP'),\n",
       "   ('purchase', 'NN'),\n",
       "   ('China', 'NNP'),\n",
       "   ('backed', 'VBD'),\n",
       "   ('Canyon', 'NNP'),\n",
       "   ('Bridge', 'NNP'),\n",
       "   ('Capital', 'NNP'),\n",
       "   ('Partners', 'NNP'),\n",
       "   ('LLC', 'NNP'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('chip', 'NN'),\n",
       "   ('maker', 'NN'),\n",
       "   ('plans', 'NNS'),\n",
       "   ('Zhongwang', 'NNP'),\n",
       "   ('USA', 'NNP'),\n",
       "   ('backed', 'VBD'),\n",
       "   ('Chinese', 'JJ'),\n",
       "   ('aluminum', 'NN'),\n",
       "   ('tycoon', 'NN'),\n",
       "   ('buy', 'VB'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('aluminum', 'NN'),\n",
       "   ('maker', 'NN')],\n",
       "  [('In', 'IN'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('ZTE', 'NNP'),\n",
       "   ('Corp', 'NNP'),\n",
       "   ('SZ', 'NNP'),\n",
       "   ('HK', 'NNP'),\n",
       "   ('subject', 'JJ'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('investigation', 'NN'),\n",
       "   ('whether', 'IN'),\n",
       "   ('equipment', 'NN'),\n",
       "   ('provided', 'VBN'),\n",
       "   ('opportunity', 'NN'),\n",
       "   ('foreign', 'JJ'),\n",
       "   ('espionage', 'NN'),\n",
       "   ('threatened', 'VBD'),\n",
       "   ('critical', 'JJ'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('infrastructure', 'NN'),\n",
       "   ('link', 'NN'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('consistently', 'RB'),\n",
       "   ('denied', 'VBD')],\n",
       "  [('In', 'IN'),\n",
       "   ('United', 'NNP'),\n",
       "   ('States', 'NNPS'),\n",
       "   ('telecom', 'VBP'),\n",
       "   ('carriers', 'NNS'),\n",
       "   ('dominate', 'VBP'),\n",
       "   ('distribution', 'NN'),\n",
       "   ('channel', 'NN'),\n",
       "   ('typically', 'RB'),\n",
       "   ('providing', 'VBG'),\n",
       "   ('subsidies', 'NNS'),\n",
       "   ('special', 'JJ'),\n",
       "   ('package', 'NN'),\n",
       "   ('deals', 'NNS'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('unable', 'JJ'),\n",
       "   ('make', 'NN'),\n",
       "   ('significant', 'JJ'),\n",
       "   ('inroads', 'NNS'),\n",
       "   ('due', 'JJ'),\n",
       "   ('national', 'JJ'),\n",
       "   ('security', 'NN'),\n",
       "   ('concerns', 'NNS')],\n",
       "  [('AT', 'NNP'), ('T', 'NNP'), ('declined', 'VBD'), ('comment', 'NN')],\n",
       "  [('The', 'DT'),\n",
       "   ('flagship', 'NN'),\n",
       "   ('Mate', 'NNP'),\n",
       "   ('Pro', 'NNP'),\n",
       "   ('introduced', 'VBD'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('high', 'JJ'),\n",
       "   ('end', 'NN'),\n",
       "   ('product', 'NN'),\n",
       "   ('date', 'NN'),\n",
       "   ('equipped', 'VBD'),\n",
       "   ('AI', 'NNP'),\n",
       "   ('powered', 'VBD'),\n",
       "   ('chips', 'NNS'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('says', 'VBZ'),\n",
       "   ('process', 'NN'),\n",
       "   ('data', 'NNS'),\n",
       "   ('much', 'RB'),\n",
       "   ('faster', 'RBR'),\n",
       "   ('used', 'VBN'),\n",
       "   ('Apple', 'NNP'),\n",
       "   ('Samsung', 'NNP')],\n",
       "  [('It', 'PRP'),\n",
       "   ('launched', 'VBD'),\n",
       "   ('Europe', 'NNP'),\n",
       "   ('October', 'NNP'),\n",
       "   ('price', 'NN'),\n",
       "   ('tag', 'NN'),\n",
       "   ('euros', 'NN')],\n",
       "  [('This', 'DT'),\n",
       "   ('makes', 'VBZ'),\n",
       "   ('difficult', 'JJ'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('get', 'VB'),\n",
       "   ('significant', 'JJ'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('open', 'JJ'),\n",
       "   ('channels', 'NNS'),\n",
       "   ('account', 'VBP'),\n",
       "   ('percent', 'JJ'),\n",
       "   ('market', 'NN'),\n",
       "   ('said', 'VBD'),\n",
       "   ('Canalys', 'NNP'),\n",
       "   ('analyst', 'NN'),\n",
       "   ('Mo', 'NNP'),\n",
       "   ('Jia', 'NNP'),\n",
       "   ('referring', 'VBG'),\n",
       "   ('sales', 'NNS'),\n",
       "   ('channels', 'NNS'),\n",
       "   ('outside', 'IN'),\n",
       "   ('telecom', 'NN'),\n",
       "   ('carriers', 'NNS'),\n",
       "   ('vendors', 'NNS'),\n",
       "   ('stores', 'NNS')],\n",
       "  [('He', 'PRP'),\n",
       "   ('said', 'VBD'),\n",
       "   ('Huawei', 'NNP'),\n",
       "   ('proprietary', 'JJ'),\n",
       "   ('mobile', 'JJ'),\n",
       "   ('chips', 'NNS'),\n",
       "   ('may', 'MD'),\n",
       "   ('presented', 'VB'),\n",
       "   ('bigger', 'JJR'),\n",
       "   ('regulatory', 'JJ'),\n",
       "   ('hurdle', 'NN'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('market', 'NN'),\n",
       "   ('entry', 'NN'),\n",
       "   ('current', 'JJ'),\n",
       "   ('political', 'JJ'),\n",
       "   ('climate', 'NN'),\n",
       "   ('compared', 'VBN'),\n",
       "   ('Chinese', 'JJ'),\n",
       "   ('vendors', 'NNS'),\n",
       "   ('entry', 'VBP'),\n",
       "   ('strategy', 'NN'),\n",
       "   ('relies', 'NNS'),\n",
       "   ('U', 'NNP'),\n",
       "   ('S', 'NNP'),\n",
       "   ('chip', 'NN'),\n",
       "   ('suppliers', 'NNS')],\n",
       "  [('euro', 'NN')]],\n",
       " 'company': 'Apple Inc.',\n",
       " 'rate': -0.00011473628770084758,\n",
       " 'date': '2018-01-08',\n",
       " 'DsVector': [-0.10449180760681044,\n",
       "  0,\n",
       "  0.00013775926293284053,\n",
       "  0.007112788503411243],\n",
       " 'DsVector_rate': [-0.5341107071963779,\n",
       "  -0.16236085989343077,\n",
       "  -1.7139857682830364,\n",
       "  -1.1114960592751553],\n",
       " 'SnVector': [8.184, 0.3549574418302819, 22.732, 4.354000000000001],\n",
       " 'BlVector': [-3, -2, -2, -3],\n",
       " 'PmiVector': [1.1762454584062887,\n",
       "  0.04674299128888393,\n",
       "  0.9132988941284559,\n",
       "  1.3223480491514508],\n",
       " 'ContextVector': [0, 0, 0, 0]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# output = open('D:\\\\xxx\\\\data.pkl', 'wb')\n",
    "# input = open('D:\\\\xxx\\\\data.pkl', 'rb')\n",
    "# s = pickle.dump(clf, output)\n",
    "# output.close()\n",
    "# clf2 = pickle.load(input)\n",
    "# input.close()\n",
    "# print clf2.predict(X[0:1])\n",
    "datas[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
